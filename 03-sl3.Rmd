# Modern Super (Machine) Learning with `sl3`

_Rachael Phillips_, based on the [`sl3` package](https://github.com/tlverse/sl3)
by _Jeremy Coyle, Nima Hejazi, Ivana Malenica, and Oleg Sofrygin_

Updated: `r Sys.Date()`

## Learning Objectives

By the end of this chapter you will be able to:

1. Assemble an ensemble of learners based on the properties that identify what
features they support.
2. Customize learner hyperparameters to incorporate a diversity of different
settings.
3. Select a subset of available covariates and pass only those variables to the
modeling algorithm.
4. Fit an ensemble with nested cross-validation to obtain an estimate of the
performance of the ensemble itself.

## Background

Now that we have defined the statistical estimation problem, we are ready construct
the TMLE; an asymptotically efficient substitution estimator of this target
quantity. The first step in this estimation procedure is an initial estimate of
the data-generating distribution, or the relevant part of this distribution that
is needed to evaluate the target parameter. For this initial estimation, we use
the Super Learner, an important step in creating a robust estimator.

#### Super Learner

* Loss-function-based tool that uses V-fold cross-validation to obtain the best
prediction of the relevant part of the likelihood (needed to evaluate target parameter)
based on a weighted average of a *library* of machine learning algorithms.

* The library of machine learning algorithms consists of functions (“learners”
in the `sl3` nomenclature) that we think might be consistent with the true
data-generating distribution.

* Proven to be asymptotically as accurate as the best possible prediction
algorithm that is tested.

## Modern Super (Machine) Learning with `sl3`

### Basic Implementation

**0. Load necessary libraries**

First, we will load the relevant `R` packages and set a seed.

```{r setup, message=FALSE, warning=FALSE}
library(here)
library(tidyverse)
library(data.table)
library(sl3)
library(SuperLearner)
library(origami)
set.seed(7194)
```
**1. Load data**

We begin by illustrating the default functionality of the Super Learner
algorithm as implemented in `sl3`. Using the WASH data, we are interested in
predicting weight-for-height z-score `whz` using the available covariate data.

```{r data}
# load data set and take a peek
washb_data <- fread(here("data", "washb_data.csv"), stringsAsFactors = TRUE)
head(washb_data)
```
**2. Define machine learning task**

To define the machine learning **"task"** (predict weight-for-height z-score
`whz` using the available covariate data), we need to create an `sl3_Task`
object. The `sl3_Task` keeps track of the roles the variables play in the machine
learning problem, the data, and any metadata (e.g., observational-level weights,
id, offset).

```{r task}
# specify the outcome and covariates
outcome <- "whz"
covars <- colnames(washb_data)[-which(names(washb_data) == outcome)]

# create the sl3 task
task <- make_sl3_Task(
  data = washb_data, covariates = covars,
  outcome = outcome
)

# examine it
task
```

**3. Specify base learner library**

Now that we have defined our machine learning problem by making the task, we are
ready to define the machine learning algorithms. Learners have properties that
indicate what features they support. Use `sl3_list_properties()` to get a list
of all properties supported by at least one learner.

```{r list_properties}
sl3_list_properties()
```
Since we have a continuous outcome, we may identify the learners that support
this outcome type with `sl3_list_learners()`.

```{r list_learners}
sl3_list_learners(c("continuous"))
```

Now that we have an idea of some learners, we can construct them by the
`make_learner` function.

```{r baselearners}
# choose base learners
lrnr_glm <- make_learner(Lrnr_glm)
lrnr_mean <- make_learner(Lrnr_mean)
lrnr_ranger <- make_learner(Lrnr_ranger)
lrnr_glmnet <- make_learner(Lrnr_glmnet)
```
In order to assemble the library of learners, we need to **"stack"** them
together. A `Stack` is just a special learner and so has the same interface as
all other learners:

```{r stack}
stack <- make_learner(
  Stack, lrnr_glm, lrnr_mean, lrnr_ranger,
  lrnr_glmnet
)
```
A `stack` combines multiple learners by training them simultaneously, so that
their predictions can be either combined or compared.

We're almost ready to super learn! Just one more necessary specification.

**4. Specify meta-learner**

We will fit a non-negative least squares meta-learner using `Lrnr_nnls`. Note
that any learner can be used as a meta-learner.

```{r metalearner}
metalearner <- make_learner(Lrnr_nnls)
```

**5. Super learn**

The Super Learner algorithm fits a meta-learner on the validation-set predictions
in a cross-validated manner, thereby avoiding overfitting. This procedure is
referred to as the *continuous* super learner. The cross-validation selector is the
*discrete* super learner.

First, we create a super learner object and then we need to **"train"** it on
our `sl3_task` object:

```{r sl_basic}
# run sl and predict on WASH
sl <- Lrnr_sl$new(learners = stack, metalearner = metalearner)
sl_fit <- sl$train(task)
```
Now that we have fit the super leaner, we are ready to obtain our predicted
values and summarized results.

```{r sl_basic-summary}
sl_preds <- sl_fit$predict()
head(sl_preds)
sl_fit$print()
```
### Extensions

We can customize learner hyperparameters to incorporate a diversity of different
settings. We can also include learners from the `SuperLearner` `R` package.

```{r extra-lrnr}
lrnr_ranger100 <- make_learner(Lrnr_ranger, num.trees = 100)
lrnr_ranger1k <- make_learner(Lrnr_ranger, num.trees = 1000)
lrnr_polymars <- Lrnr_pkg_SuperLearner$new("SL.polymars")
lrnr_gam <- Lrnr_pkg_SuperLearner$new("SL.gam")
lrnr_bayesglm <- Lrnr_pkg_SuperLearner$new("SL.bayesglm")

# let's create a new stack with these new learners
new_stack <- make_learner(
  Stack, lrnr_glm, lrnr_mean, lrnr_ranger,
  lrnr_glmnet, lrnr_ranger1k, lrnr_ranger100,
  lrnr_polymars, lrnr_gam, lrnr_bayesglm
)
```
We can also select a subset of available covariates and **"pipe"** only those
variables to the modeling algorithm. A `Pipeline` is a set of learners to be
fit sequentially, where the fit from one learner is used to define the task for
the next learner.

```{r pipe-screen}
# design a screener to reduce covariate size
screen_cor <- Lrnr_pkg_SuperLearner_screener$new("screen.corP")

# incorporate screener in the learner combo (i.e., Pipeline)
cor_pipeline <- make_learner(Pipeline, screen_cor, stack)

# put it all together again with a Stack
stack <- make_learner(Stack, cor_pipeline, new_stack)
```

Now we can super learn with this "fancy" implementation.

```{r sl_fancy}
# run sl and predict on WASH
sl <- Lrnr_sl$new(learners = new_stack, metalearner = metalearner)
sl_fit <- sl$train(task)
sl_preds <- sl_fit$predict()
sl_fit$print()
```

## Exercise
1. Create an sl3 task with the same outcome and covariate data.
2. Make a library of 10 algorithms. Customize hyperparameters for at least two
of your learners. Feel free to use learners from sl3 or SuperLearner.
3. Incorporate at least two variations of feature selection.
4. Use nonnegative least squares to fit the meta-learning step.
5. Justify this base learner library and meta-learner selection.
6. With the meta-learner and base learners, make the Super Learner and train it
on the task.
7. Print your Super Learner fit by calling print() with $.
8. Which learner is the discrete super learner?
9. Report the weights that the continuous Super Learner assigned to each learner.
10. What might be the case if the mean risk of the continuous Super Learner is
higher than the mean risk of the discrete Super Learner?

## Appendix: More advanced extensions of `sl3`

### Variable importance
```{r varimp}
get_variable_importance <- function(data, outcome, covars) {

  # create the sl3 task
  task <- make_sl3_Task(data = data, covariates = covars, outcome = outcome)

  # choose base learners
  lrnr_glm <- make_learner(Lrnr_glm)
  lrnr_mean <- make_learner(Lrnr_mean)
  lrnr_glmnet <- make_learner(Lrnr_glmnet)
  lrnr_polymars <- Lrnr_pkg_SuperLearner$new("SL.polymars")
  lrnr_gam <- Lrnr_pkg_SuperLearner$new("SL.gam")
  lrnr_bayesglm <- Lrnr_pkg_SuperLearner$new("SL.bayesglm")

  # stack them together
  stack <- make_learner(
    Stack, lrnr_glm, lrnr_mean, lrnr_glmnet,
    lrnr_polymars, lrnr_gam, lrnr_bayesglm
  )

  # choose metalearner
  metalearner <- make_learner(Lrnr_nnls)

  # run sl and predict on raw data
  sl <- Lrnr_sl$new(learners = stack, metalearner = metalearner)
  sl_fit <- sl$train(task)
  sl_preds <- sl_fit$predict()
  risk <- mean(loss_squared_error(task$Y, sl_preds))

  risk_diffs <- lapply(covars, function(x) {
    # scramble cov column and give it the same name as the raw cov col
    scrambled_col <- data.table(sample(
      unlist(data[, x, with = FALSE]),
      nrow(data)
    ))
    names(scrambled_col) <- x

    # replace raw col with scrambled col in the task
    scrambled_col_names <- task$add_columns(scrambled_col)
    scrambled_col_task <- task$next_in_chain(column_names = scrambled_col_names)

    # obtain preds on the scrambled col task
    scrambled_sl_preds <- sl_fit$predict_fold(scrambled_col_task)

    # risk on scrambled col task
    risk_scrambled <- mean(loss_squared_error(task$Y, scrambled_sl_preds))

    # calculate risk difference
    rd <- risk_scrambled - risk
    return(rd)
  })

  names(risk_diffs) <- covars
  results <- data.table(
    cov = rep(
      names(risk_diffs),
      sapply(risk_diffs, length)
    ),
    risk_diff = unlist(risk_diffs)
  )
  return(results)
}

varimp_washb <- get_variable_importance(
  data = washb_data, outcome = outcome,
  covars = covars
)
```

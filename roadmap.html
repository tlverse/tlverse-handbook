<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 4 Learning from Data: A Roadmap | Targeted Learning in R</title>
<meta name="author" content="Mark van der Laan, Jeremy Coyle, Nima Hejazi, Ivana Malenica, Rachael Phillips, Alan Hubbard">
<meta name="description" content="Learning Objectives Translate scientific questions to statistical questions. Define a statistical model based on knowledge about the scientific experiment or study that generated the data....">
<meta name="generator" content="bookdown 0.26.3 with bs4_book()">
<meta property="og:title" content="Chapter 4 Learning from Data: A Roadmap | Targeted Learning in R">
<meta property="og:type" content="book">
<meta property="og:url" content="https://tlverse.org/tlverse-handbook/roadmap.html">
<meta property="og:description" content="Learning Objectives Translate scientific questions to statistical questions. Define a statistical model based on knowledge about the scientific experiment or study that generated the data....">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 4 Learning from Data: A Roadmap | Targeted Learning in R">
<meta name="twitter:site" content="@tlverse">
<meta name="twitter:description" content="Learning Objectives Translate scientific questions to statistical questions. Define a statistical model based on knowledge about the scientific experiment or study that generated the data....">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet">
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>
</head>
<body>
<span class="math inline">
    \(\DeclareMathOperator{\expit}{expit}\)
    \(\DeclareMathOperator{\logit}{logit}\)
    \(\DeclareMathOperator*{\argmin}{\arg\!\min}\)
    \(\newcommand{\indep}{\perp\!\!\!\perp}\)
    \(\newcommand{\coloneqq}{\mathrel{=}}\)
    \(\newcommand{\R}{\mathbb{R}}\)
    \(\newcommand{\E}{\mathbb{E}}\)
    \(\newcommand{\M}{\mathcal{M}}\)
    \(\renewcommand{\P}{\mathbb{P}}\)
    \(\newcommand{\I}{\mathbb{I}}\)
    \(\newcommand{\1}{\mathbbm{1}}\)
    </span>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="css/style.css">
<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="Causal Data Science with the tlverse Software Ecosystem">Targeted Learning in R</a>:
        <small class="text-muted">Causal Data Science with the tlverse Software Ecosystem</small>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="introduction.html">Introduction</a></li>
<li class="book-part">Part 1: Preliminaries</li>
<li><a class="" href="tlverse.html"><span class="header-section-number">1</span> About the tlverse</a></li>
<li><a class="" href="setup.html"><span class="header-section-number">2</span> Software Setup</a></li>
<li><a class="" href="example-datasets.html"><span class="header-section-number">3</span> Example Datasets</a></li>
<li class="book-part">Part 2: Foundations</li>
<li><a class="active" href="roadmap.html"><span class="header-section-number">4</span> Learning from Data: A Roadmap</a></li>
<li><a class="" href="origami.html"><span class="header-section-number">5</span> Cross-validation</a></li>
<li><a class="" href="sl3.html"><span class="header-section-number">6</span> Super Learning</a></li>
<li><a class="" href="tmle3.html"><span class="header-section-number">7</span> The TMLE Framework</a></li>
<li class="book-part">Part 3: Advanced Topics</li>
<li><a class="" href="dynamic-and-optimal-individualized-treatment-regimes.html"><span class="header-section-number">8</span> Dynamic and Optimal Individualized Treatment Regimes</a></li>
<li><a class="" href="shift.html"><span class="header-section-number">9</span> Stochastic Treatment Regimes</a></li>
<li><a class="" href="causal-mediation-analysis.html"><span class="header-section-number">10</span> Causal Mediation Analysis</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/tlverse/tlverse-handbook">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="roadmap" class="section level1" number="4">
<h1>
<span class="header-section-number">4</span> Learning from Data: A Roadmap<a class="anchor" aria-label="anchor" href="#roadmap"><i class="fas fa-link"></i></a>
</h1>
<div class="infobox tlverse">
<div class="center">
<p><strong>Learning Objectives</strong></p>
</div>
<ol style="list-style-type: decimal">
<li>Translate scientific questions to statistical questions.</li>
<li>Define a statistical model based on knowledge about the scientific experiment
or study that generated the data.</li>
<li>Identify a causal parameter as a function of the observed data distribution.</li>
<li>Explain the following statistical and causal assumptions alongside their
implications: independent and identically distributed (i.i.d.), consistency,
no unmeasured confounding, interference, positivity.</li>
</ol>
</div>
<div id="introduction-1" class="section level2 unnumbered">
<h2>Introduction<a class="anchor" aria-label="anchor" href="#introduction-1"><i class="fas fa-link"></i></a>
</h2>
<p>The roadmap of statistical learning is concerned with the process of translating
real-world scientific questions to mathematical formalisms
necessary for formulating relevant statistical inference problems.
This involves viewing data as a random variable (complete with its own
underlying probability distribution), incorporating scientific knowledge into
the choice of statistical model, selecting a statistical target parameter that
represents an answer to the scientific question of interest, and developing
efficient estimators of the statistical estimand.</p>
</div>
<div id="roadmap" class="section level2" number="4.1">
<h2>
<span class="header-section-number">4.1</span> The Roadmap<a class="anchor" aria-label="anchor" href="#roadmap"><i class="fas fa-link"></i></a>
</h2>
<p>The roadmap is a six-stage process:</p>
<ol style="list-style-type: decimal">
<li>Define the data as a random variable with a probability distribution, <span class="math inline">\(O \sim P_0\)</span>
</li>
<li>Specify the statistical model <span class="math inline">\(\M\)</span> realistically, such that <span class="math inline">\(P_0 \in \M\)</span>
</li>
<li>Translate the scientific question of interest into a statistical target
parameter <span class="math inline">\(\Psi\)</span> and establish the target population</li>
<li>Choose an estimator <span class="math inline">\(\hat{\Psi}\)</span> for <span class="math inline">\(\Psi\)</span> under realistic <span class="math inline">\(\M\)</span>
</li>
<li>Construct a measure of uncertainty for the estimate <span class="math inline">\(\hat{\Psi}(P_n)\)</span>
</li>
<li>Make substantive conclusion</li>
</ol>
<div id="data-a-random-variable-with-a-probability-distribution-o-sim-p_0" class="section level3 unnumbered">
<h3>(1) Data: A random variable with a probability distribution, <span class="math inline">\(O \sim P_0\)</span><a class="anchor" aria-label="anchor" href="#data-a-random-variable-with-a-probability-distribution-o-sim-p_0"><i class="fas fa-link"></i></a>
</h3>
<p>The dataset we are confronted with is the collection of the results of a
scientific (or natural) experiment. We can view the data as a <em>random variable</em>;
that is, if the same experiment were to be repeated, we should expect to
see a different realization of the data generated by the same underlying law
governing the experiment in question. In particular, if the experiment were
repeated many times, the underlying probability distribution generating the
data, <span class="math inline">\(P_0\)</span>, would be revealed. The observed data on a single unit, <span class="math inline">\(O\)</span>,
may be thought of as being drawn from this probability distribution <span class="math inline">\(P_0\)</span>. Most
often, we have <span class="math inline">\(n\)</span> <em>independent and identically distributed</em> (i.i.d.)
observations of the random variable <span class="math inline">\(O\)</span> in our dataset. Then, the observed data
is the collection
<span class="math inline">\(O_1, \ldots, O_n\)</span>, where the subscripts denote the individual observational
units. While not all data are i.i.d., this is certainly the most common case in
applied data analysis. There are a number of techniques for handling non-i.i.d.
data, including establishing conditional independence, such that
conditional on some variable (e.g., subject ID for repeated measures data) the
i.i.d. assumption holds, and incorporating inferential corrections for
repeated or clustered observations, to name but a few.</p>
<div id="the-empirical-probability-measure-p_n" class="section level4 unnumbered">
<h4>The empirical probability measure, <span class="math inline">\(P_n\)</span><a class="anchor" aria-label="anchor" href="#the-empirical-probability-measure-p_n"><i class="fas fa-link"></i></a>
</h4>
<p>With <span class="math inline">\(n\)</span> i.i.d. observations in hand, we can define an empirical probability
measure, <span class="math inline">\(P_n\)</span>. The empirical probability measure is an approximation of the
true probability measure, <span class="math inline">\(P_0\)</span>, allowing us to learn from the observed data.
For example, we can define the empirical probability measure of a set of
variables, say <span class="math inline">\(W\)</span>, to be the proportion of observations that belong in <span class="math inline">\(W\)</span>.
That is,
<span class="math display">\[\begin{equation*}
  P_n(W) = \frac{1}{n}\sum_{i=1}^{n} \I(O_i \in W)
\end{equation*}\]</span></p>
<p>In order to understand the scope for learning from a particular dataset, we
next need to ask <em>“What do we know about the process that led to the data’s
generation?”</em> This brings us on to Step 2.</p>
</div>
</div>
<div id="defining-the-statistical-model-m-such-that-p_0-in-m" class="section level3 unnumbered">
<h3>(2) Defining the statistical model <span class="math inline">\(\M\)</span> such that <span class="math inline">\(P_0 \in \M\)</span><a class="anchor" aria-label="anchor" href="#defining-the-statistical-model-m-such-that-p_0-in-m"><i class="fas fa-link"></i></a>
</h3>
<p>The statistical model <span class="math inline">\(\M\)</span> is the set of all possible probability
distributions that could describe the process by which our observed data have
been generated, appropriately constrained by background scientific knowledge.
Often, <span class="math inline">\(\M\)</span> is necessarily very large (i.e., non-parametric), reflecting the fact
that statistical knowledge about <span class="math inline">\(P_0\)</span> is limited.</p>
<p>If <span class="math inline">\(P_0\)</span> is described by a finite number of parameters, then the statistical
model is referred to as <em>parametric</em>. Such an assumption is made, for example,
by the proposition that <span class="math inline">\(O\)</span> has a Normal distribution with mean <span class="math inline">\(\mu\)</span>
and variance <span class="math inline">\(\sigma^2\)</span>. More generally, a parametric model may be defined as</p>
<p><span class="math display">\[\begin{equation*}
  \M(\theta) = \{P_{\theta} : \theta \in \R^d \},
\end{equation*}\]</span>
which describes a constrained statistical model consisting of all distributions
<span class="math inline">\(P_{\theta}\)</span> that are indexed by some finite, <span class="math inline">\(d\)</span>-dimensional parameter <span class="math inline">\(\theta\)</span>.</p>
<p>The assumption that <span class="math inline">\(P_0\)</span> has a specific, parametric form is made quite
commonly. Unfortunately, this is even the case when such
assumptions are not supported by domain knowledge about the data-generating
process. This
practice of oversimplification in the current, and traditional, culture of
statistical data analysis typically complicates or entirely thwarts any attempt
to reliably answer the scientific question at hand. Why, you ask? Consider how
much knowledge one must have to <em>know</em> (beyond a shadow of a doubt) that the
data-generating distribution underlying a given dataset is, in fact, governed by
just two parameters, as is the case with the ubiquitously
relied upon Normal distribution. Similarly, main terms Cox proportional hazards,
logistic regression, and linear models imply a highly constrained statistical
model, and if any of the assumptions are unwarranted then there will be bias in
their result (except when treatment is randomized). The philosophy used to justify
parametric assumptions is rooted in misinterpretations of the often-quoted
saying of George Box, that “All models are wrong but some are useful”, which has
been irresponsibly used to encourage the data analyst to make arbitrary modeling
choices. However, when one makes such unfounded assumptions, it is more likely
that <span class="math inline">\(\M\)</span> does not contain <span class="math inline">\(P_0\)</span>, in which case the statistical model
is said to be misspecified. Statistical model misspecification introduces a
bias that leads to misleading, unrealiable results and inference.</p>
<p>The result of unwarranted assumptions and oversimplifications is a practice of
statistical data science in which starkly disparate answers to the same
scientific problem emerge. Practically, this is owed to the application of distinct
statistical techniques under differing modeling decisions and assumptions made
(but not communicated well) by different data analysts. Even in the nascent days
of statistical data analysis, it was recognized that it is “far
better [to develop] an approximate answer to the right question…than an exact
answer to the wrong question, which can always be made precise”
<span class="citation">(<a href="references.html#ref-tukey1962future" role="doc-biblioref">Tukey 1962</a>)</span>, though traditional statistics failed to heed this advice for
a number of decades <span class="citation">(<a href="references.html#ref-donoho2017fifty" role="doc-biblioref">Donoho 2017</a>)</span>. The roadmap avoids
this bias by defining the statistical model through a representation of the true
data-generating distribution underlying the observed data. The ultimate goal is
to formulate the statistical estimation problem <em>precisely</em> (up to the
constraints imposed by available scientific knowledge), so that one
can then tailor the estimation procedure to the motivating scientific problem.</p>
<p>It is crucial that the domain scientist(s) have absolute
clarity about what is <em>actually known</em> about the process/experiment that
generated the data, and that this is communicated to data scientists with as
much detail as possible. This knowledge is rarely ground truth
itself, but instead comes in the form of scientific conventions, accepted
hypotheses, and operational assumptions.
<!--
rp: This is vague. What's an example of scientific conventions, example of 
accepted hypotheses, and example of operational assumptions that would be 
incorporated in M? 
-->
It is then the data scientist’s responsibility to translate the domain knowledge
into statistical knowledge about <span class="math inline">\(P_0\)</span>, and then to define the statistical
model <span class="math inline">\(\M\)</span> so that it respects what is known about <span class="math inline">\(P_0\)</span> and makes no further
restrictions. In this manner, we can ensure that <span class="math inline">\(P_0\)</span> is contained in <span class="math inline">\(\M\)</span>,
which we refer to generally as defining a <em>realistic</em> statistical model <span class="math inline">\(\M\)</span>.</p>
<p>Defining <span class="math inline">\(\M\)</span> realistically requires a shift in the paradigm of statistical problem
solving. Instead of considering the methods/software one is familiar with and
then trying to solve most problems with that toolbox, one must obtain a deep
understanding of the experiment and scientific question first and then formulate
a plan for learning from the data in a way that respects this. This requires
statisticians to have not only solid methodological and
theoretical foundations, but good communication skills, as
several meetings with domain experts are typically required to review details
of the study, possibly refine of the question of interest, translate technical
details, and interpret the findings in a way that is statistically correct and
agreeable with non-statistician domain experts. Unfortunately, communication
between statisticians and non-statistician researchers is often fraught with
misinterpretation. This is to be expected, as each have their own expertise,
but proper communication about the underlying science and the motivating study
can help to ensure each have appropriate context for a given statistical data
analysis. The roadmap provides a principled mechanism for learning from data
realistically, so that what is learned from the data represents a reliable and
reproducible approximation of the answer to the scientific question of interest.
As the roadmap provides a rigorous method for translating scientific knowledge and
questions into a statistical framework that can be used to learn from data, it
is an invaluable tool to guide communication between statisticians and
non-statistician domain scientists. This brings us to our next step in the
roadmap, <em>“What are we trying to learn from the data?”</em></p>
</div>
<div id="the-statistical-target-parameter-psi-and-statistical-estimand-psi_0" class="section level3 unnumbered">
<h3>(3) The statistical target parameter <span class="math inline">\(\Psi\)</span> and statistical estimand <span class="math inline">\(\psi_0\)</span><a class="anchor" aria-label="anchor" href="#the-statistical-target-parameter-psi-and-statistical-estimand-psi_0"><i class="fas fa-link"></i></a>
</h3>
<p>The statistical target parameter,
<span class="math inline">\(\Psi\)</span>, is defined as a mapping from the
statistical model, <span class="math inline">\(\M\)</span>, to the parameter space. Usually, the parameter
space is a real number (but not necessarily so), in which case we can
formally define the target parameter as the mapping <span class="math inline">\(\Psi: \M \rightarrow \R\)</span>.
The statistical estimand may be seen as a representation of the quantity that
we wish to learn from the data, the answer to a well-specified — often causal —
question of interest about a particular target population. In contrast to
ordinary statistical estimands, causal estimands require an extra set of
assumptions to allow for their <em>identification from the observed data</em>. Based
on causal models <span class="citation">(<a href="references.html#ref-pearl2009causality" role="doc-biblioref">Pearl 2009</a>; <a href="references.html#ref-hernan2022causal" role="doc-biblioref">Hernán and Robins 2022</a>)</span>, identification
assumptions are untestable and must be justified through a combination of
knowledge about the system under study or the process by which the experiment
was conducted. These assumptions are described in greater detail in the
following section on <a href="roadmap.html#causal">causal target parameters</a>.</p>
<p>For a simple example, consider a dataset containing observations of a survival
time on every adult, for which our question of interest is “What’s the
probability that an adult lives longer than five years?” We have,</p>
<p><span class="math display">\[\begin{equation*}
  \psi_0 = \Psi(P_0) = \E_{P_0}(O &gt; 5) = \int_5^{\infty} dP_0(o).
\end{equation*}\]</span></p>
<p>This answer to this question is the <strong>statistical estimand, <span class="math inline">\(\Psi(P_0)=\psi_0\)</span></strong>,
which is the quantity we wish to learn from the data. As discussed above,
back-and-forth communication between domain scientists and statisticians is
often required to define <span class="math inline">\(\M\)</span> realistically, and to finalize <span class="math inline">\(\Psi\)</span> and the
target population such that the question is supported in the data.
For instance, say we are interested in learning the average effect of a headache
medication for treating migraines in adults and we learn that no one with high
blood pressure can receive the medication. In the next meeting with domain
scientists, we might suggest that the target population be modified to adults
without high blood pressure or ask a question involving a dynamic treatment
such that within <span class="math inline">\(\Psi\)</span> adults with high blood pressure are never considered
as individuals who could receive treatment. Once we have defined
<span class="math inline">\(O\)</span>, <span class="math inline">\(\M\)</span> realistically and <span class="math inline">\(\Psi\)</span>, we have formally defined the statistical
estimation problem. Next comes Step 4: “<em>How do we learn from the data the
approximate answer to the question of interest?</em>”</p>
</div>
<div id="the-estimator-hatpsi-and-estimate-psi_n" class="section level3 unnumbered">
<h3>(4) The estimator <span class="math inline">\(\hat{\Psi}\)</span> and estimate <span class="math inline">\(\psi_n\)</span><a class="anchor" aria-label="anchor" href="#the-estimator-hatpsi-and-estimate-psi_n"><i class="fas fa-link"></i></a>
</h3>
<p>To obtain a good approximation of the statistical estimand, we need an estimator
$, an <em>a priori</em>-specified algorithm defined as a mapping from the set
of the set of possible empirical distributions <span class="math inline">\(P_n\)</span> (which live in a
non-parametric statistical model <span class="math inline">\(\M_{NP}\)</span>) to the parameter space for our
target parameter of interest: <span class="math inline">\(\hat{\Psi} : \M_{NP} \rightarrow \R\)</span>. In other
words, $ is a function that takes as input the observed data, a
realization of <span class="math inline">\(P_n\)</span>, and then outputs a value in the parameter space. Where
the estimator may be seen as an operator that maps the observed data’s
corresponding empirical distribution to a value in the parameter space, the
numerical output produced by such a function is the
<strong>estimate, <span class="math inline">\(\hat{\Psi}(P_n)=\psi_n\)</span></strong>. Thus, <span class="math inline">\(\psi_n\)</span> is an
element of the parameter space as informed by the empirical probability
distribution <span class="math inline">\(P_n\)</span> of the observed data <span class="math inline">\(O_1, \ldots, O_n\)</span>. If we plug in a
realization of <span class="math inline">\(P_n\)</span> (based on a sample size <span class="math inline">\(n\)</span> of the random variable <span class="math inline">\(O\)</span>), we
get back an estimate <span class="math inline">\(\psi_n\)</span> of the true parameter value <span class="math inline">\(\psi_0\)</span>.
<!--
nh: idk how i feel about the above; it's a bit repetitive and feels imprecise
nh: also, the use of \hat{} over \Psi feels redundant and misleading, since it
would seem that \hat{\Psi} implies an approximate mapping, i.e., \hat{\Psi} is
_not_ \Psi, which makes it sound as though we're answering an approximate
version of the question represented by the mapping \Psi
rp: Yes, with \hat{\Psi} we are approximating \Psi, \hat{\Psi} is an estimator of \Psi
-->
As we have motivated in step 2, it is imperative to consider realistic
statistical models for estimation. Therefore, flexible estimators that allow
for parts of the data-generating process to be unrestricted are necessary.
Semiparametric statistical theory and empirical process theory provide a
framework for constructing, benchmarking, and understanding the behavior of
estimators that depend on flexible estimation strategies in realistic
statistical models. In general, desirable properties of an estimator are that
it is regular asymptotically linear (RAL) and efficient, thereby admitting a
Normal limit distribution that has minimal variance. Substitution/plug-in RAL
estimators are also advantageous: they are guaranteed to remain within the
bounds of <span class="math inline">\(\M\)</span> and, relative to estimators that are not plug-in, have improved
bias and variance in finite samples. In-depth discussion of the theory and
these properties are available in the literature <span class="citation">(e.g., <a href="references.html#ref-kennedy2016semiparametric" role="doc-biblioref">Kennedy 2016</a>; <a href="references.html#ref-vdl2011targeted" role="doc-biblioref">van der Laan and Rose 2011</a>)</span>. We review a few key concepts in the following step.</p>
<p>In order to quantify the uncertainty in our estimate of the target parameter,
part of the process of conducting statistical inference, an understanding of the
sampling distribution of our estimator is necessary. This brings us to Step
5: “<em>How confident should we be in our statistical answer to the scientific
question?</em>”</p>
</div>
<div id="a-measure-of-uncertainty-for-the-estimate-psi_n" class="section level3 unnumbered">
<h3>(5) A measure of uncertainty for the estimate <span class="math inline">\(\psi_n\)</span><a class="anchor" aria-label="anchor" href="#a-measure-of-uncertainty-for-the-estimate-psi_n"><i class="fas fa-link"></i></a>
</h3>
<p>Since the estimator <span class="math inline">\(\hat{\Psi}\)</span> is a function of the empirical distribution
<span class="math inline">\(P_n\)</span>, the estimator itself is a random variable with a sampling distribution.
Therefore, if we repeat the experiment of drawing <span class="math inline">\(n\)</span> observations, we would
every time end up with a different realization of our estimate. The hypothetical
distribution of these estimates is the sampling distribution of the estimator.</p>
<p>A primary goal in the construction of estimators is to be able to derive their
asymptotic sampling distribution through a theoretical analysis involving
empirical process theory. In this regard, an important property of the
estimators on which we focus is their asymptotic linearity. In particular,
asymptotic linearity states that the difference between the estimator and the
target parameter (i.e., the truth) can be represented, asymptotically, as an
average of i.i.d. random variables plus an asymptotically negligible remainder
term:</p>
<p><span class="math display">\[\begin{equation*}
  \hat{\Psi}(P_n) - \Psi(P_0) = \frac{1}{n} \sum_{i=1}^n IC(P_0)(O_i) +
    o_p(n^{-1/2}),
\end{equation*}\]</span>
where the influence curve (IC) is a function of the observed data <span class="math inline">\(O\)</span> but the
function itself is defined by the underlying data-generating distribution <span class="math inline">\(P_0\)</span>.
Based on this asymptotic approximation, the Central Limit Theorem can be used to
show</p>
<p><span class="math display">\[\begin{equation*}
  \sqrt{n} \left(\hat{\Psi}(P_n) - \Psi(P_0)\right) \sim N(0, \sigma^2_{IC}),
\end{equation*}\]</span>
where <span class="math inline">\(\sigma^2_{IC}\)</span> is the variance of <span class="math inline">\(IC(P_0)(O)\)</span>. Given an estimate of
<span class="math inline">\(\sigma^2_{IC}\)</span>, it is then possible to construct classic, <em>asymptotically
accurate</em> Wald-type confidence intervals (CIs) and hypothesis tests. For
example, a standard <span class="math inline">\((1 - \alpha)\)</span> CI takes the form</p>
<p><span class="math display">\[\begin{equation*}
  \psi_n \pm Z \frac{\hat{\sigma}_{IC}}{\sqrt{n}} \ ,
\end{equation*}\]</span>
where <span class="math inline">\(Z\)</span> is the <span class="math inline">\((1 - \alpha / 2)^\text{th}\)</span>
quantile of the standard Normal distribution. Following convention, we will
often be interested in constructing 95% two-tailed CIs, corresponding to
probability mass <span class="math inline">\(\alpha/2 = 0.025\)</span> in each tail of the limit distribution;
thus, we will take <span class="math inline">\(Z \approx 1.96\)</span> as the quantile.</p>
<p>Steps (1)–(5) of the roadmap define the statistical analysis plan, all of which
can be done before any data is revealed. The last step of the roadmap involves
interpreting the results obtained in step (4) and (5) and therefore requires
the data to be analyzed; however, any additional analysis that may take place
as part of step (6) can be pre-specified as well. This final step of the
roadmap addresses the question, “<em>what is the interpretation and robustness
of the study’s findings, and what conclusions can be drawn from them?</em>”</p>
</div>
<div id="make-substantive-conclusion" class="section level3 unnumbered">
<h3>(6) Make substantive conclusion<a class="anchor" aria-label="anchor" href="#make-substantive-conclusion"><i class="fas fa-link"></i></a>
</h3>
<p>Making the substantive conclusion involves interpreting the study findings. It
also provides an opportunity to ask follow-up questions that might be
addressed later and/or discuss issues that can inform future studies.
Statistical estimands <span class="math inline">\(\psi_0\)</span> can have statistical (noncausal) and causal
interpretations. Both are often of interest and can be provided. The target
population should be clearly mentioned in the interpretation, regardless of
whether it’s a purely statistical or causal interpretation, to curtail
extrapolation of results.</p>
<p>The major distinction between statistical versus causal interpretations is that
the latter relies on untestable so-called “identifiability” assumptions. In the
following section, we review these
assumptions one-by-one. Here, we focus on the interpretation and robustness of
the study findings with respect to them. Specifically, causal target parameters
cannot be estimated from observed data without additional identifiability
assumptions, and so the validity of a result’s causal interpretation
hinges on them holding in the data. The more these assumptions do not hold, the
larger the <em>causal gap</em>, the difference between the statistical estimand and
the causal estimand. In a perfect randomized control trial with no loss to
follow-up, the causal gap will be zero as the statistical and causal estimands
are equivalent. In <span class="citation">Dı́az and Laan (<a href="references.html#ref-diaz2013sensitivity" role="doc-biblioref">2013</a>)</span>, a non-parametric sensitivity analysis
for assessing the impact of a hypothesized causal gaps on estimates and
inference is proposed. In <span class="citation">Gruber, Phillips, Lee, Ho, et al. (<a href="references.html#ref-gruber2022targeted" role="doc-biblioref">2022</a>)</span> and <span class="citation">Gruber, Phillips, Lee, Concato, et al. (<a href="references.html#ref-gruber2022evaluating" role="doc-biblioref">2022</a>)</span>,
there are example implementations of the methods proposed in
<span class="citation">Dı́az and Laan (<a href="references.html#ref-diaz2013sensitivity" role="doc-biblioref">2013</a>)</span>; in particular, the difference between
adjusted and unadjusted effect estimates is used to define a range of
possible causal gaps relative to this difference. If the question of interest
is causal, then such a model-free sensitivity analysis (possibly as a
complement to other sensitivity analyses) is recommended to assess the
robustness of the study findings.</p>
</div>
</div>
<div id="roadmap-summary" class="section level2" number="4.2">
<h2>
<span class="header-section-number">4.2</span> Summary of the Roadmap<a class="anchor" aria-label="anchor" href="#roadmap-summary"><i class="fas fa-link"></i></a>
</h2>
<p>Data collected across <span class="math inline">\(n\)</span> i.i.d. units, <span class="math inline">\(O_1, \ldots, O_n\)</span>, may be viewed as a
collection of random variables arising from the
same underlying probability distribution <span class="math inline">\(\P_0\)</span>. This is expressed by denoting
the collection of data as being generated as <span class="math inline">\(O_1, \ldots, O_n \sim P_0\)</span>.
Domain knowledge about the experiment that generated the data (e.g., if the
treatment was randomized, if the treatment decision or loss to follow-up
depended on a subset of covariates, time ordering in which the variables were
added to the data) is translated by the statistician / data scientist to define
the statistical model <span class="math inline">\(\M\)</span>, a postulated space of candidate probability
distributions that is supposed to contain <span class="math inline">\(P_0\)</span>. In particular, the roadmap
emphasizes the critical role of defining <span class="math inline">\(\M\)</span> such that <span class="math inline">\(P_0\)</span> is guaranteed to
be encapsulated by it, <span class="math inline">\(P_0 \in \M\)</span>. By only limiting <span class="math inline">\(\M\)</span> based on domain
knowledge about the experiment (i.e., reality) — opposed to constraining it
unrealistically (e.g., assuming a restrictive functional form, like a main
terms linear/logistic model, describes <span class="math inline">\(P_0\)</span>) — it can be ensured that
<span class="math inline">\(P_0 \in \M\)</span>, and we refer to this as defining a realistic statistical model.
Often, knowledge that can be used to constrain <span class="math inline">\(\M\)</span> is very limited, and so
<span class="math inline">\(\M\)</span> must be very large to define it such that <span class="math inline">\(P_0 \in \M\)</span>; hence, realistic
statistical models are often termed semi- or non-parametric, since they are
too large to be indexed by a finite-dimensional set of parameters. Necessarily,
our statistical query must begin with, “What are we trying to learn from the data?”,
a question whose answer is captured by the statistical target parameter, <span class="math inline">\(\Psi\)</span>,
a function defined by the true data-generating distribution <span class="math inline">\(P_0\)</span>, that
maps <span class="math inline">\(\M\)</span> into the statistical estimand, <span class="math inline">\(\psi_0\)</span>. At this stage, the
statistical estimation problem is formally defined, allowing for the use of
statistical theory to guide the construction of estimators, which are
algorithms that approximate the answer the question of interest by learning
from the data. Desirable properties of an estimator are that it is unbiased,
efficient, plug-in, and robust in finite samples. If the question of interest
is causal, then a model-free sensitivity analysis is recommended to assess the
robustness of the study’s findings under various hypothesized causal gaps.</p>
</div>
<div id="causal" class="section level2" number="4.3">
<h2>
<span class="header-section-number">4.3</span> Causal Target Parameters<a class="anchor" aria-label="anchor" href="#causal"><i class="fas fa-link"></i></a>
</h2>
<p>In many cases, we are interested in problems that ask questions regarding the
<em>causal effect</em> of an intervention, whether an assigned treatment (e.g., a
prescribed drug) or a “naturally occurring” exposure (e.g., pollution from
a nearby factory), on a future outcome of interest. These causal
effects may be defined as summaries of the population of interest (e.g.,
population mean of a particular outcome) under contrasting interventions
(e.g., comparing the treated to the untreated condition).
For example, a causal effect could be
defined as the mean difference of a disease outcome between two
<em>causal contrasts</em>, counterfactual cases in which the study population were set
to uniformly experience low pollution levels for some pollutant, and in which
the same population were set to uniformly experience high levels of the same pollutant.</p>
<p>There are different ways of operationalizing the theoretical experiments that
generate the counterfactual data necessary for describing such causal contrasts
of interest. We could simply assume that the counterfactual outcomes exist in
theory for all treatment contrasts of interest <span class="citation">(<a href="references.html#ref-neyman1938contribution" role="doc-biblioref">Neyman 1938</a>; <a href="references.html#ref-rubin2005causal" role="doc-biblioref">Rubin 2005</a>; <a href="references.html#ref-imbens2015causal" role="doc-biblioref">Imbens and Rubin 2015</a>)</span>, which may be encoded in so-called “science
tables”. Alternatively, we could consider interventions on structural causal models (SCMs)
<span class="citation">(<a href="references.html#ref-pearl1995causal" role="doc-biblioref">Pearl 1995</a>, <a href="references.html#ref-pearl2009causality" role="doc-biblioref">2009</a>)</span>, which may be represented by directed
acyclic graphs (DAGs). Both frameworks allow for the known or hypothesized set
of relationships between variables in the system under study to be encoded and
mathematically formalized.</p>
<div id="the-causal-model" class="section level3 unnumbered">
<h3>The Causal Model<a class="anchor" aria-label="anchor" href="#the-causal-model"><i class="fas fa-link"></i></a>
</h3>
<p>Throughout, we will focus on the use of DAGs and SCMs for the description of
causal parameters. Estimators of statistical parameters that correspond, under
standard but untestable <em>identifiability</em> assumptions, to these causal
parameters are introduced below. DAGs are a particularly useful tool for
visually expressing what we know about the causal relations among variables in
the system under study. Ignoring exogenous <span class="math inline">\(U\)</span> terms (explained below), we
assume the following ordering of the variables that compose the observed data
<span class="math inline">\(O\)</span>. We demonstrate the construction of a DAG below using <code>DAGitty</code>
<span class="citation">(<a href="references.html#ref-textor2011dagitty" role="doc-biblioref">Textor, Hardt, and Knüppel 2011</a>)</span>:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.dagitty.net">dagitty</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/malcolmbarrett/ggdag">ggdag</a></span><span class="op">)</span>

<span class="co"># make DAG by specifying dependence structure</span>
<span class="va">dag</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/dagitty/man/dagitty.html">dagitty</a></span><span class="op">(</span>
  <span class="st">"dag {
    W -&gt; A
    W -&gt; Y
    A -&gt; Y
    W -&gt; A -&gt; Y
  }"</span>
<span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/pkg/dagitty/man/VariableStatus.html">exposures</a></span><span class="op">(</span><span class="va">dag</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"A"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/pkg/dagitty/man/VariableStatus.html">outcomes</a></span><span class="op">(</span><span class="va">dag</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Y"</span><span class="op">)</span>
<span class="va">tidy_dag</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/ggdag/man/tidy_dagitty.html">tidy_dagitty</a></span><span class="op">(</span><span class="va">dag</span><span class="op">)</span>

<span class="co"># visualize DAG</span>
<span class="fu"><a href="https://rdrr.io/pkg/ggdag/man/ggdag.html">ggdag</a></span><span class="op">(</span><span class="va">tidy_dag</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://rdrr.io/pkg/ggdag/man/theme_dag_blank.html">theme_dag</a></span><span class="op">(</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="04-roadmap_files/figure-html/simple-DAG-1.png" width="60%" style="display: block; margin: auto;"></div>
<p>While DAGs like the above provide a convenient means by which to express the
causal relations between variables, these same causal relations can be
equivalently represented by an SCM:
<span class="math display">\[\begin{align*}
  W &amp;= f_W(U_W) \\
  A &amp;= f_A(W, U_A) \\
  Y &amp;= f_Y(W, A, U_Y),
\end{align*}\]</span>
where the <span class="math inline">\(f\)</span>’s are unspecified deterministic functions that generate the
corresponding random variables as a function of the variable’s “parents” (i.e.,
upstream nodes with arrows into the given random variable) in the DAG, and the
unobserved, exogenous error terms (i.e., the <span class="math inline">\(U\)</span>’s). An SCM may be thought of as
a representation of the algorithm that produces the data, <span class="math inline">\(O\)</span>, in the population
of interest. Much of statistics and data science is devoted to discovering
properties of this system of equations (e.g., estimation of the functional form
<span class="math inline">\(f_Y\)</span> governing the outcome variable <span class="math inline">\(Y\)</span>).</p>
<!--
where $U_W$, $U_A$, and $U_Y$ represent the unmeasured exogenous background
characteristics that influence the value of each variable. In the NPSEM, $f_W$,
$f_A$ and $f_Y$ denote that each variable (for $W$, $A$ and $Y$, respectively)
is a function of its parents and unmeasured background characteristics, but
note that there is no imposition of any particular functional constraints(e.g.,
linear, logit-linear, only one interaction, etc.). For this reason, they are
called non-parametric structural equation models (NPSEMs). The DAG and set of
nonparametric structural equations represent exactly the same information and
so may be used interchangeably.
-->
<p>The first hypothetical experiment we will consider is assigning exposure to the
entire population and observing the outcome, and then withholding exposure to
the same population and observing the outcome. This corresponds to a comparison
of the outcome distribution in the population under two distinct interventions:</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(A\)</span> is set to <span class="math inline">\(1\)</span> for all individuals, and</li>
<li>
<span class="math inline">\(A\)</span> is set to <span class="math inline">\(0\)</span> for all individuals.</li>
</ol>
<p>These interventions may be thought of as operations that imply changes
to the structural equations in the system under study. For the case <span class="math inline">\(A = 1\)</span>, we
have
<span class="math display">\[\begin{align*}
  W &amp;= f_W(U_W) \\
  A &amp;= 1 \\
  Y(1) &amp;= f_Y(W, 1, U_Y) \ ,
\end{align*}\]</span>
while, for the case <span class="math inline">\(A=0\)</span>,
<span class="math display">\[\begin{align*}
  W &amp;= f_W(U_W) \\
  A &amp;= 0 \\
  Y(0) &amp;= f_Y(W, 0, U_Y) \ .
\end{align*}\]</span></p>
<p>In these equations, <span class="math inline">\(A\)</span> is no longer a function of <span class="math inline">\(W\)</span> because the intervention
on the system set <span class="math inline">\(A\)</span> deterministically
to one of the values <span class="math inline">\(1\)</span> or <span class="math inline">\(0\)</span> consistent with the intervention performed. The
new symbols <span class="math inline">\(Y(1)\)</span> and <span class="math inline">\(Y(0)\)</span> indicate the values the outcome variable would
take in the population of interest when it is generated by removing the
contribution of <span class="math inline">\(A\)</span> to <span class="math inline">\(f_Y\)</span> and instead setting <span class="math inline">\(A\)</span> to the values <span class="math inline">\(1\)</span> and <span class="math inline">\(0\)</span>,
respectively. The variables <span class="math inline">\(Y(1)\)</span> and <span class="math inline">\(Y(0)\)</span> are often called counterfactuals
(since they arise from interventions that run contrary to fact) and are, in
other frameworks, called the <em>potential outcomes</em> of <span class="math inline">\(Y\)</span>
[<span class="citation">Neyman (<a href="references.html#ref-neyman1938contribution" role="doc-biblioref">1938</a>)</span>; rubin2005causal; imbens2015causal]. The difference in
the counterfactual means of the outcome under these two interventions defines a
well known causal parameter that is most often called the “average treatment
effect” (ATE) and is denoted</p>
<p><span class="math display" id="eq:ate">\[\begin{equation}
  ATE = \E_X(Y(1) - Y(0)),
  \tag{4.1}
\end{equation}\]</span>
where <span class="math inline">\(\E_X\)</span> is the mean under the theoretical (unobservable) full data
<span class="math inline">\(X = (W, Y(1), Y(0))\)</span>. Note that the full data structure <span class="math inline">\(X\)</span> is, by its very
definition, unobservable since one can never observe both of <span class="math inline">\(Y(1)\)</span> and <span class="math inline">\(Y(0)\)</span>
for the same observational unit.</p>
<p>We can define much more complicated interventions on SCMs, such as
interventions based upon dynamic rules (which assign particular interventions
based on a function of the covariates <span class="math inline">\(W\)</span>), stochastic rules (which can even
account for the natural value of <span class="math inline">\(A\)</span> observed in the absence of the
intervention), and much more. Each results in a different target causal
parameter and entails different identifiability assumptions discussed below.</p>
</div>
<div id="identifiability" class="section level3 unnumbered">
<h3>Identifiability<a class="anchor" aria-label="anchor" href="#identifiability"><i class="fas fa-link"></i></a>
</h3>
<p>Since we can never observe both <span class="math inline">\(Y(0)\)</span> (the counterfactual outcome when <span class="math inline">\(A=0\)</span>)
and <span class="math inline">\(Y(1)\)</span> (similarly, the counterfactual outcome when <span class="math inline">\(A=1\)</span>), we cannot
estimate the quantity in Equation <a href="roadmap.html#eq:ate">(4.1)</a> directly. This is called the
<em>Fundamental Problem of Causal Inference</em> <span class="citation">(<a href="references.html#ref-holland1986statistics" role="doc-biblioref">Holland 1986</a>)</span>. Thus, one of
the primary activities in causal inference is to <em>identify</em> the assumptions
necessary to express causal quantities of interest as functions of the
data-generating distribution of the observed data. To do this, we must make
assumptions under which such quantities may be estimated from the observed data
<span class="math inline">\(O \sim P_0\)</span> and its corresponding data-generating distribution <span class="math inline">\(P_0\)</span>.
Fortunately, given the causal model specified in the SCM above, we can, with a
handful of untestable assumptions, estimate the ATE from observational data.
These assumptions may be summarized as follows.</p>
<div class="definition">
<p><span id="def:consist-ass" class="definition"><strong>Definition 4.1  (Consistency) </strong></span>The outcome for unit <span class="math inline">\(i\)</span> is <span class="math inline">\(Y_i(a)\)</span> whenever <span class="math inline">\(A_i = a\)</span>, which may be thought of
as “no other versions of treatment” or “no side effects of treatment.”</p>
</div>
<div class="definition">
<p><span id="def:interf-ass" class="definition"><strong>Definition 4.2  (No Interference) </strong></span>The outcome for unit <span class="math inline">\(i\)</span>, <span class="math inline">\(Y_i\)</span>, cannot be affected by the exposure of unit <span class="math inline">\(j\)</span>,
<span class="math inline">\(A_j\)</span>, for all <span class="math inline">\(i \neq j\)</span>.</p>
</div>
<div class="definition">
<p><span id="def:noconf-ass" class="definition"><strong>Definition 4.3  (No Unmeasured Confounding) </strong></span><span class="math inline">\(A \perp Y(a) \mid W\)</span> for all <span class="math inline">\(a \in \mathcal{A}\)</span>, which states that the
potential outcomes <span class="math inline">\((Y(a) : a \in \mathcal{A})\)</span> arise independently from
exposure status <span class="math inline">\(A\)</span>, conditional on the observed covariates <span class="math inline">\(W\)</span>. This is the
analog of the <em>randomization</em> assumption in data arising from natural
experiments, ensuring that the effect of <span class="math inline">\(A\)</span> on <span class="math inline">\(Y\)</span> can be disentangled from
that of <span class="math inline">\(W\)</span> on <span class="math inline">\(Y\)</span>, even though <span class="math inline">\(W\)</span> affects both.</p>
</div>
<div class="definition">
<p><span id="def:posit-ass" class="definition"><strong>Definition 4.4  (Positivity (or Overlap)) </strong></span>All observed units, across strata defined by <span class="math inline">\(W\)</span>, must have a bounded
(non-deterministic) probability of receiving treatment – that is,
<span class="math inline">\(\epsilon &lt; \P(A = a \mid W) &lt; 1 - \epsilon\)</span> for all <span class="math inline">\(a\)</span> and <span class="math inline">\(W\)</span> and for some
<span class="math inline">\(\epsilon &gt; 0\)</span>)  .</p>
</div>
<p>Technically speaking, only the latter two of these assumptions are necessary
when working within the SCM framework, as the first two are implied properties
of an SCM for i.i.d. data (if you’re really curious, see this commentary of
<span class="citation">Pearl (<a href="references.html#ref-pearl2010brief" role="doc-biblioref">2010</a>)</span> for an extended philosophical discussion). We introduce all four
identification assumptions because they are most often considered together, and
all four are necessary when working within the potential outcomes framework.</p>
<p>Given these assumptions, the ATE may be re-written as a function of <span class="math inline">\(P_0\)</span> –
specifically</p>
<p><span class="math display" id="eq:estimand">\[\begin{align}
  \psi_{\text{ATE}} &amp;= \E_0(Y(1) - Y(0)) \\ \nonumber
                    &amp;= \E_0 \left(\E_0[Y \mid A = 1, W] -
                       \E_0[Y \mid A = 0, W]\right) \ .
  \tag{4.2}
\end{align}\]</span>
In words, the ATE is the mean difference in the predicted outcome values for
each subject, under the contrast of treatment conditions (<span class="math inline">\(A = 0\)</span> versus <span class="math inline">\(A = 1\)</span>), in the population (when averaged over all observations). Thus, a parameter
of a theoretical complete (or “full”) data distribution can be represented as an
estimand of the observed data distribution. Significantly, there is nothing
about the representation in Equation <a href="roadmap.html#eq:estimand">(4.2)</a> that requires
parameteric assumptions; thus, the regression functions on the right hand side
may be estimated without restrictive assumptions about their underlying
functional forms. With different parameters, there will be potentially
different identifiability assumptions and the resulting estimands can be
functions of different components of <span class="math inline">\(P_0\)</span>. We discuss several more complex
estimands in subsequent chapters.</p>
</div>
<div id="missingness" class="section level3 unnumbered">
<h3>Missingness<a class="anchor" aria-label="anchor" href="#missingness"><i class="fas fa-link"></i></a>
</h3>
<p>TODO: Ask Mark to add note about this.</p>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="example-datasets.html"><span class="header-section-number">3</span> Example Datasets</a></div>
<div class="next"><a href="origami.html"><span class="header-section-number">5</span> Cross-validation</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#roadmap"><span class="header-section-number">4</span> Learning from Data: A Roadmap</a></li>
<li><a class="nav-link" href="#introduction-1">Introduction</a></li>
<li>
<a class="nav-link" href="#roadmap"><span class="header-section-number">4.1</span> The Roadmap</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#data-a-random-variable-with-a-probability-distribution-o-sim-p_0">(1) Data: A random variable with a probability distribution, \(O \sim P_0\)</a></li>
<li><a class="nav-link" href="#defining-the-statistical-model-m-such-that-p_0-in-m">(2) Defining the statistical model \(\M\) such that \(P_0 \in \M\)</a></li>
<li><a class="nav-link" href="#the-statistical-target-parameter-psi-and-statistical-estimand-psi_0">(3) The statistical target parameter \(\Psi\) and statistical estimand \(\psi_0\)</a></li>
<li><a class="nav-link" href="#the-estimator-hatpsi-and-estimate-psi_n">(4) The estimator \(\hat{\Psi}\) and estimate \(\psi_n\)</a></li>
<li><a class="nav-link" href="#a-measure-of-uncertainty-for-the-estimate-psi_n">(5) A measure of uncertainty for the estimate \(\psi_n\)</a></li>
<li><a class="nav-link" href="#make-substantive-conclusion">(6) Make substantive conclusion</a></li>
</ul>
</li>
<li><a class="nav-link" href="#roadmap-summary"><span class="header-section-number">4.2</span> Summary of the Roadmap</a></li>
<li>
<a class="nav-link" href="#causal"><span class="header-section-number">4.3</span> Causal Target Parameters</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#the-causal-model">The Causal Model</a></li>
<li><a class="nav-link" href="#identifiability">Identifiability</a></li>
<li><a class="nav-link" href="#missingness">Missingness</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/tlverse/tlverse-handbook/blob/master/04-roadmap.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/tlverse/tlverse-handbook/edit/master/04-roadmap.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Targeted Learning in R</strong>: Causal Data Science with the tlverse Software Ecosystem" was written by Mark van der Laan, Jeremy Coyle, Nima Hejazi, Ivana Malenica, Rachael Phillips, Alan Hubbard. It was last built on May 16, 2023.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>

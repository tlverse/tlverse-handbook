\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[12pt, krantz2,]{krantz}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Targeted Learning in R},
            pdfauthor={Mark van der Laan, Jeremy Coyle, Nima Hejazi, Ivana Malenica, Rachael Phillips, Alan Hubbard},
            colorlinks=true,
            linkcolor=Maroon,
            citecolor=Blue,
            urlcolor=Blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.61,0.61,0.61}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.14,0.14,0.14}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.43,0.43,0.43}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{longtable}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage[inline]{enumitem}
\usepackage{float}
\usepackage{graphicx}
\usepackage[round]{natbib}
\usepackage{geometry}
\usepackage{tikz}
\usepackage[english]{babel}
\usepackage{longtable}
\usepackage{color}
\usepackage{mathtools,bm,amssymb,amsmath,amsthm}
\usepackage{multirow}
\usepackage[titletoc,title]{appendix}
\usepackage{authblk}
\usepackage{setspace}
\usepackage{dsfont}
\PassOptionsToPackage{utf8x}{inputenc}
\usepackage[OT1]{fontenc}
\usepackage[bf,singlelinecheck=off]{caption}
\usepackage{refcount}
\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}
\urlstyle{tt}

\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

\usepackage{makeidx}
\makeindex

\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\newtheorem*{remark}{Remark}
\newtheorem{theorem}{Theorem}
\AtEndDocument{\refstepcounter{theorem}\label{finalthm}}
{
  \theoremstyle{definition}
  \newtheorem{assumption}{}
}
{
  \theoremstyle{definition}
  \newtheorem{assumptioniden}{}
}
{
  \theoremstyle{definition}
  \newtheorem{example}{Example}[section]
}
\DeclareMathOperator{\opt}{opt}
\DeclareMathOperator{\dr}{IF}
\newcommand{\hopt}{\hat h_{\opt}}
\newcommand{\supp}{\mathop{\mathrm{supp}}}
\renewcommand\theassumptioniden{{A}\arabic{assumptioniden}}
\renewcommand\theassumption{{C}\arabic{assumption}}
\renewcommand\theexample{\arabic{example}}

\newtheorem{lemma}{Lemma}
\newtheorem{coro}{Corollary}
\newtheorem{definition}{Definition}
\DeclareMathOperator{\bern}{Bern}
\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\Rem}{Rem}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\I}{\mathbb{I}}
\newcommand{\1}{\mathbbm{1}}
\DeclareMathOperator{\expit}{expit}
\DeclareMathOperator{\logit}{logit}
\newcommand{\indep}{\mbox{$\perp\!\!\!\perp$}}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}

% setting bookdown frontmatter option
\frontmatter
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Targeted Learning in R}
\providecommand{\subtitle}[1]{}
\subtitle{Causal Data Science with the tlverse Software Ecosystem}
\author{Mark van der Laan, Jeremy Coyle, Nima Hejazi, Ivana Malenica, Rachael Phillips, Alan Hubbard}
\date{April 19, 2021}

\begin{document}
\maketitle

% you may need to leave a few empty pages before the dedication page

%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage
\thispagestyle{empty}

\begin{center}
%\includegraphics{images/dedication.pdf}
\end{center}

\setlength{\abovedisplayskip}{-5pt}
\setlength{\abovedisplayshortskip}{-5pt}

% setting bookdown mainmatter (e.g., arabic numerals for page numbering)
\mainmatter

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{3}
\tableofcontents
}
\listoftables
\listoffigures
\hypertarget{about-this-book}{%
\section*{About this book}\label{about-this-book}}


\emph{Targeted Learning in \texttt{R}: Causal Data Science with the \texttt{tlverse} Software
Ecosystem} is an open source, reproducible electronic handbook for applying the
Targeted Learning methodology in practice using the \href{https://github.com/tlverse}{\texttt{tlverse} software
ecosystem}. This work is currently in an early draft
phase and is available to facilitate input from the community. To view or
contribute to the available content, consider visiting the \href{https://github.com/tlverse/tlverse-handbook}{GitHub
repository}.

\hypertarget{outline}{%
\subsection{Outline}\label{outline}}

The contents of this handbook are meant to serve as a reference guide for
applied research as well as materials that can be taught in a series of short
courses focused on the applications of Targeted Learning. Each section
introduces a set of distinct causal questions, motivated by a case study,
alongside statistical methodology and software for assessing the causal claim of
interest. The (evolving) set of materials includes

\begin{itemize}
\tightlist
\item
  Motivation: \href{https://senseaboutscienceusa.org/super-learning-and-the-revolution-in-knowledge/}{Why we need a statistical
  revolution}
\item
  The Roadmap and introductory case study: the WASH Beneifits data
\item
  Introduction to the \href{https://tlverse.org}{\texttt{tlverse} software
  ecosystem}
\item
  Cross-validation with the \href{https://github.com/tlverse/origami}{\texttt{origami}}
  package
\item
  Ensemble machine learning with the
  \href{https://github.com/tlverse/sl3}{\texttt{sl3}} package
\item
  Targeted learning for causal inference with the
  \href{https://github.com/tlverse/tmle3}{\texttt{tmle3}} package
\item
  Optimal treatments regimes and the
  \href{https://github.com/tlverse/tmle3mopttx}{\texttt{tmle3mopttx}} package
\item
  Stochastic treatment regimes and the
  \href{https://github.com/tlverse/tmle3shift}{\texttt{tmle3shift}} package
\item
  Causal mediation analysis with the
  \href{https://github.com/tlverse/tmle3mediate}{\texttt{tmle3mediate}} package
  (\emph{work in progress})
\item
  \emph{Coda}: \href{https://senseaboutscienceusa.org/super-learning-and-the-revolution-in-knowledge/}{Why we need a statistical
  revolution}
\end{itemize}

\hypertarget{what-this-book-is-not}{%
\subsection*{What this book is not}\label{what-this-book-is-not}}


The focus of this work is \textbf{not} on providing in-depth technical descriptions
of current statistical methodology or recent advancements. Instead, the goal is
to convey key details of state-of-the-art techniques in an manner that is both
clear and complete, without burdening the reader with extraneous information.
We hope that the presentations herein will serve as references for researchers
-- methodologists and domain specialists alike -- that empower them to deploy
the central tools of Targeted Learning in an efficient manner. For technical
details and in-depth descriptions of both classical theory and recent advances
in the field of Targeted Learning, the interested reader is invited to consult
\citet{vdl2011targeted} and/or \citet{vdl2018targeted} as appropriate. The primary literature
in statistical causal inference, machine learning, and non/semiparametric theory
include many of the most recent advances in Targeted Learning and related areas.

\hypertarget{about-the-authors}{%
\subsection*{About the authors}\label{about-the-authors}}


\hypertarget{mark-van-der-laan}{%
\subsubsection*{Mark van der Laan}\label{mark-van-der-laan}}


Mark van der Laan, PhD, is Professor of Biostatistics and Statistics at UC
Berkeley. His research interests include statistical methods in computational
biology, survival analysis, censored data, adaptive designs, targeted maximum
likelihood estimation, causal inference, data-adaptive loss-based learning, and
multiple testing. His research group developed loss-based super learning in
semiparametric models, based on cross-validation, as a generic optimal tool for
the estimation of infinite-dimensional parameters, such as nonparametric density
estimation and prediction with both censored and uncensored data. Building on
this work, his research group developed targeted maximum likelihood estimation
for a target parameter of the data-generating distribution in arbitrary
semiparametric and nonparametric models, as a generic optimal methodology for
statistical and causal inference. Most recently, Mark's group has focused in
part on the development of a centralized, principled set of software tools for
targeted learning, the \texttt{tlverse}.

\hypertarget{jeremy-coyle}{%
\subsubsection*{Jeremy Coyle}\label{jeremy-coyle}}


Jeremy Coyle, PhD, is a consulting data scientist and statistical programmer,
currently leading the software development effort that has produced the
\texttt{tlverse} ecosystem of R packages and related software tools. Jeremy earned his
PhD in Biostatistics from UC Berkeley in 2016, primarily under the supervision
of Alan Hubbard.

\hypertarget{nima-hejazi}{%
\subsubsection*{Nima Hejazi}\label{nima-hejazi}}


Nima Hejazi is a PhD candidate in biostatistics, working under the collaborative
direction of Mark van der Laan and Alan Hubbard. Nima is affiliated with UC
Berkeley's Center for Computational Biology and NIH Biomedical Big Data training
program, as well as with the Fred Hutchinson Cancer Research Center. Previously,
he earned an MA in Biostatistics and a BA (with majors in Molecular and Cell
Biology, Psychology, and Public Health), both at UC Berkeley. His research
interests fall at the intersection of causal inference and machine learning,
drawing on ideas from non/semi-parametric estimation in large, flexible
statistical models to develop efficient and robust statistical procedures for
evaluating complex target estimands in observational and randomized studies.
Particular areas of current emphasis include mediation/path analysis,
outcome-dependent sampling designs, targeted loss-based estimation, and vaccine
efficacy trials. Nima is also passionate about statistical computing and open
source software development for applied statistics.

\hypertarget{ivana-malenica}{%
\subsubsection*{Ivana Malenica}\label{ivana-malenica}}


Ivana Malenica is a PhD student in biostatistics advised by Mark van der Laan.
Ivana is currently a fellow at the Berkeley Institute for Data Science, after
serving as a NIH Biomedical Big Data and Freeport-McMoRan Genomic Engine fellow.
She earned her Master's in Biostatistics and Bachelor's in Mathematics, and
spent some time at the Translational Genomics Research Institute. Very broadly,
her research interests span non/semi-parametric theory, probability theory,
machine learning, causal inference and high-dimensional statistics. Most of her
current work involves complex dependent settings (dependence through time and
network) and adaptive sequential designs.

\hypertarget{rachael-phillips}{%
\subsubsection*{Rachael Phillips}\label{rachael-phillips}}


Rachael Phillips is a PhD student in biostatistics, advised by Alan Hubbard and
Mark van der Laan. She has an MA in Biostatistics, BS in Biology, and BA in
Mathematics. A student of targeted learning and causal inference; her research
integrates personalized medicine, human-computer interaction, experimental
design, and regulatory policy.

\hypertarget{alan-hubbard}{%
\subsubsection*{Alan Hubbard}\label{alan-hubbard}}


Alan Hubbard is Professor of Biostatistics, former head of the Division of
Biostatistics at UC Berkeley, and head of data analytics core at UC Berkeley's
SuperFund research program. His current research interests include causal
inference, variable importance analysis, statistical machine learning,
estimation of and inference for data-adaptive statistical target parameters, and
targeted minimum loss-based estimation. Research in his group is generally
motivated by applications to problems in computational biology, epidemiology,
and precision medicine.

\hypertarget{learn}{%
\subsection{Learning resources}\label{learn}}

To effectively utilize this handbook, the reader need not be a fully trained
statistician to begin understanding and applying these methods. However, it is
highly recommended for the reader to have an understanding of basic statistical
concepts such as confounding, probability distributions, confidence intervals,
hypothesis tests, and regression. Advanced knowledge of mathematical statistics
may be useful but is not necessary. Familiarity with the \texttt{R} programming
language will be essential. We also recommend an understanding of introductory
causal inference.

For learning the \texttt{R} programming language we recommend the following (free)
introductory resources:

\begin{itemize}
\tightlist
\item
  \href{http://swcarpentry.github.io/r-novice-inflammation/}{Software Carpentry's \emph{Programming with
  \texttt{R}}}
\item
  \href{http://swcarpentry.github.io/r-novice-gapminder/}{Software Carpentry's \emph{\texttt{R} for Reproducible Scientific
  Analysis}}
\item
  \href{https://r4ds.had.co.nz}{Garret Grolemund and Hadley Wickham's \emph{\texttt{R} for Data
  Science}}
\end{itemize}

For a general introduction to causal inference, we recommend

\begin{itemize}
\tightlist
\item
  \href{https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/}{Miguel A. HernÃ¡n and James M. Robins' \emph{Causal Inference: What If},
  2021}
\item
  \href{https://www.coursera.org/learn/crash-course-in-causality}{Jason A. Roy's \emph{A Crash Course in Causality: Inferring Causal Effects from
  Observational Data} on
  Coursera}
\end{itemize}

\hypertarget{setup}{%
\subsection{Setup instructions}\label{setup}}

\hypertarget{r-and-rstudio}{%
\subsubsection{R and RStudio}\label{r-and-rstudio}}

\textbf{R} and \textbf{RStudio} are separate downloads and installations. R is the
underlying statistical computing environment. RStudio is a graphical integrated
development environment (IDE) that makes using R much easier and more
interactive. You need to install R before you install RStudio.

\hypertarget{windows}{%
\paragraph{Windows}\label{windows}}

\hypertarget{if-you-already-have-r-and-rstudio-installed}{%
\subparagraph{If you already have R and RStudio installed}\label{if-you-already-have-r-and-rstudio-installed}}

\begin{itemize}
\tightlist
\item
  Open RStudio, and click on ``Help'' \textgreater{} ``Check for updates''. If a new version is
  available, quit RStudio, and download the latest version for RStudio.
\item
  To check which version of R you are using, start RStudio and the first thing
  that appears in the console indicates the version of R you are
  running. Alternatively, you can type \texttt{sessionInfo()}, which will also display
  which version of R you are running. Go on the \href{https://cran.r-project.org/bin/windows/base/}{CRAN
  website} and check whether a
  more recent version is available. If so, please download and install it. You
  can \href{https://cran.r-project.org/bin/windows/base/rw-FAQ.html\#How-do-I-UNinstall-R_003f}{check here}
  for more information on how to remove old versions from your system if you
  wish to do so.
\end{itemize}

\hypertarget{if-you-dont-have-r-and-rstudio-installed}{%
\subparagraph{If you don't have R and RStudio installed}\label{if-you-dont-have-r-and-rstudio-installed}}

\begin{itemize}
\tightlist
\item
  Download R from
  the \href{http://cran.r-project.org/bin/windows/base/release.htm}{CRAN website}.
\item
  Run the \texttt{.exe} file that was just downloaded
\item
  Go to the \href{https://www.rstudio.com/products/rstudio/download/\#download}{RStudio download page}
\item
  Under \emph{Installers} select \textbf{RStudio x.yy.zzz - Windows
  XP/Vista/7/8} (where x, y, and z represent version numbers)
\item
  Double click the file to install it
\item
  Once it's installed, open RStudio to make sure it works and you don't get any
  error messages.
\end{itemize}

\hypertarget{macos-mac-os-x}{%
\paragraph{macOS / Mac OS X}\label{macos-mac-os-x}}

\hypertarget{if-you-already-have-r-and-rstudio-installed-1}{%
\subparagraph{If you already have R and RStudio installed}\label{if-you-already-have-r-and-rstudio-installed-1}}

\begin{itemize}
\tightlist
\item
  Open RStudio, and click on ``Help'' \textgreater{} ``Check for updates''. If a new version is
  available, quit RStudio, and download the latest version for RStudio.
\item
  To check the version of R you are using, start RStudio and the first thing
  that appears on the terminal indicates the version of R you are running.
  Alternatively, you can type \texttt{sessionInfo()}, which will also display which
  version of R you are running. Go on the \href{https://cran.r-project.org/bin/macosx/}{CRAN
  website} and check whether a more
  recent version is available. If so, please download and install it.
\end{itemize}

\hypertarget{if-you-dont-have-r-and-rstudio-installed-1}{%
\subparagraph{If you don't have R and RStudio installed}\label{if-you-dont-have-r-and-rstudio-installed-1}}

\begin{itemize}
\tightlist
\item
  Download R from
  the \href{http://cran.r-project.org/bin/macosx}{CRAN website}.
\item
  Select the \texttt{.pkg} file for the latest R version
\item
  Double click on the downloaded file to install R
\item
  It is also a good idea to install \href{https://www.xquartz.org/}{XQuartz} (needed
  by some packages)
\item
  Go to the \href{https://www.rstudio.com/products/rstudio/download/\#download}{RStudio download
  page}
\item
  Under \emph{Installers} select \textbf{RStudio x.yy.zzz - Mac OS X 10.6+ (64-bit)}
  (where x, y, and z represent version numbers)
\item
  Double click the file to install RStudio
\item
  Once it's installed, open RStudio to make sure it works and you don't get any
  error messages.
\end{itemize}

\hypertarget{linux}{%
\paragraph{Linux}\label{linux}}

\begin{itemize}
\tightlist
\item
  Follow the instructions for your distribution
  from \href{https://cloud.r-project.org/bin/linux}{CRAN}, they provide information
  to get the most recent version of R for common distributions. For most
  distributions, you could use your package manager (e.g., for Debian/Ubuntu run
  \texttt{sudo\ apt-get\ install\ r-base}, and for Fedora \texttt{sudo\ yum\ install\ R}), but we
  don't recommend this approach as the versions provided by this are
  usually out of date. In any case, make sure you have at least R 3.3.1.
\item
  Go to the \href{https://www.rstudio.com/products/rstudio/download/\#download}{RStudio download
  page}
\item
  Under \emph{Installers} select the version that matches your distribution, and
  install it with your preferred method (e.g., with Debian/Ubuntu \texttt{sudo\ dpkg\ -i\ rstudio-x.yy.zzz-amd64.deb} at the terminal).
\item
  Once it's installed, open RStudio to make sure it works and you don't get any
  error messages.
\end{itemize}

These setup instructions are adapted from those written for \href{http://www.datacarpentry.org/R-ecology-lesson/}{Data Carpentry: R
for Data Analysis and Visualization of Ecological
Data}.

\hypertarget{robust}{%
\section{Robust Statistics and Reproducible Science}\label{robust}}

\begin{quote}
``One enemy of robust science is our humanity --- our appetite for
being right, and our tendency to find patterns in noise, to see supporting
evidence for what we already believe is true, and to ignore the facts that do
not fit.''

--- \citet{naturenews_2015}
\end{quote}

Scientific research is at a unique point in history. The need to improve rigor
and reproducibility in our field is greater than ever; corroboration moves
science forward, yet there is a growing alarm about results that cannot be
reproduced and that report false discoveries \citep{baker2016there}. Consequences of
not meeting this need will result in further decline in the rate of scientific
progression, the reputation of the sciences, and the public's trust in its
findings \citep{munafo2017manifesto, naturenews2_2015}.

\begin{quote}
``The key question we want to answer when seeing the results of any scientific
study is whether we can trust the data analysis.''

--- \citet{peng2015reproducibility}
\end{quote}

Unfortunately, at its current state the culture of data analysis and statistics
actually enables human bias through improper model selection. All hypothesis
tests and estimators are derived from statistical models, so to obtain valid
estimates and inference it is critical that the statistical model contains the
process that generated the data. Perhaps treatment was randomized or only
depended on a small number of baseline covariates; this knowledge should and
can be incorporated in the model. Alternatively, maybe the data is
observational, and there is no knowledge about the data-generating process (DGP).
If this is the case, then the statistical model should contain \emph{all} data
distributions. In practice; however, models are not selected based on knowledge
of the DGP, instead models are often selected based on (1) the p-values they
yield, (2) their convenience of implementation, and/or (3) an analysts loyalty
to a particular model. This practice of ``cargo-cult statistics --- the
ritualistic miming of statistics rather than conscientious practice,''
\citep{stark2018cargo} is characterized by arbitrary modeling choices, even though
these choices often result in different answers to the same research question.
That is, ``increasingly often, {[}statistics{]} is used instead to aid and
abet weak science, a role it can perform well when used mechanically or
ritually,'' as opposed to its original purpose of safeguarding against weak
science \citep{stark2018cargo}. This presents a fundamental drive behind the epidemic
of false findings that scientific research is suffering from \citep{vdl2014entering}.

\begin{quote}
``We suggest that the weak statistical understanding is probably due to
inadequate''statistics lite" education. This approach does not build up
appropriate mathematical fundamentals and does not provide scientifically
rigorous introduction into statistics. Hence, students' knowledge may remain
imprecise, patchy, and prone to serious misunderstandings. What this approach
achieves, however, is providing students with false confidence of being able
to use inferential tools whereas they usually only interpret the p-value
provided by black box statistical software. While this educational problem
remains unaddressed, poor statistical practices will prevail regardless of
what procedures and measures may be favored and/or banned by editorials."

--- \citet{szucs2017null}
\end{quote}

Our team at The University of California, Berkeley, is uniquely positioned to
provide such an education. Spearheaded by Professor Mark van der Laan, and
spreading rapidly by many of his students and colleagues who have greatly
enriched the field, the aptly named ``Targeted Learning'' methodology targets the
scientific question at hand and is counter to the current culture of
``convenience statistics'' which opens the door to biased estimation, misleading
results, and false discoveries. Targeted Learning restores the fundamentals that
formalized the field of statistics, such as the that facts that a statistical
model represents real knowledge about the experiment that generated the data,
and a target parameter represents what we are seeking to learn from the data as
a feature of the distribution that generated it \citep{vdl2014entering}. In this way,
Targeted Learning defines a truth and establishes a principled standard for
estimation, thereby inhibiting these all-too-human biases (e.g., hindsight bias,
confirmation bias, and outcome bias) from infiltrating analysis.

\begin{quote}
``The key for effective classical {[}statistical{]} inference is to have
well-defined questions and an analysis plan that tests those questions.''

--- \citet{nosek2018preregistration}
\end{quote}

The objective for this handbook is to provide training to students, researchers,
industry professionals, faculty in science, public health, statistics, and other
fields to empower them with the necessary knowledge and skills to utilize the
sound methodology of Targeted Learning --- a technique that provides tailored
pre-specified machines for answering queries, so that each data analysis is
completely reproducible, and estimators are efficient, minimally biased, and
provide formal statistical inference.

Just as the conscientious use of modern statistical methodology is necessary to
ensure that scientific practice thrives, it remains critical to acknowledge the
role that robust software plays in allowing practitioners direct access to
published results. We recall that ``an article\ldots{}in a scientific publication is
not the scholarship itself, it is merely advertising of the scholarship. The
actual scholarship is the complete software development environment and the
complete set of instructions which generated the figures,'' thus making the
availability and adoption of robust statistical software key to enhancing the
transparency that is an inherent aspect of science \citep{buckheit1995wavelab}.

For a statistical methodology to be readily accessible in practice, it is
crucial that it is accompanied by robust user-friendly software
\citep{pullenayegum2016knowledge, stromberg2004write}. The \texttt{tlverse} software
ecosystem was developed to fulfill this need for the Targeted Learning
methodology. Not only does this software facilitate computationally reproducible
and efficient analyses, it is also a tool for Targeted Learning education since
its workflow mirrors that of the methodology. In particular, the \texttt{tlverse}
paradigm does not focus on implementing a specific estimator or a small set of
related estimators. Instead, the focus is on exposing the statistical framework
of Targeted Learning itself --- all \texttt{R} packages in the \texttt{tlverse} ecosystem
directly model the key objects defined in the mathematical and theoretical
framework of Targeted Learning. What's more, the \texttt{tlverse} \texttt{R} packages share a
core set of design principles centered on extensibility, allowing for them to be
used in conjunction with each other and built upon one other in a cohesive
fashion. For an introduction to Targeted Learning, we recommend the \href{https://arxiv.org/abs/2006.07333}{recent
review paper} from \citet{coyle2021targeted}.

In this handbook, the reader will embark on a journey through the \texttt{tlverse}
ecosystem. Guided by \texttt{R} programming exercises, case studies, and
intuitive explanation readers will build a toolbox for applying the Targeted
Learning statistical methodology, which will translate to real-world causal
inference analyses. Some preliminaries are required prior to this learning
endeavor -- we have made available a list of \protect\hyperlink{learn}{recommended learning
resources}.

\hypertarget{intro}{%
\section{The Roadmap for Targeted Learning}\label{intro}}

\hypertarget{learning-objectives}{%
\subsection*{Learning Objectives}\label{learning-objectives}}


By the end of this chapter you will be able to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Translate scientific questions to statistical questions.
\item
  Define a statistical model based on the knowledge of the experiment that
  generated the data.
\item
  Identify a causal parameter as a function of the observed data distribution.
\item
  Explain the following causal and statistical assumptions and their
  implications: i.i.d., consistency, interference, positivity, SUTVA.
\end{enumerate}

\hypertarget{introduction}{%
\subsection*{Introduction}\label{introduction}}


The roadmap of statistical learning is concerned with the translation from
real-world data applications to a mathematical and statistical formulation of
the relevant estimation problem. This involves data as a random variable having
a probability distribution, scientific knowledge represented by a statistical
model, a statistical target parameter representing an answer to the question of
interest, and the notion of an estimator and sampling distribution of the
estimator.

\hypertarget{roadmap}{%
\subsection{The Roadmap}\label{roadmap}}

Following the roadmap is a process of five stages.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Data as a random variable with a probability distribution, \(O \sim P_0\).
\item
  The statistical model \(\M\) such that \(P_0 \in \M\).
\item
  The statistical target parameter \(\Psi\) and estimand \(\Psi(P_0)\).
\item
  The estimator \(\hat{\Psi}\) and estimate \(\hat{\Psi}(P_n)\).
\item
  A measure of uncertainty for the estimate \(\hat{\Psi}(P_n)\).
\end{enumerate}

\hypertarget{data-a-random-variable-with-a-probability-distribution-o-sim-p_0}{%
\subsubsection*{\texorpdfstring{(1) Data: A random variable with a probability distribution, \(O \sim P_0\)}{(1) Data: A random variable with a probability distribution, O \textbackslash{}sim P\_0}}\label{data-a-random-variable-with-a-probability-distribution-o-sim-p_0}}


The data set we're confronted with is the result of an experiment and we can
view the data as a random variable, \(O\), because if we repeat the experiment
we would have a different realization of this experiment. In particular, if we
repeat the experiment many times we could learn the probability distribution,
\(P_0\), of our data. So, the observed data \(O\) with probability distribution
\(P_0\) are \(n\) independent identically distributed (i.i.d.) observations of the
random variable \(O; O_1, \ldots, O_n\). Note that while not all data are i.i.d.,
there are ways to handle non-i.i.d. data, such as establishing conditional
independence, stratifying data to create sets of identically distributed data,
etc. It is crucial that researchers be absolutely clear about what they actually
know about the data-generating distribution for a given problem of interest.
Unfortunately, communication between statisticians and researchers is often
fraught with misinterpretation. The roadmap provides a mechanism by which to
ensure clear communication between research and statistician -- it truly helps
with this communication!

\hypertarget{the-empirical-probability-measure-p_n}{%
\paragraph{\texorpdfstring{The empirical probability measure, \(P_n\)}{The empirical probability measure, P\_n}}\label{the-empirical-probability-measure-p_n}}
\addcontentsline{toc}{paragraph}{The empirical probability measure, \(P_n\)}

Once we have \(n\) of such i.i.d. observations we have an empirical probability
measure, \(P_n\). The empirical probability measure is an approximation of the
true probability measure \(P_0\), allowing us to learn from our data. For
example, we can define the empirical probability measure of a set, \(A\), to be
the proportion of observations which end up in \(A\). That is,
\begin{equation*}
  P_n(A) = \frac{1}{n}\sum_{i=1}^{n} \I(O_i \in A)
\end{equation*}

In order to start learning something, we need to ask \emph{``What do we know about the
probability distribution of the data?''} This brings us to Step 2.

\hypertarget{the-statistical-model-m-such-that-p_0-in-m}{%
\subsubsection*{\texorpdfstring{(2) The statistical model \(\M\) such that \(P_0 \in \M\)}{(2) The statistical model \textbackslash{}M such that P\_0 \textbackslash{}in \textbackslash{}M}}\label{the-statistical-model-m-such-that-p_0-in-m}}


The statistical model \(\M\) is defined by the question we asked at the end of
Step 1. It is defined as the set of possible probability distributions for our
observed data. Often \(\M\) is very large (possibly infinite-dimensional), to
reflect the fact that statistical knowledge is limited. In the case that \(\M\) is
infinite-dimensional, we deem this a nonparametric statistical model.

Alternatively, if the probability distribution of the data at hand is described
by a finite number of parameters, then the statistical model is parametric. In
this case, we subscribe to the belief that the random variable \(O\) being
observed has, for example, a normal distribution with mean \(\mu\) and variance
\(\sigma^2\). Formally, a parametric model may be defined
\begin{equation*}
  \M = \{P_{\theta} : \theta \in \R^d \}
\end{equation*}

Sadly, the assumption that the data-generating distribution has a specific,
parametric form is all too common, especially since this is a leap of faith or
an assumption made of convenience. This practice of oversimplification in the
current culture of data analysis typically derails any attempt at trying to
answer the scientific question at hand; alas, such statements as the
ever-popular quip of Box that ``All models are wrong but some are useful''
encourage the data analyst to make arbitrary choices even when such a practice
often forces starkly different answers to the same estimation problem. The
Targeted Learning paradigm does not suffer from this bias since it defines the
statistical model through a representation of the true data-generating
distribution corresponding to the observed data.

Now, on to Step 3: \emph{``What are we trying to learn from the data?''}

\hypertarget{the-statistical-target-parameter-psi-and-estimand-psip_0}{%
\subsubsection*{\texorpdfstring{(3) The statistical target parameter \(\Psi\) and estimand \(\Psi(P_0)\)}{(3) The statistical target parameter \textbackslash{}Psi and estimand \textbackslash{}Psi(P\_0)}}\label{the-statistical-target-parameter-psi-and-estimand-psip_0}}


The statistical target parameter, \(\Psi\), is defined as a mapping from the
statistical model, \(\M\), to the parameter space (i.e., a real number) \(\R\). That
is, \(\Psi: \M \rightarrow \R\). The estimand may be seen as a representation of
the quantity that we wish to learn from the data, the answer to a well-specified
(often causal) question of interest. In contrast to purely statistical
estimands, causal estimands require \emph{identification from the observed data},
based on causal models that include several untestable assumptions, described in
more detail in the section on \protect\hyperlink{causal}{causal target parameters}.

For a simple example, consider a data set which contains observations of a
survival time on every subject, for which our question of interest is ``What's
the probability that someone lives longer than five years?'' We have,
\begin{equation*}
  \Psi(P_0) = \P(O > 5)
\end{equation*}

This answer to this question is the \textbf{estimand, \(\Psi(P_0)\)}, which is the
quantity we're trying to learn from the data. Once we have defined \(O\), \(\M\) and
\(\Psi(P_0)\) we have formally defined the statistical estimation problem.

\hypertarget{the-estimator-hatpsi-and-estimate-hatpsip_n}{%
\subsubsection*{\texorpdfstring{(4) The estimator \(\hat{\Psi}\) and estimate \(\hat{\Psi}(P_n)\)}{(4) The estimator \textbackslash{}hat\{\textbackslash{}Psi\} and estimate \textbackslash{}hat\{\textbackslash{}Psi\}(P\_n)}}\label{the-estimator-hatpsi-and-estimate-hatpsip_n}}


To obtain a good approximation of the estimand, we need an estimator, an \emph{a
priori}-specified algorithm defined as a mapping from the set of possible
empirical distributions, \(P_n\), which live in a non-parametric statistical
model, \(\M_{NP}\) (\(P_n \in \M_{NP}\)), to the parameter space of the parameter of
interest. That is, \(\hat{\Psi} : \M_{NP} \rightarrow \R^d\). The estimator is a
function that takes as input the observed data, a realization of \(P_n\), and
gives as output a value in the parameter space, which is the \textbf{estimate,
\(\hat{\Psi}(P_n)\)}.

Where the estimator may be seen as an operator that maps the observed data and
corresponding empirical distribution to a value in the parameter space, the
numerical output that produced such a function is the estimate. Thus, it is an
element of the parameter space based on the empirical probability distribution
of the observed data. If we plug in a realization of \(P_n\) (based on a sample
size \(n\) of the random variable \(O\)), we get back an estimate \(\hat{\Psi}(P_n)\)
of the true parameter value \(\Psi(P_0)\).

In order to quantify the uncertainty in our estimate of the target parameter
(i.e., to construct statistical inference), an understanding of the sampling
distribution of our estimator will be necessary. This brings us to Step 5.

\hypertarget{a-measure-of-uncertainty-for-the-estimate-hatpsip_n}{%
\subsubsection*{\texorpdfstring{(5) A measure of uncertainty for the estimate \(\hat{\Psi}(P_n)\)}{(5) A measure of uncertainty for the estimate \textbackslash{}hat\{\textbackslash{}Psi\}(P\_n)}}\label{a-measure-of-uncertainty-for-the-estimate-hatpsip_n}}


Since the estimator \(\hat{\Psi}\) is a function of the empirical distribution
\(P_n\), the estimator itself is a random variable with a sampling distribution.
So, if we repeat the experiment of drawing \(n\) observations we would every time
end up with a different realization of our estimate and our estimator has a
sampling distribution. The sampling distribution of some estimators can be
theoretically validated to be approximately normally distributed by a Central
Limit Theorem (CLT).

A \textbf{Central Limit Theorem} (CLTs) is a statement regarding the convergence of
the \textbf{sampling distribution of an estimator} to a normal distribution. In
general, we will construct estimators whose limit sampling distributions may be
shown to be approximately normal distributed as sample size increases. For large
enough \(n\) we have,
\begin{equation*}
  \hat{\Psi}(P_n) \sim N \left(\Psi(P_0), \frac{\sigma^2}{n}\right),
\end{equation*}
permitting statistical inference. Now, we can proceed to quantify the
uncertainty of our chosen estimator by construction of hypothesis tests and
confidence intervals. For example, we may construct a confidence interval at
level \((1 - \alpha)\) for our estimand, \(\Psi(P_0)\):
\begin{equation*}
  \hat{\Psi}(P_n) \pm z_{1 - \frac{\alpha}{2}}
    \left(\frac{\sigma}{\sqrt{n}}\right),
\end{equation*}
where \(z_{1 - \frac{\alpha}{2}}\) is the \((1 - \frac{\alpha}{2})^\text{th}\)
quantile of the standard normal distribution. Often, we will be interested in
constructing 95\% confidence intervals, corresponding to mass \(\alpha = 0.05\) in
either tail of the limit distribution; thus, we will typically take
\(z_{1 - \frac{\alpha}{2}} \approx 1.96\).

\emph{Note:} we will typically have to estimate the standard error,
\(\frac{\sigma}{\sqrt{n}}\).

A 95\% confidence interval means that if we were to take 100 different samples
of size \(n\) and compute a 95\% confidence interval for each sample, then
approximately 95 of the 100 confidence intervals would contain the estimand,
\(\Psi(P_0)\). More practically, this means that there is a 95\% probability
that the confidence interval procedure generates intervals containing the
true estimand value (or 95\% confidence of ``covering'' the true value). That is,
any single estimated confidence interval either will contain the true estimand
or will not (also called ``coverage'').

\hypertarget{roadmap-summary}{%
\subsection{Summary of the Roadmap}\label{roadmap-summary}}

Data, \(O\), is viewed as a random variable that has a probability distribution.
We often have \(n\) units of independent identically distributed units with
probability distribution \(P_0\), such that \(O_1, \ldots, O_n \sim P_0\). We have
statistical knowledge about the experiment that generated this data. In other
words, we make a statement that the true data distribution \(P_0\) falls in a
certain set called a statistical model, \(\M\). Often these sets are very large
because statistical knowledge is very limited - hence, these statistical models
are often infinite dimensional models. Our statistical query is, ``What are we
trying to learn from the data?'' denoted by the statistical target parameter,
\(\Psi\), which maps the \(P_0\) into the estimand, \(\Psi(P_0)\). At this point the
statistical estimation problem is formally defined and now we will need
statistical theory to guide us in the construction of estimators. There's a lot
of statistical theory we will review in this course that, in particular, relies
on the Central Limit Theorem, allowing us to come up with estimators that are
approximately normally distributed and also allowing us to come with statistical
inference (i.e., confidence intervals and hypothesis tests).

\hypertarget{causal}{%
\subsection{Causal Target Parameters}\label{causal}}

In many cases, we are interested in problems that ask questions regarding the
effect of an intervention on a future outcome of interest. These questions can
be represented as causal estimands.

\hypertarget{the-causal-model}{%
\subsubsection*{The Causal Model}\label{the-causal-model}}


After formalizing the data and the statistical model, we can define a causal
model to express causal parameters of interest. Directed acyclic graphs (DAGs)
are one useful tool to express what we know about the causal relations among
variables. Ignoring exogenous \(U\) terms (explained below), we assume the
following ordering of the variables in the observed data \(O\). We do this below
using \texttt{DAGitty} \citep{textor2011dagitty}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dagitty)}
\KeywordTok{library}\NormalTok{(ggdag)}

\CommentTok{# make DAG by specifying dependence structure}
\NormalTok{dag <-}\StringTok{ }\KeywordTok{dagitty}\NormalTok{(}
  \StringTok{"dag \{}
\StringTok{    W -> A}
\StringTok{    W -> Y}
\StringTok{    A -> Y}
\StringTok{    W -> A -> Y}
\StringTok{  \}"}
\NormalTok{)}
\KeywordTok{exposures}\NormalTok{(dag) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{)}
\KeywordTok{outcomes}\NormalTok{(dag) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Y"}\NormalTok{)}
\NormalTok{tidy_dag <-}\StringTok{ }\KeywordTok{tidy_dagitty}\NormalTok{(dag)}

\CommentTok{# visualize DAG}
\KeywordTok{ggdag}\NormalTok{(tidy_dag) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{02-roadmap_files/figure-latex/simple-DAG-1} \end{center}

While directed acyclic graphs (DAGs) like above provide a convenient means by
which to visualize causal relations between variables, the same causal relations
among variables can be represented via a set of structural equations, which
define the non-parametric structural equation model (NPSEM):
\begin{align*}
  W &= f_W(U_W) \\
  A &= f_A(W, U_A) \\
  Y &= f_Y(W, A, U_Y),
\end{align*}
where \(U_W\), \(U_A\), and \(U_Y\) represent the unmeasured exogenous background
characteristics that influence the value of each variable. In the NPSEM, \(f_W\),
\(f_A\) and \(f_Y\) denote that each variable (for \(W\), \(A\) and \(Y\), respectively)
is a function of its parents and unmeasured background characteristics, but note
that there is no imposition of any particular functional constraints(e.g.,
linear, logit-linear, only one interaction, etc.). For this reason, they are
called non-parametric structural equation models (NPSEMs). The DAG and set of
nonparametric structural equations represent exactly the same information and so
may be used interchangeably.

The first hypothetical experiment we will consider is assigning exposure to the
whole population and observing the outcome, and then assigning no exposure to
the whole population and observing the outcome. On the nonparametric structural
equations, this corresponds to a comparison of the outcome distribution in the
population under two interventions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(A\) is set to \(1\) for all individuals, and
\item
  \(A\) is set to \(0\) for all individuals.
\end{enumerate}

These interventions imply two new nonparametric structural equation models. For
the case \(A = 1\), we have
\begin{align*}
  W &= f_W(U_W) \\
  A &= 1 \\
  Y(1) &= f_Y(W, 1, U_Y),
\end{align*}
and for the case \(A=0\),
\begin{align*}
  W &= f_W(U_W) \\
  A &= 0 \\
  Y(0) &= f_Y(W, 0, U_Y).
\end{align*}

In these equations, \(A\) is no longer a function of \(W\) because we have
intervened on the system, setting \(A\) deterministically to either of the values
\(1\) or \(0\). The new symbols \(Y(1)\) and \(Y(0)\) indicate the outcome variable in
our population if it were generated by the respective NPSEMs above; these are
often called \emph{counterfactuals} (since they run contrary-to-fact). The difference
between the means of the outcome under these two interventions defines a
parameter that is often called the ``average treatment effect'' (ATE), denoted
\begin{equation}
  ATE = \E_X(Y(1) - Y(0)),
  \label{eq:ate}
\end{equation}
where \(\E_X\) is the mean under the theoretical (unobserved) full data \(X = (W, Y(1), Y(0))\).

Note, we can define much more complicated interventions on NPSEM's, such as
interventions based upon rules (themselves based upon covariates), stochastic
rules, etc. and each results in a different targeted parameter and entails
different identifiability assumptions discussed below.

\hypertarget{identifiability}{%
\subsubsection*{Identifiability}\label{identifiability}}


Because we can never observe both \(Y(0)\) (the counterfactual outcome when \(A=0\))
and \(Y(1)\) (similarly, the counterfactual outcome when \(A=1\)), we cannot
estimate the quantity in Equation \eqref{eq:ate} directly. Instead, we have to
make assumptions under which this quantity may be estimated from the observed
data \(O \sim P_0\) under the data-generating distribution \(P_0\). Fortunately,
given the causal model specified in the NPSEM above, we can, with a handful of
untestable assumptions, estimate the ATE, even from observational data. These
assumptions may be summarized as follows.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The causal graph implies \(Y(a) \perp A\) for all \(a \in \mathcal{A}\), which
  is the \emph{randomization} assumption. In the case of observational data, the
  analogous assumption is \emph{strong ignorability} or \emph{no unmeasured confounding}
  \(Y(a) \perp A \mid W\) for all \(a \in \mathcal{A}\);
\item
  Although not represented in the causal graph, also required is the assumption
  of no interference between units, that is, the outcome for unit \(i\) \(Y_i\) is
  not affected by exposure for unit \(j\) \(A_j\) unless \(i=j\);
\item
  \emph{Consistency} of the treatment mechanism is also required, i.e., the outcome
  for unit \(i\) is \(Y_i(a)\) whenever \(A_i = a\), an assumption also known as ``no
  other versions of treatment'';
\item
  It is also necessary that all observed units, across strata defined by \(W\),
  have a bounded (non-deterministic) probability of receiving treatment --
  that is, \(0 < \P(A = a \mid W) < 1\) for all \(a\) and \(W\)). This assumption
  is referred to as \emph{positivity} or \emph{overlap}.
\end{enumerate}

\emph{Remark}: Together, (2) and (3), the assumptions of no interference and
consistency, respectively, are jointly referred to as the \emph{stable unit
treatment value assumption} (SUTVA).

Given these assumptions, the ATE may be re-written as a function of \(P_0\),
specifically
\begin{equation}
  ATE = \E_0(Y(1) - Y(0)) = \E_0
    \left(\E_0[Y \mid A = 1, W] - \E_0[Y \mid A = 0, W]\right).
  \label{eq:estimand}
\end{equation}
In words, the ATE is the difference in the predicted outcome values for each
subject, under the contrast of treatment conditions (\(A = 0\) versus \(A = 1\)),
in the population, averaged over all observations. Thus, a parameter of a
theoretical ``full'' data distribution can be represented as an estimand of the
observed data distribution. Significantly, there is nothing about the
representation in Equation \eqref{eq:estimand} that requires parameteric
assumptions; thus, the regressions on the right hand side may be estimated
freely with machine learning. With different parameters, there will be
potentially different identifiability assumptions and the resulting estimands
can be functions of different components of \(P_0\). We discuss several more
complex estimands in later sections of this handbook.

\hypertarget{tlverse}{%
\section{\texorpdfstring{Welcome to the \texttt{tlverse}}{Welcome to the tlverse}}\label{tlverse}}

\hypertarget{learning-objectives-1}{%
\subsection*{Learning Objectives}\label{learning-objectives-1}}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Understand the \texttt{tlverse} ecosystem conceptually
\item
  Identify the core components of the \texttt{tlverse}
\item
  Install \texttt{tlverse} \texttt{R} packages
\item
  Understand the Targeted Learning roadmap
\item
  Learn about the WASH Benefits example data
\end{enumerate}

\hypertarget{what-is-the-tlverse}{%
\subsection*{\texorpdfstring{What is the \texttt{tlverse}?}{What is the tlverse?}}\label{what-is-the-tlverse}}


The \texttt{tlverse} is a new framework for doing Targeted Learning in R, inspired by
the \href{https://tidyverse.org}{\texttt{tidyverse} ecosystem} of R packages.

By analogy to the \href{https://tidyverse.org/}{\texttt{tidyverse}}:

\begin{quote}
The \texttt{tidyverse} is an opinionated collection of R packages designed for data
science. All packages share an underlying design philosophy, grammar, and data
structures.
\end{quote}

So, the \href{https://tlverse.org}{\texttt{tlverse}} is

\begin{itemize}
\tightlist
\item
  an opinionated collection of R packages for Targeted Learning
\item
  sharing an underlying philosophy, grammar, and set of data structures
\end{itemize}

\hypertarget{anatomy-of-the-tlverse}{%
\subsection*{\texorpdfstring{Anatomy of the \texttt{tlverse}}{Anatomy of the tlverse}}\label{anatomy-of-the-tlverse}}


These are the main packages that represent the \textbf{core} of the \texttt{tlverse}:

\begin{itemize}
\tightlist
\item
  \href{https://github.com/tlverse/sl3}{\texttt{sl3}}: Modern Super Learning with Pipelines

  \begin{itemize}
  \tightlist
  \item
    \emph{What?} A modern object-oriented re-implementation of the Super Learner
    algorithm, employing recently developed paradigms for \texttt{R} programming.
  \item
    \emph{Why?} A design that leverages modern tools for fast computation, is
    forward-looking, and can form one of the cornerstones of the \texttt{tlverse}.
  \end{itemize}
\item
  \href{https://github.com/tlverse/tmle3}{\texttt{tmle3}}: An Engine for Targeted Learning

  \begin{itemize}
  \tightlist
  \item
    \emph{What?} A generalized framework that simplifies Targeted Learning by
    identifying and implementing a series of common statistical estimation
    procedures.
  \item
    \emph{Why?} A common interface and engine that accommodates current algorithmic
    approaches to Targeted Learning and is still flexible enough to remain the
    engine even as new techniques are developed.
  \end{itemize}
\end{itemize}

In addition to the engines that drive development in the \texttt{tlverse}, there are
some supporting packages -- in particular, we have two\ldots{}

\begin{itemize}
\tightlist
\item
  \href{https://github.com/tlverse/origami}{\texttt{origami}}: A Generalized Framework for
  Cross-Validation

  \begin{itemize}
  \tightlist
  \item
    \emph{What?} A generalized framework for flexible cross-validation
  \item
    \emph{Why?} Cross-validation is a key part of ensuring error estimates are honest
    and preventing overfitting. It is an essential part of the both the Super
    Learner algorithm and Targeted Learning.
  \end{itemize}
\item
  \href{https://github.com/tlverse/delayed}{\texttt{delayed}}: Parallelization Framework for
  Dependent Tasks

  \begin{itemize}
  \tightlist
  \item
    \emph{What?} A framework for delayed computations (futures) based on task
    dependencies.
  \item
    \emph{Why?} Efficient allocation of compute resources is essential when deploying
    large-scale, computationally intensive algorithms.
  \end{itemize}
\end{itemize}

A key principle of the \texttt{tlverse} is extensibility. That is, we want to support
new Targeted Learning estimators as they are developed. The model for this is
new estimators are implemented in additional packages using the core packages
above. There are currently two featured examples of this:

\begin{itemize}
\tightlist
\item
  \href{https://github.com/tlverse/tmle3mopttx}{\texttt{tmle3mopttx}}: Optimal Treatments
  in \texttt{tlverse}

  \begin{itemize}
  \tightlist
  \item
    \emph{What?} Learn an optimal rule and estimate the mean outcome under the rule
  \item
    \emph{Why?} Optimal Treatment is a powerful tool in precision healthcare and
    other settings where a one-size-fits-all treatment approach is not
    appropriate.
  \end{itemize}
\item
  \href{https://github.com/tlverse/tmle3shift}{\texttt{tmle3shift}}: Shift Interventions in
  \texttt{tlverse}

  \begin{itemize}
  \tightlist
  \item
    \emph{What?} Shift interventions for continuous treatments
  \item
    \emph{Why?} Not all treatment variables are discrete. Being able to estimate the
    effects of continuous treatment represents a powerful extension of the
    Targeted Learning approach.
  \end{itemize}
\end{itemize}

\hypertarget{installtlverse}{%
\subsection{Installation}\label{installtlverse}}

The \texttt{tlverse} ecosystem of packages are currently hosted at
\url{https://github.com/tlverse}, not yet on \href{https://CRAN.R-project.org/}{CRAN}. You
can use the \href{https://usethis.r-lib.org/}{\texttt{usethis} package} to install them:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"devtools"}\NormalTok{)}
\NormalTok{devtools}\OperatorTok{::}\KeywordTok{install_github}\NormalTok{(}\StringTok{"tlverse/tlverse"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The \texttt{tlverse} depends on a large number of other packages that are also hosted
on GitHub. Because of this, you may see the following error:

\begin{verbatim}
Error: HTTP error 403.
  API rate limit exceeded for 71.204.135.82. (But here's the good news:
  Authenticated requests get a higher rate limit. Check out the documentation
  for more details.)

  Rate limit remaining: 0/60
  Rate limit reset at: 2019-03-04 19:39:05 UTC

  To increase your GitHub API rate limit
  - Use `usethis::browse_github_pat()` to create a Personal Access Token.
  - Use `usethis::edit_r_environ()` and add the token as `GITHUB_PAT`.
\end{verbatim}

This just means that R tried to install too many packages from GitHub in too
short of a window. To fix this, you need to tell R how to use GitHub as your
user (you'll need a GitHub user account). Follow these two steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Type \texttt{usethis::browse\_github\_pat()} in your R console, which will direct
  you to GitHub's page to create a New Personal Access Token (PAT).
\item
  Create a PAT simply by clicking ``Generate token'' at the bottom of the page.
\item
  Copy your PAT, a long string of lowercase letters and numbers.
\item
  Type \texttt{usethis::edit\_r\_environ()} in your R console, which will open your
  \texttt{.Renviron} file in the source window of RStudio.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    If your \texttt{.Renviron} file does not pop-up after calling
    \texttt{usethis::edit\_r\_environ()}; then try inputting
    \texttt{Sys.setenv(GITHUB\_PAT\ =\ "yourPAT")}, replacing your PAT with inside the
    quotes. If this does not error, then skip to step 8.
  \end{enumerate}
\item
  In your \texttt{.Renviron} file, type \texttt{GITHUB\_PAT=} and then paste your PAT after
  the equals symbol with no space.
\item
  In your \texttt{.Renviron} file, press the enter key to ensure that your \texttt{.Renviron}
  ends with a new line.
\item
  Save your \texttt{.Renviron} file. The example below shows how this syntax should
  look.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GITHUB_PAT=yourPAT}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  Restart R. You can restart R via the drop-down menu on RStudio's ``Session''
  tab, which is located at the top of the RStudio interface. You have to
  restart R for the changes to take effect!
\end{enumerate}

After following these steps, you should be able to successfully install the
package which threw the error above.

\hypertarget{data}{%
\section{Meet the Data}\label{data}}

\hypertarget{wash}{%
\subsection{WASH Benefits Example Dataset}\label{wash}}

The data come from a study of the effect of water quality, sanitation, hand
washing, and nutritional interventions on child development in rural Bangladesh
(WASH Benefits Bangladesh): a cluster randomized controlled trial
\citep{luby2018effect}. The study enrolled pregnant women in their first or second
trimester from the rural villages of Gazipur, Kishoreganj, Mymensingh, and
Tangail districts of central Bangladesh, with an average of eight women per
cluster. Groups of eight geographically adjacent clusters were block randomized,
using a random number generator, into six intervention groups (all of which
received weekly visits from a community health promoter for the first 6 months
and every 2 weeks for the next 18 months) and a double-sized control group (no
intervention or health promoter visit). The six intervention groups were:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  chlorinated drinking water;
\item
  improved sanitation;
\item
  hand-washing with soap;
\item
  combined water, sanitation, and hand washing;
\item
  improved nutrition through counseling and provision of lipid-based nutrient
  supplements; and
\item
  combined water, sanitation, handwashing, and nutrition.
\end{enumerate}

In the handbook, we concentrate on child growth (size for age) as the outcome of
interest. For reference, this trial was registered with ClinicalTrials.gov as
NCT01590095.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(readr)}
\CommentTok{# read in data via readr::read_csv}
\NormalTok{dat <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}
  \KeywordTok{paste0}\NormalTok{(}
    \StringTok{"https://raw.githubusercontent.com/tlverse/tlverse-data/master/"}\NormalTok{,}
    \StringTok{"wash-benefits/washb_data.csv"}
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For the purposes of this handbook, we start by treating the data as independent
and identically distributed (i.i.d.) random draws from a very large target
population. We could, with available options, account for the clustering of the
data (within sampled geographic units), but, for simplification, we avoid these
details in the handbook, although modifications of our methodology for biased
samples, repeated measures, and related complications, are available.

We have 28 variables measured, of which a single variable is set to
be the outcome of interest. This outcome, \(Y\), is the weight-for-height Z-score
(\texttt{whz} in \texttt{dat}); the treatment of interest, \(A\), is the randomized treatment
group (\texttt{tr} in \texttt{dat}); and the adjustment set, \(W\), consists simply of
\emph{everything else}. This results in our observed data structure being \(n\) i.i.d.
copies of \(O_i = (W_i, A_i, Y_i)\), for \(i = 1, \ldots, n\).

Using the \href{https://CRAN.R-project.org/package=skimr}{\texttt{skimr} package}, we can
quickly summarize the variables measured in the WASH Benefits data set:

\begin{tabular}{l|l|r|r|r|r|r|r|r|r|r|r|r|r|r|r}
\hline
skim\_type & skim\_variable & n\_missing & complete\_rate & character.min & character.max & character.empty & character.n\_unique & character.whitespace & numeric.mean & numeric.sd & numeric.p0 & numeric.p25 & numeric.p50 & numeric.p75 & numeric.p100\\
\hline
character & tr & 0 & 1.00000 & 3 & 15 & 0 & 7 & 0 & NA & NA & NA & NA & NA & NA & NA\\
\hline
character & fracode & 0 & 1.00000 & 2 & 6 & 0 & 20 & 0 & NA & NA & NA & NA & NA & NA & NA\\
\hline
character & sex & 0 & 1.00000 & 4 & 6 & 0 & 2 & 0 & NA & NA & NA & NA & NA & NA & NA\\
\hline
character & momedu & 0 & 1.00000 & 12 & 15 & 0 & 3 & 0 & NA & NA & NA & NA & NA & NA & NA\\
\hline
character & hfiacat & 0 & 1.00000 & 11 & 24 & 0 & 4 & 0 & NA & NA & NA & NA & NA & NA & NA\\
\hline
numeric & whz & 0 & 1.00000 & NA & NA & NA & NA & NA & -0.58608 & 1.03212 & -4.67 & -1.28 & -0.6 & 0.08 & 4.97\\
\hline
numeric & month & 0 & 1.00000 & NA & NA & NA & NA & NA & 6.45474 & 3.33214 & 1.00 & 4.00 & 6.0 & 9.00 & 12.00\\
\hline
numeric & aged & 0 & 1.00000 & NA & NA & NA & NA & NA & 266.31502 & 52.17465 & 42.00 & 230.00 & 266.0 & 303.00 & 460.00\\
\hline
numeric & momage & 18 & 0.99617 & NA & NA & NA & NA & NA & 23.90592 & 5.24055 & 14.00 & 20.00 & 23.0 & 27.00 & 60.00\\
\hline
numeric & momheight & 31 & 0.99340 & NA & NA & NA & NA & NA & 150.50407 & 5.22667 & 120.65 & 147.05 & 150.6 & 154.06 & 168.00\\
\hline
numeric & Nlt18 & 0 & 1.00000 & NA & NA & NA & NA & NA & 1.60469 & 1.24726 & 0.00 & 1.00 & 1.0 & 2.00 & 10.00\\
\hline
numeric & Ncomp & 0 & 1.00000 & NA & NA & NA & NA & NA & 11.04324 & 6.35044 & 2.00 & 6.00 & 10.0 & 14.00 & 52.00\\
\hline
numeric & watmin & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.94867 & 9.48125 & 0.00 & 0.00 & 0.0 & 1.00 & 600.00\\
\hline
numeric & elec & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.59510 & 0.49092 & 0.00 & 0.00 & 1.0 & 1.00 & 1.00\\
\hline
numeric & floor & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.10671 & 0.30878 & 0.00 & 0.00 & 0.0 & 0.00 & 1.00\\
\hline
numeric & walls & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.71502 & 0.45145 & 0.00 & 0.00 & 1.0 & 1.00 & 1.00\\
\hline
numeric & roof & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.98530 & 0.12035 & 0.00 & 1.00 & 1.0 & 1.00 & 1.00\\
\hline
numeric & asset\_wardrobe & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.16720 & 0.37319 & 0.00 & 0.00 & 0.0 & 0.00 & 1.00\\
\hline
numeric & asset\_table & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.73440 & 0.44170 & 0.00 & 0.00 & 1.0 & 1.00 & 1.00\\
\hline
numeric & asset\_chair & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.73440 & 0.44170 & 0.00 & 0.00 & 1.0 & 1.00 & 1.00\\
\hline
numeric & asset\_khat & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.61321 & 0.48707 & 0.00 & 0.00 & 1.0 & 1.00 & 1.00\\
\hline
numeric & asset\_chouki & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.78126 & 0.41344 & 0.00 & 1.00 & 1.0 & 1.00 & 1.00\\
\hline
numeric & asset\_tv & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.30394 & 0.46001 & 0.00 & 0.00 & 0.0 & 1.00 & 1.00\\
\hline
numeric & asset\_refrig & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.07945 & 0.27046 & 0.00 & 0.00 & 0.0 & 0.00 & 1.00\\
\hline
numeric & asset\_bike & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.31906 & 0.46616 & 0.00 & 0.00 & 0.0 & 1.00 & 1.00\\
\hline
numeric & asset\_moto & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.06603 & 0.24836 & 0.00 & 0.00 & 0.0 & 0.00 & 1.00\\
\hline
numeric & asset\_sewmach & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.06475 & 0.24611 & 0.00 & 0.00 & 0.0 & 0.00 & 1.00\\
\hline
numeric & asset\_mobile & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.85857 & 0.34850 & 0.00 & 1.00 & 1.0 & 1.00 & 1.00\\
\hline
\end{tabular}

A convenient summary of the relevant variables is given just above, complete
with a small visualization describing the marginal characteristics of each
covariate. Note that the \emph{asset} variables reflect socio-economic status of the
study participants. Notice also the uniform distribution of the treatment groups
(with twice as many controls); this is, of course, by design.

\hypertarget{ist}{%
\subsection{International Stroke Trial Example Dataset}\label{ist}}

The International Stroke Trial database contains individual patient data from
the International Stroke Trial (IST), a multi-national randomized trial
conducted between 1991 and 1996 (pilot phase between 1991 and 1993) that aimed
to assess whether early administration of aspirin, heparin, both aspirin and
heparin, or neither influenced the clinical course of acute ischaemic stroke
\citep{sandercock1997international}. The IST dataset includes data on 19,435 patients
with acute stroke, with 99\% complete follow-up. De-identified data are
available for download at \url{https://datashare.is.ed.ac.uk/handle/10283/128}. This
study is described in more detail in \citet{sandercock2011international}. The example
data for this handbook considers a sample of 5,000 patients and the binary
outcome of recurrent ischemic stroke within 14 days after randomization. Also
in this example data, we ensure that we have subjects with a missing outcome.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# read in data}
\NormalTok{ist <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}
  \KeywordTok{paste0}\NormalTok{(}
    \StringTok{"https://raw.githubusercontent.com/tlverse/tlverse-handbook/master/"}\NormalTok{,}
    \StringTok{"data/ist_sample.csv"}
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We have 26 variables measured, and the outcome of interest, \(Y\),
indicates recurrent ischemic stroke within 14 days after randomization (\texttt{DRSISC}
in \texttt{ist}); the treatment of interest, \(A\), is the randomized aspirin vs.~no
aspirin treatment allocation (\texttt{RXASP} in \texttt{ist}); and the adjustment set, \(W\),
consists of all other variables measured at baseline. In this data, the outcome
is occasionally missing, but there is no need to create a variable indicating
this missingness (such as \(\Delta\)) for analyses in the \texttt{tlverse}, since it is
automatically detected when \texttt{NA} are present in the outcome. This observed data
structure can be denoted as \(n\) i.i.d. copies of \(O_i = (W_i, A_i, \Delta_i, \Delta Y_i)\), for \(i = 1, \ldots, n\), where \(\Delta\) denotes the binary
indicator that the outcome is observed.

Like before, we can summarize the variables measured in the IST sample data set
with \texttt{skimr}:

\begin{tabular}{l|l|r|r|r|r|r|r|r|r|r|r|r|r|r|r}
\hline
skim\_type & skim\_variable & n\_missing & complete\_rate & character.min & character.max & character.empty & character.n\_unique & character.whitespace & numeric.mean & numeric.sd & numeric.p0 & numeric.p25 & numeric.p50 & numeric.p75 & numeric.p100\\
\hline
character & RCONSC & 0 & 1.000 & 1 & 1 & 0 & 3 & 0 & NA & NA & NA & NA & NA & NA & NA\\
\hline
character & SEX & 0 & 1.000 & 1 & 1 & 0 & 2 & 0 & NA & NA & NA & NA & NA & NA & NA\\
\hline
character & RSLEEP & 0 & 1.000 & 1 & 1 & 0 & 2 & 0 & NA & NA & NA & NA & NA & NA & NA\\
\hline
character & RATRIAL & 0 & 1.000 & 1 & 1 & 0 & 3 & 0 & NA & NA & NA & NA & NA & NA & NA\\
\hline
character & RCT & 0 & 1.000 & 1 & 1 & 0 & 2 & 0 & NA & NA & NA & NA & NA & NA & NA\\
\hline
character & RVISINF & 0 & 1.000 & 1 & 1 & 0 & 2 & 0 & NA & NA & NA & NA & NA & NA & NA\\
\hline
character & RHEP24 & 0 & 1.000 & 1 & 1 & 0 & 3 & 0 & NA & NA & NA & NA & NA & NA & NA\\
\hline
character & RASP3 & 0 & 1.000 & 1 & 1 & 0 & 3 & 0 & NA & NA & NA & NA & NA & NA & NA\\
\hline
character & RDEF1 & 0 & 1.000 & 1 & 1 & 0 & 3 & 0 & NA & NA & NA & NA & NA & NA & NA\\
\hline
character & RDEF2 & 0 & 1.000 & 1 & 1 & 0 & 3 & 0 & NA & NA & NA & NA & NA & NA & NA\\
\hline
character & RDEF3 & 0 & 1.000 & 1 & 1 & 0 & 3 & 0 & NA & NA & NA & NA & NA & NA & NA\\
\hline
character & RDEF4 & 0 & 1.000 & 1 & 1 & 0 & 3 & 0 & NA & NA & NA & NA & NA & NA & NA\\
\hline
character & RDEF5 & 0 & 1.000 & 1 & 1 & 0 & 3 & 0 & NA & NA & NA & NA & NA & NA & NA\\
\hline
character & RDEF6 & 0 & 1.000 & 1 & 1 & 0 & 3 & 0 & NA & NA & NA & NA & NA & NA & NA\\
\hline
character & RDEF7 & 0 & 1.000 & 1 & 1 & 0 & 3 & 0 & NA & NA & NA & NA & NA & NA & NA\\
\hline
character & RDEF8 & 0 & 1.000 & 1 & 1 & 0 & 3 & 0 & NA & NA & NA & NA & NA & NA & NA\\
\hline
character & STYPE & 0 & 1.000 & 3 & 4 & 0 & 5 & 0 & NA & NA & NA & NA & NA & NA & NA\\
\hline
character & RXHEP & 0 & 1.000 & 1 & 1 & 0 & 4 & 0 & NA & NA & NA & NA & NA & NA & NA\\
\hline
character & REGION & 0 & 1.000 & 10 & 26 & 0 & 7 & 0 & NA & NA & NA & NA & NA & NA & NA\\
\hline
numeric & RDELAY & 0 & 1.000 & NA & NA & NA & NA & NA & 20.14400 & 12.43485 & 1 & 9 & 19 & 29 & 48\\
\hline
numeric & AGE & 0 & 1.000 & NA & NA & NA & NA & NA & 71.93460 & 11.65016 & 16 & 65 & 74 & 81 & 99\\
\hline
numeric & RSBP & 0 & 1.000 & NA & NA & NA & NA & NA & 160.61560 & 27.84196 & 71 & 140 & 160 & 180 & 290\\
\hline
numeric & MISSING\_RATRIAL\_RASP3 & 0 & 1.000 & NA & NA & NA & NA & NA & 0.05000 & 0.21797 & 0 & 0 & 0 & 0 & 1\\
\hline
numeric & MISSING\_RHEP24 & 0 & 1.000 & NA & NA & NA & NA & NA & 0.01840 & 0.13441 & 0 & 0 & 0 & 0 & 1\\
\hline
numeric & RXASP & 0 & 1.000 & NA & NA & NA & NA & NA & 0.49780 & 0.50005 & 0 & 0 & 0 & 1 & 1\\
\hline
numeric & DRSISC & 10 & 0.998 & NA & NA & NA & NA & NA & 0.02365 & 0.15196 & 0 & 0 & 0 & 0 & 1\\
\hline
\end{tabular}

\hypertarget{NHEFS}{%
\subsection{NHANES I Epidemiologic Follow-up Study (NHEFS)}\label{NHEFS}}

This data is from the National Health and Nutrition Examination Survey (NHANES)
Data I Epidemiologic Follow-up Study. More coming soon.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# read in data}
\NormalTok{nhefs_data <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}
  \KeywordTok{paste0}\NormalTok{(}
    \StringTok{"https://raw.githubusercontent.com/tlverse/tlverse-handbook/master/"}\NormalTok{,}
    \StringTok{"data/NHEFS.csv"}
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

A snapshot of the data set is shown below:

\begin{tabular}{l|l|r|r|r|r|r|r|r|r|r}
\hline
skim\_type & skim\_variable & n\_missing & complete\_rate & numeric.mean & numeric.sd & numeric.p0 & numeric.p25 & numeric.p50 & numeric.p75 & numeric.p100\\
\hline
numeric & seqn & 0 & 1.00000 & 16552.36464 & 7498.91820 & 233.00000 & 10607.00000 & 20333.00000 & 2.2719e+04 & 2.5061e+04\\
\hline
numeric & qsmk & 0 & 1.00000 & 0.26274 & 0.44026 & 0.00000 & 0.00000 & 0.00000 & 1.0000e+00 & 1.0000e+00\\
\hline
numeric & death & 0 & 1.00000 & 0.19521 & 0.39649 & 0.00000 & 0.00000 & 0.00000 & 0.0000e+00 & 1.0000e+00\\
\hline
numeric & yrdth & 1311 & 0.19521 & 87.56918 & 2.65941 & 83.00000 & 85.00000 & 88.00000 & 9.0000e+01 & 9.2000e+01\\
\hline
numeric & modth & 1307 & 0.19767 & 6.25776 & 3.61530 & 1.00000 & 3.00000 & 6.00000 & 1.0000e+01 & 1.2000e+01\\
\hline
numeric & dadth & 1307 & 0.19767 & 15.87267 & 8.90549 & 1.00000 & 8.00000 & 15.50000 & 2.4000e+01 & 3.1000e+01\\
\hline
numeric & sbp & 77 & 0.95273 & 128.70941 & 19.05156 & 87.00000 & 116.00000 & 126.00000 & 1.4000e+02 & 2.2900e+02\\
\hline
numeric & dbp & 81 & 0.95028 & 77.74483 & 10.63486 & 47.00000 & 70.00000 & 77.00000 & 8.5000e+01 & 1.3000e+02\\
\hline
numeric & sex & 0 & 1.00000 & 0.50952 & 0.50006 & 0.00000 & 0.00000 & 1.00000 & 1.0000e+00 & 1.0000e+00\\
\hline
numeric & age & 0 & 1.00000 & 43.91529 & 12.17043 & 25.00000 & 33.00000 & 44.00000 & 5.3000e+01 & 7.4000e+01\\
\hline
numeric & race & 0 & 1.00000 & 0.13198 & 0.33858 & 0.00000 & 0.00000 & 0.00000 & 0.0000e+00 & 1.0000e+00\\
\hline
numeric & income & 62 & 0.96194 & 17.94767 & 2.66328 & 11.00000 & 17.00000 & 19.00000 & 2.0000e+01 & 2.2000e+01\\
\hline
numeric & marital & 0 & 1.00000 & 2.50338 & 1.08237 & 2.00000 & 2.00000 & 2.00000 & 2.0000e+00 & 8.0000e+00\\
\hline
numeric & school & 0 & 1.00000 & 11.13505 & 3.08960 & 0.00000 & 10.00000 & 12.00000 & 1.2000e+01 & 1.7000e+01\\
\hline
numeric & education & 0 & 1.00000 & 2.70350 & 1.19010 & 1.00000 & 2.00000 & 3.00000 & 3.0000e+00 & 5.0000e+00\\
\hline
numeric & ht & 0 & 1.00000 & 168.74096 & 9.05313 & 142.87500 & 161.78125 & 168.28125 & 1.7538e+02 & 1.9809e+02\\
\hline
numeric & wt71 & 0 & 1.00000 & 71.05213 & 15.72959 & 36.17000 & 59.65000 & 69.40000 & 7.9950e+01 & 1.6919e+02\\
\hline
numeric & wt82 & 63 & 0.96133 & 73.46922 & 16.15805 & 35.38020 & 61.68856 & 72.12119 & 8.3461e+01 & 1.3653e+02\\
\hline
numeric & wt82\_71 & 63 & 0.96133 & 2.63830 & 7.87991 & -41.28047 & -1.47840 & 2.60381 & 6.6896e+00 & 4.8538e+01\\
\hline
numeric & birthplace & 92 & 0.94352 & 31.59532 & 14.50050 & 1.00000 & 22.00000 & 34.00000 & 4.2000e+01 & 5.6000e+01\\
\hline
numeric & smokeintensity & 0 & 1.00000 & 20.55126 & 11.80375 & 1.00000 & 10.00000 & 20.00000 & 3.0000e+01 & 8.0000e+01\\
\hline
numeric & smkintensity82\_71 & 0 & 1.00000 & -4.73788 & 13.74136 & -80.00000 & -10.00000 & -1.00000 & 1.0000e+00 & 5.0000e+01\\
\hline
numeric & smokeyrs & 0 & 1.00000 & 24.87109 & 12.19807 & 1.00000 & 15.00000 & 24.00000 & 3.3000e+01 & 6.4000e+01\\
\hline
numeric & asthma & 0 & 1.00000 & 0.04850 & 0.21488 & 0.00000 & 0.00000 & 0.00000 & 0.0000e+00 & 1.0000e+00\\
\hline
numeric & bronch & 0 & 1.00000 & 0.08533 & 0.27946 & 0.00000 & 0.00000 & 0.00000 & 0.0000e+00 & 1.0000e+00\\
\hline
numeric & tb & 0 & 1.00000 & 0.01412 & 0.11802 & 0.00000 & 0.00000 & 0.00000 & 0.0000e+00 & 1.0000e+00\\
\hline
numeric & hf & 0 & 1.00000 & 0.00491 & 0.06993 & 0.00000 & 0.00000 & 0.00000 & 0.0000e+00 & 1.0000e+00\\
\hline
numeric & hbp & 0 & 1.00000 & 1.05095 & 0.95821 & 0.00000 & 0.00000 & 1.00000 & 2.0000e+00 & 2.0000e+00\\
\hline
numeric & pepticulcer & 0 & 1.00000 & 0.10374 & 0.30502 & 0.00000 & 0.00000 & 0.00000 & 0.0000e+00 & 1.0000e+00\\
\hline
numeric & colitis & 0 & 1.00000 & 0.03376 & 0.18067 & 0.00000 & 0.00000 & 0.00000 & 0.0000e+00 & 1.0000e+00\\
\hline
numeric & hepatitis & 0 & 1.00000 & 0.01719 & 0.13001 & 0.00000 & 0.00000 & 0.00000 & 0.0000e+00 & 1.0000e+00\\
\hline
numeric & chroniccough & 0 & 1.00000 & 0.05402 & 0.22613 & 0.00000 & 0.00000 & 0.00000 & 0.0000e+00 & 1.0000e+00\\
\hline
numeric & hayfever & 0 & 1.00000 & 0.08963 & 0.28573 & 0.00000 & 0.00000 & 0.00000 & 0.0000e+00 & 1.0000e+00\\
\hline
numeric & diabetes & 0 & 1.00000 & 0.97974 & 0.99579 & 0.00000 & 0.00000 & 0.00000 & 2.0000e+00 & 2.0000e+00\\
\hline
numeric & polio & 0 & 1.00000 & 0.01412 & 0.11802 & 0.00000 & 0.00000 & 0.00000 & 0.0000e+00 & 1.0000e+00\\
\hline
numeric & tumor & 0 & 1.00000 & 0.02333 & 0.15099 & 0.00000 & 0.00000 & 0.00000 & 0.0000e+00 & 1.0000e+00\\
\hline
numeric & nervousbreak & 0 & 1.00000 & 0.02885 & 0.16744 & 0.00000 & 0.00000 & 0.00000 & 0.0000e+00 & 1.0000e+00\\
\hline
numeric & alcoholpy & 0 & 1.00000 & 0.87600 & 0.33887 & 0.00000 & 1.00000 & 1.00000 & 1.0000e+00 & 2.0000e+00\\
\hline
numeric & alcoholfreq & 0 & 1.00000 & 1.92020 & 1.30714 & 0.00000 & 1.00000 & 2.00000 & 3.0000e+00 & 5.0000e+00\\
\hline
numeric & alcoholtype & 0 & 1.00000 & 2.47575 & 1.20816 & 1.00000 & 1.00000 & 3.00000 & 4.0000e+00 & 4.0000e+00\\
\hline
numeric & alcoholhowmuch & 417 & 0.74401 & 3.28713 & 2.98470 & 1.00000 & 2.00000 & 2.00000 & 4.0000e+00 & 4.8000e+01\\
\hline
numeric & pica & 0 & 1.00000 & 0.97545 & 0.99785 & 0.00000 & 0.00000 & 0.00000 & 2.0000e+00 & 2.0000e+00\\
\hline
numeric & headache & 0 & 1.00000 & 0.62983 & 0.48300 & 0.00000 & 0.00000 & 1.00000 & 1.0000e+00 & 1.0000e+00\\
\hline
numeric & otherpain & 0 & 1.00000 & 0.24616 & 0.43091 & 0.00000 & 0.00000 & 0.00000 & 0.0000e+00 & 1.0000e+00\\
\hline
numeric & weakheart & 0 & 1.00000 & 0.02210 & 0.14705 & 0.00000 & 0.00000 & 0.00000 & 0.0000e+00 & 1.0000e+00\\
\hline
numeric & allergies & 0 & 1.00000 & 0.06200 & 0.24123 & 0.00000 & 0.00000 & 0.00000 & 0.0000e+00 & 1.0000e+00\\
\hline
numeric & nerves & 0 & 1.00000 & 0.14426 & 0.35146 & 0.00000 & 0.00000 & 0.00000 & 0.0000e+00 & 1.0000e+00\\
\hline
numeric & lackpep & 0 & 1.00000 & 0.05095 & 0.21997 & 0.00000 & 0.00000 & 0.00000 & 0.0000e+00 & 1.0000e+00\\
\hline
numeric & hbpmed & 0 & 1.00000 & 1.00552 & 0.98295 & 0.00000 & 0.00000 & 1.00000 & 2.0000e+00 & 2.0000e+00\\
\hline
numeric & boweltrouble & 0 & 1.00000 & 1.03499 & 0.96722 & 0.00000 & 0.00000 & 1.00000 & 2.0000e+00 & 2.0000e+00\\
\hline
numeric & wtloss & 0 & 1.00000 & 0.02578 & 0.15854 & 0.00000 & 0.00000 & 0.00000 & 0.0000e+00 & 1.0000e+00\\
\hline
numeric & infection & 0 & 1.00000 & 0.14794 & 0.35515 & 0.00000 & 0.00000 & 0.00000 & 0.0000e+00 & 1.0000e+00\\
\hline
numeric & active & 0 & 1.00000 & 0.65193 & 0.65274 & 0.00000 & 0.00000 & 1.00000 & 1.0000e+00 & 2.0000e+00\\
\hline
numeric & exercise & 0 & 1.00000 & 1.19521 & 0.73935 & 0.00000 & 1.00000 & 1.00000 & 2.0000e+00 & 2.0000e+00\\
\hline
numeric & birthcontrol & 0 & 1.00000 & 1.08471 & 0.94775 & 0.00000 & 0.00000 & 1.00000 & 2.0000e+00 & 2.0000e+00\\
\hline
numeric & pregnancies & 903 & 0.44567 & 3.69146 & 2.20560 & 1.00000 & 2.00000 & 3.00000 & 5.0000e+00 & 1.5000e+01\\
\hline
numeric & cholesterol & 16 & 0.99018 & 219.97396 & 45.44420 & 78.00000 & 189.00000 & 216.00000 & 2.4500e+02 & 4.1600e+02\\
\hline
numeric & hightax82 & 92 & 0.94352 & 0.16591 & 0.37212 & 0.00000 & 0.00000 & 0.00000 & 0.0000e+00 & 1.0000e+00\\
\hline
numeric & price71 & 92 & 0.94352 & 2.13875 & 0.22902 & 1.50659 & 2.03662 & 2.16797 & 2.2417e+00 & 2.6929e+00\\
\hline
numeric & price82 & 92 & 0.94352 & 1.80610 & 0.13064 & 1.45190 & 1.73999 & 1.81494 & 1.8677e+00 & 2.1030e+00\\
\hline
numeric & tax71 & 92 & 0.94352 & 1.05858 & 0.21623 & 0.52490 & 0.94495 & 1.04980 & 1.1548e+00 & 1.5225e+00\\
\hline
numeric & tax82 & 92 & 0.94352 & 0.50598 & 0.11189 & 0.21997 & 0.43994 & 0.50598 & 5.7190e-01 & 7.4792e-01\\
\hline
numeric & price71\_82 & 92 & 0.94352 & 0.33274 & 0.15504 & -0.20270 & 0.20099 & 0.33600 & 4.4379e-01 & 6.1206e-01\\
\hline
numeric & tax71\_82 & 92 & 0.94352 & 0.55261 & 0.15032 & 0.03600 & 0.46100 & 0.54395 & 6.2195e-01 & 8.8440e-01\\
\hline
\end{tabular}

\hypertarget{origami}{%
\section{Cross-validation}\label{origami}}

\emph{Ivana Malenica}

Based on the \href{https://github.com/tlverse/origami}{\texttt{origami} \texttt{R} package}
by \emph{Jeremy Coyle, Nima Hejazi, Ivana Malenica and Rachael Phillips}.

Updated: 2021-04-19

\hypertarget{learning-objectives-2}{%
\subsection{Learning Objectives}\label{learning-objectives-2}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Differentiate between training, validation and test sets.
\item
  Understand the concept of a loss function, risk and cross-validation.
\item
  Select a loss function that is appropriate for the functional parameter to be
  estimated.
\item
  Understand and contrast different cross-validation schemes for i.i.d. data.
\item
  Understand and contrast different cross-validation schemes for time dependent
  data.
\item
  Setup the proper fold structure, build custom fold-based function, and
  cross-validate the proposed function using the \texttt{origami} \texttt{R} package.
\item
  Setup the proper cross-validation structure for the use by the Super Learner
  using the the \texttt{origami} \texttt{R} package.
\end{enumerate}

\hypertarget{introduction-1}{%
\subsection{Introduction}\label{introduction-1}}

In this chapter, we start elaborating on the estimation step outlined in the
\protect\hyperlink{intro}{introductory chapter}, which discussed the \protect\hyperlink{roadmap}{\emph{Roadmap for Targeted
Learning}}. In order to generate an initial estimate of our target
parameter -- which is the focus of the following \protect\hyperlink{sl3}{chapter on Super
Learning}, we first need to translate, and incorporate, our knowledge
about the data generating process into the estimation procedure, and decide how
to evaluate our estimation performance.

The performance, or error, of any algorithm used in the estimation procedure
directly relates to its generalizability on the independent data. The proper
assessment of the performance of proposed algorithms is extremely important; it
guides the choice of the final learning method, and it gives us a quantitative
assessment of how good the chosen algorithm is doing. In order to assess the
performance of an algorithm, we introduce the concept of a \textbf{loss} function,
which helps us define the \textbf{risk}, also referred to as the \textbf{expected
prediction error}. Our goal, as further specified in the next chapter, will be
to estimate the true risk of the proposed statistical learning method. Our
goal(s) consist of:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Estimating the performance of different algorithms in order to choose the
  best one.
\item
  Having chosen a winner, try to estimate the true risk of the proposed
  statistical learning method.
\end{enumerate}

In the following, we propose a method to do so using the observed data and
\textbf{cross-validation} procedure using the \texttt{origami} package \citep{coyle2018origami}.

\hypertarget{background}{%
\subsection{Background}\label{background}}

Ideally, in a data-rich scenario, we would split our dataset into three parts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  training set,
\item
  validation set,
\item
  test set.
\end{enumerate}

The training set is used to fit algorithm(s) of interest; we evaluate the
performance of the fit(s) on a validation set, which can be used to estimate
prediction error (e.g., for tuning and model selection). The final error of the
chosen algorithm(s) is obtained by using the test set, which is kept separately,
and doesn't see the data before the final evaluation. One might wonder, with
training data readily available, why not use the training error to evaluate the
proposed algorithm's performance? Unfortunately, the training error is not a
good estimate of the true risk; it consistently decreases with model complexity,
resulting in a possible overfit to the training data and low generalizability.

Since data are often scarce, separating it into training, validation and test
set is usually not possible. In the absence of a large data set and a designated
test set, we must resort to methods that estimate the true risk by efficient
sample re-use. Re-sampling methods, in great generality, involve repeatedly
sampling from the training set and fitting proposed algorithms on the new
samples. While often computationally intensive, re-sampling methods are
particularly useful for model selection and estimation of the true risk. In
addition, they might provide more insight on variability and robustness of the
algorithm fit then fitting an algorithm only once on all the training data.

\hypertarget{introducing-cross-validation}{%
\subsubsection{Introducing: cross-validation}\label{introducing-cross-validation}}

In this chapter, we focus on \textbf{cross-validation} -- an essential tool for
evaluating how any given algorithm extends from a sample to the target
population from which the sample is derived. It has seen widespread application
in all facets of statistics, perhaps most notably statistical machine learning.
The cross-validation procedure can be used for model selection, as well as for
estimation of the true risk associated with any statistical learning method in
order to evaluate its performance. It particular, cross-validation directly
estimates the true risk when the estimate is applied to an independent sample
from the joint distribution of the predictors and outcome. When used for model
selection, cross-validation has powerful optimality properties. The asymptotic
optimality results state that the cross-validated selector performs (in terms of
risk) asymptotically as well as an optimal oracle selector based on the true,
unknown data generating distribution. For further details on the theoretical
results, we suggest \citet{vdl2004asymptotic}, \citet{dudoit2005asymptotics} and
\citet{vaart2006oracle}.

In great generality, cross-validation works by partitioning a sample into
complementary subsets, applying a particular algorithm(s) on a subset (the
training set), and evaluating the method of choice on the complementary subset
(the validation/test set). This procedure is repeated across multiple partitions
of the data. A variety of different partitioning schemes exist, depending on the
problem of interest, data size, prevalence of the outcome, and dependence
structure. The \texttt{origami} package provides a suite of tools that generalize the
application of cross-validation to arbitrary data analytic procedures. In the
the following, we describe different types of cross-validation schemes readily
available in \texttt{origami}, introduce the general structure of the \texttt{origami}
package, and show their use in applied settings.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{estimation-roadmap-how-does-it-all-fit-together}{%
\subsection{Estimation Roadmap: how does it all fit together?}\label{estimation-roadmap-how-does-it-all-fit-together}}

Similarly to how we defined the \protect\hyperlink{roadmap}{\emph{Roadmap for Targeted Learning}}, we
can define the \textbf{Estimation Roadmap} to guide the estimation process. In
particular, we have developed a unified loss-based cross-validation methodology
for estimator construction, selection, and performance assessment in a series of
articles (e.g., see \citet{vdl2004asymptotic}, \citet{dudoit2005asymptotics},
\citet{vaart2006oracle}, and \citet{vdl2007super}) that follow three main steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{The loss funtion}:
  Define the target parameter as the minimizer of the expected loss (risk) for a
  full data loss function chosen to represent the desired performance measure.
  Map the full data loss function into an observed data loss function, having the
  same expected value and leading to an efficient estimator of risk.
\item
  \textbf{The algorithms}:
  Construct a finite collection of candidate estimators for the parameter of
  interest.
\item
  \textbf{The cross-validation scheme}:
  Apply appropriate cross-validation to select an optimal estimator among the
  candidates, and assess the overall performance of the resulting estimator.
\end{enumerate}

Step 1 of the Estimation Roadmap allows us to unify a broad range of problems
that are traditionally treated separately in the statistical literature,
including density estimation, prediction of polychotomous and continuous
outcomes. For example, if we are interested in estimating the full joint
conditional density, we could use the negative log-likelihood loss. If instead
we are interested in the conditional mean with continuous outcome, one could use
the squared error loss; had the outcome been binary, one could resort to the
indicator (0-1) loss. The unified loss-based framework also reconciles censored
and full data estimation methods, as full data estimators are recovered as
special cases of censored data estimators.

\hypertarget{example-cross-validation-and-prediction}{%
\subsection{Example: cross-validation and prediction}\label{example-cross-validation-and-prediction}}

Now that we introduced the Estimation Roadmap, we can define our objective with
more mathematical notation, using prediction as an example. Let the observed
data be defined as \(X = (W,Y)\), where a unit specific data can be written as
\(X_i = (W_i,Y_i)\), for \(i = 1, \ldots, n\). For each of the \(n\) samples, we
denote \(Y_i\) as the outcome of interest (polychotomous or continuous), and \(W_i\)
as a \(p\)-dimensional set of covariates. Let \(\psi_0(W)\) denote the target
parameter of interest we want to estimate; for this example, we are interested
in estimating the conditional expectation of the outcome given the covariates,
\(\psi_0(W) = E(Y \mid W)\). Following the Estimation Roadmap, we chose the
appropriate loss function, \(L\), such that \(\psi_0(W) = \text{argmin}_{\psi} E[L(X,\psi(W))]\). But how do we know how each \(\psi\) is doing? In order to pick
the optimal estimator among the candidates, and assess the overall performance
of the resulting estimator, use cross-validation -- dividing the available data
into the training set and validation set. Observations in the training set are
used to fit (or train) the estimator, while the validation set is used to assess
the risk of (or validate) it.

To derive a general representation for cross-validation, we define a \textbf{split
vector}, \(B_n = (B_n(i): i = 1, \ldots, n) \in \{0,1\}^n\). Note that split
vector is independent of the empirical distribution, \(P_n\). A realization of
\(B_n\) defines a random split of the data into a training and validation set such
that if
\[B_n(i) = 0, \ \ \text{i sample is in the training set}\]
\[B_n(i) = 1, \ \ \text{i sample is in the validation set.}\]
We can further define \(P_{n,B_n}^0\) and \(P_{n,B_n}^1\) as the empirical
distributions of the training and validation sets, respectively. Then \(n_0 = \sum_i 1-B_n(i)\) and \(n_1 = \sum_i B_n(i)\) denote the number of samples in each
set. The particular distribution of the split vector \(B_n\) defines the type of
cross-validation scheme, tailored to the problem and data set in hand.

\hypertarget{cross-validation-schemes-in-origami}{%
\subsection{\texorpdfstring{Cross-validation schemes in \texttt{origami}}{Cross-validation schemes in origami}}\label{cross-validation-schemes-in-origami}}

As we specified earlier, the particular distribution of the split vector \(B_n\)
defines the type of cross-validation method. In the following, we describe
different types of cross-validation schemes available in \texttt{origami} package, and
show their use in the sequel.

\hypertarget{wash-benefits-study-example}{%
\subsubsection*{WASH Benefits Study Example}\label{wash-benefits-study-example}}


In order to illustrate different cross-validation schemes, we will be using the
WASH data. Detailed information on the WASH Benefits Example Dataset can be
found in \protect\hypertarget{data}{}{Chapter 3}. In particular, we are interested in predicting
weight-for-height z-score \texttt{whz} using the available covariate data. For this
illustration, we will start by treating the data as independent and identically
distributed (i.i.d.) random draws. To see what each cross-validation scheme is
doing, we will subset the data to only \(n=30\). Note that each row represents an
i.i.d. sample, indexed by the row number.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(data.table)}
\KeywordTok{library}\NormalTok{(origami)}
\KeywordTok{library}\NormalTok{(knitr)}
\KeywordTok{library}\NormalTok{(kableExtra)}

\CommentTok{# load data set and take a peek}
\NormalTok{washb_data <-}\StringTok{ }\KeywordTok{fread}\NormalTok{(}
  \KeywordTok{paste0}\NormalTok{(}
    \StringTok{"https://raw.githubusercontent.com/tlverse/tlverse-data/master/"}\NormalTok{,}
    \StringTok{"wash-benefits/washb_data.csv"}
\NormalTok{  ),}
  \DataTypeTok{stringsAsFactors =} \OtherTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|l|l|r|r|l|r|l|r|l|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r}
\hline
whz & tr & fracode & month & aged & sex & momage & momedu & momheight & hfiacat & Nlt18 & Ncomp & watmin & elec & floor & walls & roof & asset\_wardrobe & asset\_table & asset\_chair & asset\_khat & asset\_chouki & asset\_tv & asset\_refrig & asset\_bike & asset\_moto & asset\_sewmach & asset\_mobile\\
\hline
0.00 & Control & N05265 & 9 & 268 & male & 30 & Primary (1-5y) & 146.40 & Food Secure & 3 & 11 & 0 & 1 & 0 & 1 & 1 & 0 & 1 & 1 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 1\\
\hline
-1.16 & Control & N05265 & 9 & 286 & male & 25 & Primary (1-5y) & 148.75 & Moderately Food Insecure & 2 & 4 & 0 & 1 & 0 & 1 & 1 & 0 & 1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 1\\
\hline
-1.05 & Control & N08002 & 9 & 264 & male & 25 & Primary (1-5y) & 152.15 & Food Secure & 1 & 10 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1\\
\hline
-1.26 & Control & N08002 & 9 & 252 & female & 28 & Primary (1-5y) & 140.25 & Food Secure & 3 & 5 & 0 & 1 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 1\\
\hline
-0.59 & Control & N06531 & 9 & 336 & female & 19 & Secondary (>5y) & 150.95 & Food Secure & 2 & 7 & 0 & 1 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 1\\
\hline
-0.51 & Control & N06531 & 9 & 304 & male & 20 & Secondary (>5y) & 154.20 & Severely Food Insecure & 0 & 3 & 1 & 1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1\\
\hline
\end{tabular}

Above is a look at the first 30 of the data.

\hypertarget{cross-validation-for-i.i.d.-data}{%
\subsubsection{Cross-validation for i.i.d. data}\label{cross-validation-for-i.i.d.-data}}

\hypertarget{re-substitution}{%
\paragraph{Re-substitution}\label{re-substitution}}

The re-substitution method is the simplest strategy for estimating the risk
associated with fitting a proposed algorithm on a set of observations. Here, all
observed data is used for both training and validation set.

We illustrate the usage of the re-substitution method with \texttt{origami} package
below; we will use the function \texttt{folds\_resubstitution(n)}. In order to setup
\texttt{folds\_resubstitution(n)}, we just need the total number of samples we want to
allocate to training and validation sets; remember that each row of data is a
unique i.i.d. sample. Notice the structure of the \texttt{origami} output:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  v: the cross-validation fold
\item
  training\_set: the indexes of the samples in the training set
\item
  validation\_set: the indexes of the samples in the training set.
\end{enumerate}

This structure of the \texttt{origami} output (fold(s)) will persist for each of the
cross-validation schemes we present in this chapter. Below, we show the fold
generated by the re-substitution method:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{folds_resubstitution}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(washb_data))}
\NormalTok{[[}\DecValTok{1}\NormalTok{]]}
\OperatorTok{$}\NormalTok{v}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\DecValTok{1}

\OperatorTok{$}\NormalTok{training_set}
\NormalTok{ [}\DecValTok{1}\NormalTok{]  }\DecValTok{1}  \DecValTok{2}  \DecValTok{3}  \DecValTok{4}  \DecValTok{5}  \DecValTok{6}  \DecValTok{7}  \DecValTok{8}  \DecValTok{9} \DecValTok{10} \DecValTok{11} \DecValTok{12} \DecValTok{13} \DecValTok{14} \DecValTok{15} \DecValTok{16} \DecValTok{17} \DecValTok{18} \DecValTok{19} \DecValTok{20} \DecValTok{21} \DecValTok{22} \DecValTok{23} \DecValTok{24} \DecValTok{25}
\NormalTok{[}\DecValTok{26}\NormalTok{] }\DecValTok{26} \DecValTok{27} \DecValTok{28} \DecValTok{29} \DecValTok{30}

\OperatorTok{$}\NormalTok{validation_set}
\NormalTok{ [}\DecValTok{1}\NormalTok{]  }\DecValTok{1}  \DecValTok{2}  \DecValTok{3}  \DecValTok{4}  \DecValTok{5}  \DecValTok{6}  \DecValTok{7}  \DecValTok{8}  \DecValTok{9} \DecValTok{10} \DecValTok{11} \DecValTok{12} \DecValTok{13} \DecValTok{14} \DecValTok{15} \DecValTok{16} \DecValTok{17} \DecValTok{18} \DecValTok{19} \DecValTok{20} \DecValTok{21} \DecValTok{22} \DecValTok{23} \DecValTok{24} \DecValTok{25}
\NormalTok{[}\DecValTok{26}\NormalTok{] }\DecValTok{26} \DecValTok{27} \DecValTok{28} \DecValTok{29} \DecValTok{30}

\KeywordTok{attr}\NormalTok{(,}\StringTok{"class"}\NormalTok{)}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\StringTok{"fold"}
\end{Highlighting}
\end{Shaded}

\hypertarget{holdout-method}{%
\paragraph{Holdout method}\label{holdout-method}}

The holdout method, or the validation set approach, consists of randomly
dividing the available data into the training set and validation set (holdout
set). The model is then fitted on the training set, and further evaluated on
the observations in the validation set. Typically, the data is split into
\(60/40\), \(70/30\) or \(80/20\) splits.

The holdout method is intuitive, conceptually easy, and computationally not too
demanding. However, if we repeat the process of randomly splitting the data into
the training and validation set, we might get a different validation loss (e.g.,
MSE). In particular, the loss over the validation sets might be highly
variable, depending on which samples were included in the training/validation
split. For classification problems, there is a possibility of an uneven
distribution of different classes in the training and validation set unless data
is stratified. Finally, note that we are not using all of the data to train and
evaluate the performance of the proposed algorithm, which might result in bias.

\hypertarget{leave-one-out}{%
\paragraph{Leave-one-out}\label{leave-one-out}}

The leave-one-out cross-validation scheme is closely related to the holdout
method. In particular, it also involves splitting the data into the training and
validation set; however, instead of partitioning the observed data into sets of
similar size, a single observation is used as a validation set. With that,
majority of the units are employed for training (fitting) the proposed
algorithm. Since only one unit (for example \(x_1 = (w_1, y_1)\)) is not used in
the fitting process, leave-one-out cross-validation results in a possibly less
biased estimate of the true risk; typically, leave-one-out approach will not
overestimate the risk as much as the holdout method. On the other hand, since
the estimate of risk is based on a single sample, it is typically a highly
variable estimate.

We can repeat the process of spiting the data into training and validation set
until all samples are part of the validation set at some point. For example,
next iteration of the cross-validation might have \(x_2 = (w_2,y_2)\) as the
validation set and all the rest of \(n-1\) samples as the training set. Repeating
this approach \(n\) times results in, for example, \(n\) squared errors \(MSE_1, MSE_2, \ldots, MSE_n\). The estimate of the true risk is the average over the
\(n\) squared errors. While the leave-one-out cross-validation results in a less
biased (albeit, more variable) estimate of risk than the holdout method, it
could be expensive to implement if \(n\) is large.

We illustrate the usage of the leave-one-out cross-validation with \texttt{origami}
package below; we will use the function \texttt{folds\_loo(n)}. In order to setup
\texttt{folds\_loo(n)}, similarly to the re-substitution method, we just need the total
number of samples we want to cross-validate. We show the first two folds
generated by the leave-one-out cross-validation below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{folds <-}\StringTok{ }\KeywordTok{folds_loo}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(washb_data))}
\NormalTok{folds[[}\DecValTok{1}\NormalTok{]]}
\OperatorTok{$}\NormalTok{v}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\DecValTok{1}

\OperatorTok{$}\NormalTok{training_set}
\NormalTok{ [}\DecValTok{1}\NormalTok{]  }\DecValTok{2}  \DecValTok{3}  \DecValTok{4}  \DecValTok{5}  \DecValTok{6}  \DecValTok{7}  \DecValTok{8}  \DecValTok{9} \DecValTok{10} \DecValTok{11} \DecValTok{12} \DecValTok{13} \DecValTok{14} \DecValTok{15} \DecValTok{16} \DecValTok{17} \DecValTok{18} \DecValTok{19} \DecValTok{20} \DecValTok{21} \DecValTok{22} \DecValTok{23} \DecValTok{24} \DecValTok{25} \DecValTok{26}
\NormalTok{[}\DecValTok{26}\NormalTok{] }\DecValTok{27} \DecValTok{28} \DecValTok{29} \DecValTok{30}

\OperatorTok{$}\NormalTok{validation_set}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\DecValTok{1}

\KeywordTok{attr}\NormalTok{(,}\StringTok{"class"}\NormalTok{)}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\StringTok{"fold"}
\NormalTok{folds[[}\DecValTok{2}\NormalTok{]]}
\OperatorTok{$}\NormalTok{v}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\DecValTok{2}

\OperatorTok{$}\NormalTok{training_set}
\NormalTok{ [}\DecValTok{1}\NormalTok{]  }\DecValTok{1}  \DecValTok{3}  \DecValTok{4}  \DecValTok{5}  \DecValTok{6}  \DecValTok{7}  \DecValTok{8}  \DecValTok{9} \DecValTok{10} \DecValTok{11} \DecValTok{12} \DecValTok{13} \DecValTok{14} \DecValTok{15} \DecValTok{16} \DecValTok{17} \DecValTok{18} \DecValTok{19} \DecValTok{20} \DecValTok{21} \DecValTok{22} \DecValTok{23} \DecValTok{24} \DecValTok{25} \DecValTok{26}
\NormalTok{[}\DecValTok{26}\NormalTok{] }\DecValTok{27} \DecValTok{28} \DecValTok{29} \DecValTok{30}

\OperatorTok{$}\NormalTok{validation_set}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\DecValTok{2}

\KeywordTok{attr}\NormalTok{(,}\StringTok{"class"}\NormalTok{)}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\StringTok{"fold"}
\end{Highlighting}
\end{Shaded}

\hypertarget{v-fold}{%
\paragraph{V-fold}\label{v-fold}}

An alternative to leave-one-out is V-fold cross-validation. This
cross-validation scheme randomly divides the data into \(v\) sets (folds) of equal
size; for each fold, the number of samples in the validation set are the same.
For V-fold cross-validation, one of the folds is treated as a validation set,
whereas the proposed algorithm is fit on the remaining \(v-1\) folds in the
training set. The loss, for example MSE, is computed on the samples in the
validation set. With the proposed algorithm trained and its performance
evaluated on the first fold, we repeat this process \(v\) times; each time, a
different group of samples is treated as a validation set. Note that with V-fold
cross-validation we effectively use all of the data to train and evaluate the
proposed algorithm without overfitting to the training data. In the end, the
V-fold cross-validation results in \(v\) estimates of validation error. The final
V-fold CV estimate is computed as an average over all the validation losses.

For a dataset with \(n\) samples, V-fold cross-validation with \(v=n\) is just
leave-one-out; similarly, if we set \(n=1\), we can get the holdout method's
estimate of algorithm's performance. Despite the obvious computational
advantages, V-fold cross-validation often gives more accurate estimates of the
true risk. The reason for this comes from the bias-variance trade-off that comes
from employing both methods; while leave-one-out might be less biased, it has
higher variance. This difference becomes more obvious as \(v<<n\) (but not too
small, as then we increase bias). With V-fold cross-validation, we end up
averaging output from \(v\) fits that are typically less correlated than the
outputs from leave-one-out fits. Since the mean of many highly correlated
quantities has higher variance, leave-one-out estimate of the risk will also
have higher variance than the estimate based on V-fold cross-validation.

Let's see V-fold cross-validation with \texttt{origami} in action! In the next chapter
we will study the Super Learner, an actual algorithm that we fit and evaluate
its performance, that uses V-fold as default cross-validation scheme. In order
to set up V-fold CV, we need to call function \texttt{folds\_vfold(n,\ V)}. Arguments
for \texttt{folds\_vfold(n,\ V)} require the total number of samples to be
cross-validated, and the number of folds we want to get.

At \(V=2\), we get 2 folds with \(n/2\) number of samples in both training and
validation set.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{folds <-}\StringTok{ }\KeywordTok{folds_vfold}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(washb_data), }\DataTypeTok{V =} \DecValTok{2}\NormalTok{)}
\NormalTok{folds[[}\DecValTok{1}\NormalTok{]]}
\OperatorTok{$}\NormalTok{v}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\DecValTok{1}

\OperatorTok{$}\NormalTok{training_set}
\NormalTok{ [}\DecValTok{1}\NormalTok{]  }\DecValTok{2}  \DecValTok{3}  \DecValTok{4}  \DecValTok{6}  \DecValTok{7}  \DecValTok{8} \DecValTok{11} \DecValTok{12} \DecValTok{14} \DecValTok{15} \DecValTok{19} \DecValTok{22} \DecValTok{23} \DecValTok{24} \DecValTok{28}

\OperatorTok{$}\NormalTok{validation_set}
\NormalTok{ [}\DecValTok{1}\NormalTok{]  }\DecValTok{1}  \DecValTok{5}  \DecValTok{9} \DecValTok{10} \DecValTok{13} \DecValTok{16} \DecValTok{17} \DecValTok{18} \DecValTok{20} \DecValTok{21} \DecValTok{25} \DecValTok{26} \DecValTok{27} \DecValTok{29} \DecValTok{30}

\KeywordTok{attr}\NormalTok{(,}\StringTok{"class"}\NormalTok{)}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\StringTok{"fold"}
\NormalTok{folds[[}\DecValTok{2}\NormalTok{]]}
\OperatorTok{$}\NormalTok{v}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\DecValTok{2}

\OperatorTok{$}\NormalTok{training_set}
\NormalTok{ [}\DecValTok{1}\NormalTok{]  }\DecValTok{1}  \DecValTok{5}  \DecValTok{9} \DecValTok{10} \DecValTok{13} \DecValTok{16} \DecValTok{17} \DecValTok{18} \DecValTok{20} \DecValTok{21} \DecValTok{25} \DecValTok{26} \DecValTok{27} \DecValTok{29} \DecValTok{30}

\OperatorTok{$}\NormalTok{validation_set}
\NormalTok{ [}\DecValTok{1}\NormalTok{]  }\DecValTok{2}  \DecValTok{3}  \DecValTok{4}  \DecValTok{6}  \DecValTok{7}  \DecValTok{8} \DecValTok{11} \DecValTok{12} \DecValTok{14} \DecValTok{15} \DecValTok{19} \DecValTok{22} \DecValTok{23} \DecValTok{24} \DecValTok{28}

\KeywordTok{attr}\NormalTok{(,}\StringTok{"class"}\NormalTok{)}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\StringTok{"fold"}
\end{Highlighting}
\end{Shaded}

\hypertarget{monte-carlo}{%
\paragraph{Monte Carlo}\label{monte-carlo}}

With Monte Carlo cross-validation, we randomly select some fraction of the data
(without replacement) to form the training set; we assign the rest of the
samples to the validation set. With that, the data is repeatedly and randomly
divided into two sets, a training set of \(n_0 = n \cdot (1-p)\) observations and
a validation set of \(n_1 = n \cdot p\) observations. This process is then
repeated multiple times, generating (at random) new training and validation
partitions each time.

Since the partitions are independent across folds, the same sample can appear in
the validation set multiple times -- note that this is a stark difference
between Monte Carlo and V-fold cross-validation. With Monte Carlo
cross-validation, one is able to explore many more available partitions than
with V-fold cross-validation -- resulting in a possibly less variable estimate
of the risk, at a cost of an increase in bias.

We illustrate the usage of the Monte Carlo cross-validation with \texttt{origami}
package below using the function \texttt{folds\_montecarlo(n,\ V,\ pvalidation)}. In order
to setup \texttt{folds\_montecarlo(n,\ V,\ pvalidation)}, we need:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the total number of samples we want to cross-validate;
\item
  the number of folds;
\item
  the proportion of observations to be placed in the validation set.
\end{enumerate}

At \(V=2\) and \(pvalidation=0.2\), we obtain 2 folds with approximately \(6\) samples
in validation set per fold.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{folds <-}\StringTok{ }\KeywordTok{folds_montecarlo}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(washb_data), }\DataTypeTok{V =} \DecValTok{2}\NormalTok{, }\DataTypeTok{pvalidation =} \FloatTok{0.2}\NormalTok{)}
\NormalTok{folds[[}\DecValTok{1}\NormalTok{]]}
\OperatorTok{$}\NormalTok{v}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\DecValTok{1}

\OperatorTok{$}\NormalTok{training_set}
\NormalTok{ [}\DecValTok{1}\NormalTok{] }\DecValTok{19} \DecValTok{27} \DecValTok{16} \DecValTok{29} \DecValTok{23} \DecValTok{12}  \DecValTok{1}  \DecValTok{3} \DecValTok{18} \DecValTok{11}  \DecValTok{5}  \DecValTok{7}  \DecValTok{8}  \DecValTok{6}  \DecValTok{9} \DecValTok{22} \DecValTok{10} \DecValTok{25} \DecValTok{20} \DecValTok{28} \DecValTok{15}  \DecValTok{2} \DecValTok{24} \DecValTok{26}

\OperatorTok{$}\NormalTok{validation_set}
\NormalTok{[}\DecValTok{1}\NormalTok{]  }\DecValTok{4} \DecValTok{13} \DecValTok{14} \DecValTok{17} \DecValTok{21} \DecValTok{30}

\KeywordTok{attr}\NormalTok{(,}\StringTok{"class"}\NormalTok{)}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\StringTok{"fold"}
\NormalTok{folds[[}\DecValTok{2}\NormalTok{]]}
\OperatorTok{$}\NormalTok{v}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\DecValTok{2}

\OperatorTok{$}\NormalTok{training_set}
\NormalTok{ [}\DecValTok{1}\NormalTok{] }\DecValTok{19} \DecValTok{15} \DecValTok{28} \DecValTok{25} \DecValTok{29} \DecValTok{11} \DecValTok{20} \DecValTok{17} \DecValTok{14}  \DecValTok{4}  \DecValTok{9} \DecValTok{12} \DecValTok{30}  \DecValTok{8} \DecValTok{27} \DecValTok{18} \DecValTok{16} \DecValTok{10} \DecValTok{13}  \DecValTok{6} \DecValTok{24}  \DecValTok{3} \DecValTok{26}  \DecValTok{1}

\OperatorTok{$}\NormalTok{validation_set}
\NormalTok{[}\DecValTok{1}\NormalTok{]  }\DecValTok{2}  \DecValTok{5}  \DecValTok{7} \DecValTok{21} \DecValTok{22} \DecValTok{23}

\KeywordTok{attr}\NormalTok{(,}\StringTok{"class"}\NormalTok{)}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\StringTok{"fold"}
\end{Highlighting}
\end{Shaded}

\hypertarget{bootstrap}{%
\paragraph{Bootstrap}\label{bootstrap}}

The bootstrap cross-validation also consists of randomly selecting samples, with
replacement, for the training set. The rest of the samples not picked for the
training set are allocated to the validation set. This process is then repeated
multiple times, generating (at random) new training and validation partitions
each time. In contract to the Monte Carlo cross-validation, the total number of
samples in a training and validation size across folds is not constant. We also
sample with replacement, hence the same samples can be in multiple training
sets. The proportion of observations in the validation sets is a random
variable, with expectation \(\sim 0.368\).

We illustrate the usage of the bootstrap cross-validation with \texttt{origami} package
below using the function \texttt{folds\_bootstrap(n,\ V)}. In order to setup
\texttt{folds\_bootstrap(n,\ V)}, we need:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the total number of samples we want to cross-validate;
\item
  the number of folds.
\end{enumerate}

At \(V=2\), we obtain \(2\) folds with different number of samples in the validation
set across folds.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{folds <-}\StringTok{ }\KeywordTok{folds_bootstrap}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(washb_data), }\DataTypeTok{V =} \DecValTok{2}\NormalTok{)}
\NormalTok{folds[[}\DecValTok{1}\NormalTok{]]}
\OperatorTok{$}\NormalTok{v}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\DecValTok{1}

\OperatorTok{$}\NormalTok{training_set}
\NormalTok{ [}\DecValTok{1}\NormalTok{]  }\DecValTok{2}  \DecValTok{5} \DecValTok{30}  \DecValTok{1} \DecValTok{29} \DecValTok{16} \DecValTok{10} \DecValTok{11}  \DecValTok{8} \DecValTok{25} \DecValTok{28}  \DecValTok{2} \DecValTok{11}  \DecValTok{2} \DecValTok{16} \DecValTok{28} \DecValTok{15} \DecValTok{28}  \DecValTok{1} \DecValTok{27}  \DecValTok{9} \DecValTok{19} \DecValTok{20} \DecValTok{30} \DecValTok{18}
\NormalTok{[}\DecValTok{26}\NormalTok{] }\DecValTok{11} \DecValTok{13}  \DecValTok{2} \DecValTok{18} \DecValTok{12}

\OperatorTok{$}\NormalTok{validation_set}
\NormalTok{ [}\DecValTok{1}\NormalTok{]  }\DecValTok{3}  \DecValTok{4}  \DecValTok{6}  \DecValTok{7} \DecValTok{14} \DecValTok{17} \DecValTok{21} \DecValTok{22} \DecValTok{23} \DecValTok{24} \DecValTok{26}

\KeywordTok{attr}\NormalTok{(,}\StringTok{"class"}\NormalTok{)}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\StringTok{"fold"}
\NormalTok{folds[[}\DecValTok{2}\NormalTok{]]}
\OperatorTok{$}\NormalTok{v}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\DecValTok{2}

\OperatorTok{$}\NormalTok{training_set}
\NormalTok{ [}\DecValTok{1}\NormalTok{] }\DecValTok{12} \DecValTok{16} \DecValTok{10} \DecValTok{29} \DecValTok{22} \DecValTok{15} \DecValTok{27}  \DecValTok{9} \DecValTok{27} \DecValTok{16} \DecValTok{12} \DecValTok{28} \DecValTok{10} \DecValTok{28} \DecValTok{26}  \DecValTok{1} \DecValTok{14}  \DecValTok{6} \DecValTok{23} \DecValTok{14} \DecValTok{21} \DecValTok{16}  \DecValTok{5} \DecValTok{20}  \DecValTok{8}
\NormalTok{[}\DecValTok{26}\NormalTok{] }\DecValTok{23} \DecValTok{25}  \DecValTok{8} \DecValTok{27}  \DecValTok{5}

\OperatorTok{$}\NormalTok{validation_set}
\NormalTok{ [}\DecValTok{1}\NormalTok{]  }\DecValTok{2}  \DecValTok{3}  \DecValTok{4}  \DecValTok{7} \DecValTok{11} \DecValTok{13} \DecValTok{17} \DecValTok{18} \DecValTok{19} \DecValTok{24} \DecValTok{30}

\KeywordTok{attr}\NormalTok{(,}\StringTok{"class"}\NormalTok{)}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\StringTok{"fold"}
\end{Highlighting}
\end{Shaded}

\hypertarget{cross-validation-for-dependent-data}{%
\subsubsection{Cross-validation for dependent data}\label{cross-validation-for-dependent-data}}

The \texttt{origami} package also supports numerous cross-validation schemes for
time-series data, for both single and multiple time-series with arbitrary time
and network dependence.

\hypertarget{airpassenger-example}{%
\subsubsection*{AirPassenger Example}\label{airpassenger-example}}


In order to illustrate different cross-validation schemes for time-series, we
will be using the AirPassenger data; this is a widely used, freely available
dataset. The AirPassenger dataset in \texttt{R} provides monthly totals of
international airline passengers from 1949 to 1960. This dataset is already of a
time series class therefore no further class or date manipulation is required.

\textbf{Goal:} we want to forecast the number of airline passengers at time \(h\)
horizon using the historical data from 1949 to 1960.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggfortify)}

\KeywordTok{data}\NormalTok{(AirPassengers)}
\NormalTok{AP <-}\StringTok{ }\NormalTok{AirPassengers}

\KeywordTok{autoplot}\NormalTok{(AP) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}
    \DataTypeTok{x =} \StringTok{"Date"}\NormalTok{,}
    \DataTypeTok{y =} \StringTok{"Passenger numbers (1000's)"}\NormalTok{,}
    \DataTypeTok{title =} \StringTok{"Air Passengers from 1949 to 1961"}
\NormalTok{  )}

\NormalTok{t <-}\StringTok{ }\KeywordTok{length}\NormalTok{(AP)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{05-origami_files/figure-latex/plot_airpass-1} \end{center}

\hypertarget{rolling-origin}{%
\paragraph{Rolling origin}\label{rolling-origin}}

Rolling origin cross-validation scheme lends itself to ``online'' algorithms,
where large streams of data have to be fit continually, and the final fit is
constantly updated with more data acquired. In general, the rolling origin
scheme defines an initial training set, and with each iteration the size of the
training set grows by \(m\) observations until we reach time \(t\) for a particular
fold. The time points included in the training set are always behind the
validation set time points; in addition, there might be a gap between training
and validation times of size \(h\).

To further illustrate rolling origin cross-validation, we show below an example
with 3 folds. Here, the first window size is 15 time points, on which we first
train the proposed algorithm. We then evaluate its performance on 10 time
points, with a gap of size 5 between the training and validation time points.
For the following fold, we train the algorithm on a longer stream of data, 25
time points, including the original 15 we started with. We then evaluate its
performance on 10 time points in the future.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img/image/rolling_origin} 

}

\caption{Rolling origin CV}\label{fig:unnamed-chunk-1}
\end{figure}

We illustrate the usage of the rolling origin cross-validation with \texttt{origami}
package below using the function \texttt{folds\_rolling\_origin(n,\ first\_window,\ validation\_size,\ gap,\ batch)}. In order to setup \texttt{folds\_rolling\_origin(n,\ first\_window,\ validation\_size,\ gap,\ batch)}, we need:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the total number of time points we want to cross-validate
\item
  the size of the first training set
\item
  the size of the validation set
\item
  the gap between training and validation set
\item
  the size of the update on the training set per each iteration of CV
\end{enumerate}

Our time-series has \(t=144\) time points. Setting the \texttt{first\_window} to \(50\),
\texttt{validation\_size} to 10, \texttt{gap} to 5 and \texttt{batch} to 20, we get 4 time-series
folds; we show the first two below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{folds <-}\StringTok{ }\KeywordTok{folds_rolling_origin}\NormalTok{(}
\NormalTok{  t,}
  \DataTypeTok{first_window =} \DecValTok{50}\NormalTok{, }\DataTypeTok{validation_size =} \DecValTok{10}\NormalTok{, }\DataTypeTok{gap =} \DecValTok{5}\NormalTok{, }\DataTypeTok{batch =} \DecValTok{20}
\NormalTok{)}
\NormalTok{folds[[}\DecValTok{1}\NormalTok{]]}
\OperatorTok{$}\NormalTok{v}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\DecValTok{1}

\OperatorTok{$}\NormalTok{training_set}
\NormalTok{ [}\DecValTok{1}\NormalTok{]  }\DecValTok{1}  \DecValTok{2}  \DecValTok{3}  \DecValTok{4}  \DecValTok{5}  \DecValTok{6}  \DecValTok{7}  \DecValTok{8}  \DecValTok{9} \DecValTok{10} \DecValTok{11} \DecValTok{12} \DecValTok{13} \DecValTok{14} \DecValTok{15} \DecValTok{16} \DecValTok{17} \DecValTok{18} \DecValTok{19} \DecValTok{20} \DecValTok{21} \DecValTok{22} \DecValTok{23} \DecValTok{24} \DecValTok{25}
\NormalTok{[}\DecValTok{26}\NormalTok{] }\DecValTok{26} \DecValTok{27} \DecValTok{28} \DecValTok{29} \DecValTok{30} \DecValTok{31} \DecValTok{32} \DecValTok{33} \DecValTok{34} \DecValTok{35} \DecValTok{36} \DecValTok{37} \DecValTok{38} \DecValTok{39} \DecValTok{40} \DecValTok{41} \DecValTok{42} \DecValTok{43} \DecValTok{44} \DecValTok{45} \DecValTok{46} \DecValTok{47} \DecValTok{48} \DecValTok{49} \DecValTok{50}

\OperatorTok{$}\NormalTok{validation_set}
\NormalTok{ [}\DecValTok{1}\NormalTok{] }\DecValTok{56} \DecValTok{57} \DecValTok{58} \DecValTok{59} \DecValTok{60} \DecValTok{61} \DecValTok{62} \DecValTok{63} \DecValTok{64} \DecValTok{65}

\KeywordTok{attr}\NormalTok{(,}\StringTok{"class"}\NormalTok{)}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\StringTok{"fold"}
\NormalTok{folds[[}\DecValTok{2}\NormalTok{]]}
\OperatorTok{$}\NormalTok{v}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\DecValTok{2}

\OperatorTok{$}\NormalTok{training_set}
\NormalTok{ [}\DecValTok{1}\NormalTok{]  }\DecValTok{1}  \DecValTok{2}  \DecValTok{3}  \DecValTok{4}  \DecValTok{5}  \DecValTok{6}  \DecValTok{7}  \DecValTok{8}  \DecValTok{9} \DecValTok{10} \DecValTok{11} \DecValTok{12} \DecValTok{13} \DecValTok{14} \DecValTok{15} \DecValTok{16} \DecValTok{17} \DecValTok{18} \DecValTok{19} \DecValTok{20} \DecValTok{21} \DecValTok{22} \DecValTok{23} \DecValTok{24} \DecValTok{25}
\NormalTok{[}\DecValTok{26}\NormalTok{] }\DecValTok{26} \DecValTok{27} \DecValTok{28} \DecValTok{29} \DecValTok{30} \DecValTok{31} \DecValTok{32} \DecValTok{33} \DecValTok{34} \DecValTok{35} \DecValTok{36} \DecValTok{37} \DecValTok{38} \DecValTok{39} \DecValTok{40} \DecValTok{41} \DecValTok{42} \DecValTok{43} \DecValTok{44} \DecValTok{45} \DecValTok{46} \DecValTok{47} \DecValTok{48} \DecValTok{49} \DecValTok{50}
\NormalTok{[}\DecValTok{51}\NormalTok{] }\DecValTok{51} \DecValTok{52} \DecValTok{53} \DecValTok{54} \DecValTok{55} \DecValTok{56} \DecValTok{57} \DecValTok{58} \DecValTok{59} \DecValTok{60} \DecValTok{61} \DecValTok{62} \DecValTok{63} \DecValTok{64} \DecValTok{65} \DecValTok{66} \DecValTok{67} \DecValTok{68} \DecValTok{69} \DecValTok{70}

\OperatorTok{$}\NormalTok{validation_set}
\NormalTok{ [}\DecValTok{1}\NormalTok{] }\DecValTok{76} \DecValTok{77} \DecValTok{78} \DecValTok{79} \DecValTok{80} \DecValTok{81} \DecValTok{82} \DecValTok{83} \DecValTok{84} \DecValTok{85}

\KeywordTok{attr}\NormalTok{(,}\StringTok{"class"}\NormalTok{)}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\StringTok{"fold"}
\end{Highlighting}
\end{Shaded}

\hypertarget{rolling-window}{%
\paragraph{Rolling window}\label{rolling-window}}

Instead of adding more time points to the training set per each iteration, the
rolling window cross-validation scheme ``rolls'' the training sample forward by
\(m\) time units. The rolling window scheme might be considered in parametric
settings when one wishes to guard against moment or parameter drift that is
difficult to model explicitly; it is also more efficient for computationally
demanding settings such as streaming data, in which large amounts of training
data cannot be stored. In contrast to rolling origin CV, the training sample for
each iteration of the rolling window scheme is always the same.

To illustrate the rolling window cross-validation with 3 time-series folds
below. The first window size is 15 time points, on which we first train the
proposed algorithm. As in the previous illustration, we evaluate its performance
on 10 time points, with a gap of size 5 between the training and validation time
points. However, for the next fold, we train the algorithm on time points
further away from the origin (here, 10 time points). Note that the size of the
training set in the new fold is the same as in the first fold (15 time points).
This setup keeps the training sets comparable over time (and fold) as compared
to the rolling origin CV. We then evaluate the performance of the proposed
algorithm on 10 time points in the future.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img/image/rolling_window} 

}

\caption{Rolling window CV}\label{fig:unnamed-chunk-2}
\end{figure}

We illustrate the usage of the rolling window cross-validation with \texttt{origami}
package below using the function \texttt{folds\_rolling\_window(n,\ window\_size,\ validation\_size,\ gap,\ batch)}. In order to setup \texttt{folds\_rolling\_window(n,\ window\_size,\ validation\_size,\ gap,\ batch)}, we need:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the total number of time points we want to cross-validate
\item
  the size of the training sets
\item
  the size of the validation set
\item
  the gap between training and validation set
\item
  the size of the update on the training set per each iteration of CV
\end{enumerate}

Setting the \texttt{window\_size} to \(50\), \texttt{validation\_size} to 10, \texttt{gap} to 5 and
\texttt{batch} to 20, we also get 4 time-series folds; we show the first two below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{folds <-}\StringTok{ }\KeywordTok{folds_rolling_window}\NormalTok{(}
\NormalTok{  t,}
  \DataTypeTok{window_size =} \DecValTok{50}\NormalTok{, }\DataTypeTok{validation_size =} \DecValTok{10}\NormalTok{, }\DataTypeTok{gap =} \DecValTok{5}\NormalTok{, }\DataTypeTok{batch =} \DecValTok{20}
\NormalTok{)}
\NormalTok{folds[[}\DecValTok{1}\NormalTok{]]}
\OperatorTok{$}\NormalTok{v}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\DecValTok{1}

\OperatorTok{$}\NormalTok{training_set}
\NormalTok{ [}\DecValTok{1}\NormalTok{]  }\DecValTok{1}  \DecValTok{2}  \DecValTok{3}  \DecValTok{4}  \DecValTok{5}  \DecValTok{6}  \DecValTok{7}  \DecValTok{8}  \DecValTok{9} \DecValTok{10} \DecValTok{11} \DecValTok{12} \DecValTok{13} \DecValTok{14} \DecValTok{15} \DecValTok{16} \DecValTok{17} \DecValTok{18} \DecValTok{19} \DecValTok{20} \DecValTok{21} \DecValTok{22} \DecValTok{23} \DecValTok{24} \DecValTok{25}
\NormalTok{[}\DecValTok{26}\NormalTok{] }\DecValTok{26} \DecValTok{27} \DecValTok{28} \DecValTok{29} \DecValTok{30} \DecValTok{31} \DecValTok{32} \DecValTok{33} \DecValTok{34} \DecValTok{35} \DecValTok{36} \DecValTok{37} \DecValTok{38} \DecValTok{39} \DecValTok{40} \DecValTok{41} \DecValTok{42} \DecValTok{43} \DecValTok{44} \DecValTok{45} \DecValTok{46} \DecValTok{47} \DecValTok{48} \DecValTok{49} \DecValTok{50}

\OperatorTok{$}\NormalTok{validation_set}
\NormalTok{ [}\DecValTok{1}\NormalTok{] }\DecValTok{56} \DecValTok{57} \DecValTok{58} \DecValTok{59} \DecValTok{60} \DecValTok{61} \DecValTok{62} \DecValTok{63} \DecValTok{64} \DecValTok{65}

\KeywordTok{attr}\NormalTok{(,}\StringTok{"class"}\NormalTok{)}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\StringTok{"fold"}
\NormalTok{folds[[}\DecValTok{2}\NormalTok{]]}
\OperatorTok{$}\NormalTok{v}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\DecValTok{2}

\OperatorTok{$}\NormalTok{training_set}
\NormalTok{ [}\DecValTok{1}\NormalTok{] }\DecValTok{21} \DecValTok{22} \DecValTok{23} \DecValTok{24} \DecValTok{25} \DecValTok{26} \DecValTok{27} \DecValTok{28} \DecValTok{29} \DecValTok{30} \DecValTok{31} \DecValTok{32} \DecValTok{33} \DecValTok{34} \DecValTok{35} \DecValTok{36} \DecValTok{37} \DecValTok{38} \DecValTok{39} \DecValTok{40} \DecValTok{41} \DecValTok{42} \DecValTok{43} \DecValTok{44} \DecValTok{45}
\NormalTok{[}\DecValTok{26}\NormalTok{] }\DecValTok{46} \DecValTok{47} \DecValTok{48} \DecValTok{49} \DecValTok{50} \DecValTok{51} \DecValTok{52} \DecValTok{53} \DecValTok{54} \DecValTok{55} \DecValTok{56} \DecValTok{57} \DecValTok{58} \DecValTok{59} \DecValTok{60} \DecValTok{61} \DecValTok{62} \DecValTok{63} \DecValTok{64} \DecValTok{65} \DecValTok{66} \DecValTok{67} \DecValTok{68} \DecValTok{69} \DecValTok{70}

\OperatorTok{$}\NormalTok{validation_set}
\NormalTok{ [}\DecValTok{1}\NormalTok{] }\DecValTok{76} \DecValTok{77} \DecValTok{78} \DecValTok{79} \DecValTok{80} \DecValTok{81} \DecValTok{82} \DecValTok{83} \DecValTok{84} \DecValTok{85}

\KeywordTok{attr}\NormalTok{(,}\StringTok{"class"}\NormalTok{)}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\StringTok{"fold"}
\end{Highlighting}
\end{Shaded}

\hypertarget{rolling-origin-with-v-fold}{%
\paragraph{Rolling origin with V-fold}\label{rolling-origin-with-v-fold}}

A variant of rolling origin scheme which accounts for sample dependence is the
rolling-origin-\(V\)-fold cross-validation. In contrast to the canonical rolling
origin CV, samples in the training and validation set are not the same, as the
variant encompasses \(V\)-fold CV in addition to the time-series setup. The
predictions are evaluated on the future times of time-series units not seen
during the training step, allowing for dependence in both samples and time. One
can use the rolling-origin-\(v\)-fold cross-validation with \texttt{origami} package
using the function \texttt{folds\_vfold\_rolling\_origin\_pooled(n,\ t,\ id,\ time,\ V,\ first\_window,\ validation\_size,\ gap,\ batch)}. In the figure below, we show \(V=2\)
\(V\)-folds, and 2 time-series CV folds.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img/image/rolling_origin_v_fold} 

}

\caption{Rolling origin V-fold CV}\label{fig:unnamed-chunk-3}
\end{figure}

\hypertarget{rolling-window-with-v-fold}{%
\paragraph{Rolling window with v-fold}\label{rolling-window-with-v-fold}}

Analogous to the previous section, we can extend rolling window CV to support
multiple time-series with arbitrary sample dependence. One can use the
rolling-window-\(V\)-fold cross-validation with \texttt{origami} package using the
function \texttt{folds\_vfold\_rolling\_window\_pooled(n,\ t,\ id,\ time,\ V,\ window\_size,\ validation\_size,\ gap,\ batch)}. In the figure below, we show \(V=2\) \(V\)-folds, and
2 time-series CV folds.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img/image/rolling_window_v_fold} 

}

\caption{Rolling window V-fold CV}\label{fig:unnamed-chunk-4}
\end{figure}

\hypertarget{general-workflow-of-origami}{%
\subsection{\texorpdfstring{General workflow of \texttt{origami}}{General workflow of origami}}\label{general-workflow-of-origami}}

Before we dive into more details, let's take a moment to review some of the
basic functionality in \texttt{origami} R package. The main function in the \texttt{origami}
is \texttt{cross\_validate}. To start off, the user must define folds and a function
that operates on each fold. Once these are passed to \texttt{cross\_validate}, this
function will map the fold-specific function across the folds, combining the
results in a reasonable way. We will see this in action in later sections; for
now, we provide specific details on each each step of this process below.

\hypertarget{define-folds}{%
\subsubsection{(1) Define folds}\label{define-folds}}

The \texttt{folds} object passed to \texttt{cross\_validate} is a list of folds; such lists can
be generated using the \texttt{make\_folds} function. Each fold consists of a list with
a \texttt{training} index vector, a \texttt{validation} index vector, and a \texttt{fold\_index} (its
order in the list of folds). This function supports a variety of
cross-validation schemes we describe in the following section. The \texttt{make\_folds}
can balance across levels of a variable (\texttt{strata\_ids}), and it can also keep
all observations from the same independent unit together (\texttt{cluster}).

\hypertarget{define-fold-function}{%
\subsubsection{(2) Define fold function}\label{define-fold-function}}

The \texttt{cv\_fun} argument to \texttt{cross\_validate} is a function that will perform some
operation on each fold. The first argument to this function must be \texttt{fold},
which will receive an individual fold object to operate on. Additional arguments
can be passed to \texttt{cv\_fun} using the \texttt{...} argument to \texttt{cross\_validate}. Within
this function, the convenience functions \texttt{training}, \texttt{validation} and
\texttt{fold\_index} can return the various components of a fold object. If \texttt{training}
or \texttt{validation} is passed an object, it will index into it in a sensible way.
For instance, if it is a vector, it will index the vector directly. If it is a
\texttt{data.frame} or \texttt{matrix}, it will index rows. This allows the user to easily
partition data into training and validation sets. The fold function must return
a named list of results containing whatever fold-specific outputs are generated.

\hypertarget{apply-cross_validate}{%
\subsubsection{\texorpdfstring{(3) Apply \texttt{cross\_validate}}{(3) Apply cross\_validate}}\label{apply-cross_validate}}

After defining folds, \texttt{cross\_validate} can be used to map the \texttt{cv\_fun} across
the \texttt{folds} using \texttt{future\_lapply}. This means that it can be easily parallelized
by specifying a parallelization scheme (i.e., a \texttt{plan} from the \href{https://Cran.R-project.org/package=future}{future
parallelization framework for \texttt{R}}
\citep{bengtsson2020unifying}). The application of \texttt{cross\_validate} generates a list
of results. As described above, each call to \texttt{cv\_fun} itself returns a list of
results, with different elements for each type of result we care about. The main
loop generates a list of these individual lists of results (a sort of
``meta-list''). This ``meta-list'' is then inverted such that there is one element
per result type (this too is a list of the results for each fold). By default,
\texttt{combine\_results} is used to combine these results type lists in a sensible
manner. How results are combined is determined automatically by examining the
data types of the results from the first fold. This can be modified by
specifying a list of arguments to \texttt{.combine\_control}.

\hypertarget{cross-validation-in-action}{%
\subsection{Cross-validation in action}\label{cross-validation-in-action}}

Let's see \texttt{origami} in action! In the following chapter we will learn how to use
cross-validation with the Super Learner, and how we can utilize the power of
cross-validation to build optimal ensembles of algorithms, not just its use on a
single statistical learning method.

\hypertarget{cross-validation-with-linear-regression}{%
\subsubsection{Cross-validation with linear regression}\label{cross-validation-with-linear-regression}}

First, we will load the relevant \texttt{R} packages, set a seed, and load the full
WASH data once again. In order to illustrate cross-validation with \texttt{origami} and
linear regression, we will focus on predicting the weight-for-height Z-score
\texttt{whz} using all of the available covariate data. As stated previously, we will
assume the data is independent and identically distributed, ignoring the cluster
structure imposed by the clinical trial design. For the sake of illustration, we
will work with a subset of data, and remove all samples with missing data from
the dataset; we will learn in the next chapter how to deal with missingness.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(stringr)}
\KeywordTok{library}\NormalTok{(dplyr)}
\KeywordTok{library}\NormalTok{(tidyr)}

\CommentTok{# load data set and take a peek}
\NormalTok{washb_data <-}\StringTok{ }\KeywordTok{fread}\NormalTok{(}
  \KeywordTok{paste0}\NormalTok{(}
    \StringTok{"https://raw.githubusercontent.com/tlverse/tlverse-data/master/"}\NormalTok{,}
    \StringTok{"wash-benefits/washb_data.csv"}
\NormalTok{  ),}
  \DataTypeTok{stringsAsFactors =} \OtherTok{TRUE}
\NormalTok{)}

\CommentTok{# Remove missing data, then pick just the first 500 rows}
\NormalTok{washb_data <-}\StringTok{ }\NormalTok{washb_data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{drop_na}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{slice}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{500}\NormalTok{)}

\NormalTok{outcome <-}\StringTok{ "whz"}
\NormalTok{covars <-}\StringTok{ }\KeywordTok{colnames}\NormalTok{(washb_data)[}\OperatorTok{-}\KeywordTok{which}\NormalTok{(}\KeywordTok{names}\NormalTok{(washb_data) }\OperatorTok{==}\StringTok{ }\NormalTok{outcome)]}
\end{Highlighting}
\end{Shaded}

Here's a look at the data:

\begin{tabular}{r|l|l|r|r|l|r|l|r|l|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r}
\hline
whz & tr & fracode & month & aged & sex & momage & momedu & momheight & hfiacat & Nlt18 & Ncomp & watmin & elec & floor & walls & roof & asset\_wardrobe & asset\_table & asset\_chair & asset\_khat & asset\_chouki & asset\_tv & asset\_refrig & asset\_bike & asset\_moto & asset\_sewmach & asset\_mobile\\
\hline
0.00 & Control & N05265 & 9 & 268 & male & 30 & Primary (1-5y) & 146.40 & Food Secure & 3 & 11 & 0 & 1 & 0 & 1 & 1 & 0 & 1 & 1 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 1\\
\hline
-1.16 & Control & N05265 & 9 & 286 & male & 25 & Primary (1-5y) & 148.75 & Moderately Food Insecure & 2 & 4 & 0 & 1 & 0 & 1 & 1 & 0 & 1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 1\\
\hline
-1.05 & Control & N08002 & 9 & 264 & male & 25 & Primary (1-5y) & 152.15 & Food Secure & 1 & 10 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1\\
\hline
-1.26 & Control & N08002 & 9 & 252 & female & 28 & Primary (1-5y) & 140.25 & Food Secure & 3 & 5 & 0 & 1 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 1\\
\hline
-0.59 & Control & N06531 & 9 & 336 & female & 19 & Secondary (>5y) & 150.95 & Food Secure & 2 & 7 & 0 & 1 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 1\\
\hline
-0.51 & Control & N06531 & 9 & 304 & male & 20 & Secondary (>5y) & 154.20 & Severely Food Insecure & 0 & 3 & 1 & 1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1\\
\hline
\end{tabular}

We can see the covariates used in the prediction:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{outcome}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\StringTok{"whz"}
\NormalTok{covars}
\NormalTok{ [}\DecValTok{1}\NormalTok{] }\StringTok{"tr"}             \StringTok{"fracode"}        \StringTok{"month"}          \StringTok{"aged"}          
\NormalTok{ [}\DecValTok{5}\NormalTok{] }\StringTok{"sex"}            \StringTok{"momage"}         \StringTok{"momedu"}         \StringTok{"momheight"}     
\NormalTok{ [}\DecValTok{9}\NormalTok{] }\StringTok{"hfiacat"}        \StringTok{"Nlt18"}          \StringTok{"Ncomp"}          \StringTok{"watmin"}        
\NormalTok{[}\DecValTok{13}\NormalTok{] }\StringTok{"elec"}           \StringTok{"floor"}          \StringTok{"walls"}          \StringTok{"roof"}          
\NormalTok{[}\DecValTok{17}\NormalTok{] }\StringTok{"asset_wardrobe"} \StringTok{"asset_table"}    \StringTok{"asset_chair"}    \StringTok{"asset_khat"}    
\NormalTok{[}\DecValTok{21}\NormalTok{] }\StringTok{"asset_chouki"}   \StringTok{"asset_tv"}       \StringTok{"asset_refrig"}   \StringTok{"asset_bike"}    
\NormalTok{[}\DecValTok{25}\NormalTok{] }\StringTok{"asset_moto"}     \StringTok{"asset_sewmach"}  \StringTok{"asset_mobile"}  
\end{Highlighting}
\end{Shaded}

Next, we fit a linear model on the full data, with the goal of predicting the
weight-for-height Z-score \texttt{whz} using all of the available covariate data. Let's
try it out:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm_mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(whz }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ washb_data)}
\KeywordTok{summary}\NormalTok{(lm_mod)}

\NormalTok{Call}\OperatorTok{:}
\KeywordTok{lm}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ whz }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ washb_data)}

\NormalTok{Residuals}\OperatorTok{:}
\StringTok{    }\NormalTok{Min      1Q  Median      3Q     Max }
\FloatTok{-2.8890} \FloatTok{-0.6799} \FloatTok{-0.0169}  \FloatTok{0.6595}  \FloatTok{3.1005} 

\NormalTok{Coefficients}\OperatorTok{:}
\StringTok{                                }\NormalTok{Estimate Std. Error t value }\KeywordTok{Pr}\NormalTok{(}\OperatorTok{>}\ErrorTok{|}\NormalTok{t}\OperatorTok{|}\NormalTok{)   }
\NormalTok{(Intercept)                     }\FloatTok{-1.89006}    \FloatTok{1.72022}   \FloatTok{-1.10}   \FloatTok{0.2725}   
\NormalTok{trHandwashing                   }\FloatTok{-0.25276}    \FloatTok{0.17032}   \FloatTok{-1.48}   \FloatTok{0.1385}   
\NormalTok{trNutrition                     }\FloatTok{-0.09695}    \FloatTok{0.15696}   \FloatTok{-0.62}   \FloatTok{0.5371}   
\NormalTok{trNutrition }\OperatorTok{+}\StringTok{ }\NormalTok{WSH               }\FloatTok{-0.09587}    \FloatTok{0.16528}   \FloatTok{-0.58}   \FloatTok{0.5622}   
\NormalTok{trSanitation                    }\FloatTok{-0.27702}    \FloatTok{0.15846}   \FloatTok{-1.75}   \FloatTok{0.0811}\NormalTok{ . }
\NormalTok{trWSH                           }\FloatTok{-0.02846}    \FloatTok{0.15967}   \FloatTok{-0.18}   \FloatTok{0.8586}   
\NormalTok{trWater                         }\FloatTok{-0.07148}    \FloatTok{0.15813}   \FloatTok{-0.45}   \FloatTok{0.6515}   
\NormalTok{fracodeN05160                    }\FloatTok{0.62355}    \FloatTok{0.30719}    \FloatTok{2.03}   \FloatTok{0.0430} \OperatorTok{*}\StringTok{ }
\NormalTok{fracodeN05265                    }\FloatTok{0.38762}    \FloatTok{0.31011}    \FloatTok{1.25}   \FloatTok{0.2120}   
\NormalTok{fracodeN05359                    }\FloatTok{0.10187}    \FloatTok{0.31329}    \FloatTok{0.33}   \FloatTok{0.7452}   
\NormalTok{fracodeN06229                    }\FloatTok{0.30933}    \FloatTok{0.29766}    \FloatTok{1.04}   \FloatTok{0.2993}   
\NormalTok{fracodeN06453                    }\FloatTok{0.08066}    \FloatTok{0.30006}    \FloatTok{0.27}   \FloatTok{0.7882}   
\NormalTok{fracodeN06458                    }\FloatTok{0.43707}    \FloatTok{0.29970}    \FloatTok{1.46}   \FloatTok{0.1454}   
\NormalTok{fracodeN06473                    }\FloatTok{0.45406}    \FloatTok{0.30912}    \FloatTok{1.47}   \FloatTok{0.1426}   
\NormalTok{fracodeN06479                    }\FloatTok{0.60994}    \FloatTok{0.31463}    \FloatTok{1.94}   \FloatTok{0.0532}\NormalTok{ . }
\NormalTok{fracodeN06489                    }\FloatTok{0.25923}    \FloatTok{0.31901}    \FloatTok{0.81}   \FloatTok{0.4169}   
\NormalTok{fracodeN06500                    }\FloatTok{0.07539}    \FloatTok{0.35794}    \FloatTok{0.21}   \FloatTok{0.8333}   
\NormalTok{fracodeN06502                    }\FloatTok{0.36748}    \FloatTok{0.30504}    \FloatTok{1.20}   \FloatTok{0.2290}   
\NormalTok{fracodeN06505                    }\FloatTok{0.20038}    \FloatTok{0.31560}    \FloatTok{0.63}   \FloatTok{0.5258}   
\NormalTok{fracodeN06516                    }\FloatTok{0.55455}    \FloatTok{0.29807}    \FloatTok{1.86}   \FloatTok{0.0635}\NormalTok{ . }
\NormalTok{fracodeN06524                    }\FloatTok{0.49429}    \FloatTok{0.31423}    \FloatTok{1.57}   \FloatTok{0.1164}   
\NormalTok{fracodeN06528                    }\FloatTok{0.75966}    \FloatTok{0.31060}    \FloatTok{2.45}   \FloatTok{0.0148} \OperatorTok{*}\StringTok{ }
\NormalTok{fracodeN06531                    }\FloatTok{0.36856}    \FloatTok{0.30155}    \FloatTok{1.22}   \FloatTok{0.2223}   
\NormalTok{fracodeN06862                    }\FloatTok{0.56932}    \FloatTok{0.29293}    \FloatTok{1.94}   \FloatTok{0.0526}\NormalTok{ . }
\NormalTok{fracodeN08002                    }\FloatTok{0.36779}    \FloatTok{0.26846}    \FloatTok{1.37}   \FloatTok{0.1714}   
\NormalTok{month                            }\FloatTok{0.17161}    \FloatTok{0.10865}    \FloatTok{1.58}   \FloatTok{0.1149}   
\NormalTok{aged                            }\FloatTok{-0.00336}    \FloatTok{0.00112}   \FloatTok{-3.00}   \FloatTok{0.0029} \OperatorTok{**}
\NormalTok{sexmale                          }\FloatTok{0.12352}    \FloatTok{0.09203}    \FloatTok{1.34}   \FloatTok{0.1802}   
\NormalTok{momage                          }\FloatTok{-0.01379}    \FloatTok{0.00973}   \FloatTok{-1.42}   \FloatTok{0.1570}   
\KeywordTok{momeduPrimary}\NormalTok{ (}\DecValTok{1}\OperatorTok{-}\NormalTok{5y)            }\FloatTok{-0.13214}    \FloatTok{0.15225}   \FloatTok{-0.87}   \FloatTok{0.3859}   
\KeywordTok{momeduSecondary}\NormalTok{ (}\OperatorTok{>}\NormalTok{5y)            }\FloatTok{0.12632}    \FloatTok{0.16041}    \FloatTok{0.79}   \FloatTok{0.4314}   
\NormalTok{momheight                        }\FloatTok{0.00512}    \FloatTok{0.00919}    \FloatTok{0.56}   \FloatTok{0.5776}   
\NormalTok{hfiacatMildly Food Insecure      }\FloatTok{0.05804}    \FloatTok{0.19341}    \FloatTok{0.30}   \FloatTok{0.7643}   
\NormalTok{hfiacatModerately Food Insecure }\FloatTok{-0.01362}    \FloatTok{0.12887}   \FloatTok{-0.11}   \FloatTok{0.9159}   
\NormalTok{hfiacatSeverely Food Insecure   }\FloatTok{-0.13447}    \FloatTok{0.25418}   \FloatTok{-0.53}   \FloatTok{0.5970}   
\NormalTok{Nlt18                           }\FloatTok{-0.02557}    \FloatTok{0.04060}   \FloatTok{-0.63}   \FloatTok{0.5291}   
\NormalTok{Ncomp                            }\FloatTok{0.00179}    \FloatTok{0.00762}    \FloatTok{0.23}   \FloatTok{0.8145}   
\NormalTok{watmin                           }\FloatTok{0.01347}    \FloatTok{0.00861}    \FloatTok{1.57}   \FloatTok{0.1182}   
\NormalTok{elec                             }\FloatTok{0.08906}    \FloatTok{0.10700}    \FloatTok{0.83}   \FloatTok{0.4057}   
\NormalTok{floor                           }\FloatTok{-0.17763}    \FloatTok{0.17734}   \FloatTok{-1.00}   \FloatTok{0.3171}   
\NormalTok{walls                           }\FloatTok{-0.03001}    \FloatTok{0.21445}   \FloatTok{-0.14}   \FloatTok{0.8888}   
\NormalTok{roof                            }\FloatTok{-0.03716}    \FloatTok{0.49214}   \FloatTok{-0.08}   \FloatTok{0.9399}   
\NormalTok{asset_wardrobe                  }\FloatTok{-0.05754}    \FloatTok{0.13736}   \FloatTok{-0.42}   \FloatTok{0.6755}   
\NormalTok{asset_table                     }\FloatTok{-0.22079}    \FloatTok{0.12276}   \FloatTok{-1.80}   \FloatTok{0.0728}\NormalTok{ . }
\NormalTok{asset_chair                      }\FloatTok{0.28012}    \FloatTok{0.13750}    \FloatTok{2.04}   \FloatTok{0.0422} \OperatorTok{*}\StringTok{ }
\NormalTok{asset_khat                       }\FloatTok{0.02306}    \FloatTok{0.11766}    \FloatTok{0.20}   \FloatTok{0.8447}   
\NormalTok{asset_chouki                    }\FloatTok{-0.13943}    \FloatTok{0.14084}   \FloatTok{-0.99}   \FloatTok{0.3227}   
\NormalTok{asset_tv                         }\FloatTok{0.17723}    \FloatTok{0.12972}    \FloatTok{1.37}   \FloatTok{0.1726}   
\NormalTok{asset_refrig                     }\FloatTok{0.12613}    \FloatTok{0.23162}    \FloatTok{0.54}   \FloatTok{0.5863}   
\NormalTok{asset_bike                      }\FloatTok{-0.02568}    \FloatTok{0.10083}   \FloatTok{-0.25}   \FloatTok{0.7990}   
\NormalTok{asset_moto                      }\FloatTok{-0.32094}    \FloatTok{0.19944}   \FloatTok{-1.61}   \FloatTok{0.1083}   
\NormalTok{asset_sewmach                    }\FloatTok{0.05090}    \FloatTok{0.17795}    \FloatTok{0.29}   \FloatTok{0.7750}   
\NormalTok{asset_mobile                     }\FloatTok{0.01420}    \FloatTok{0.14972}    \FloatTok{0.09}   \FloatTok{0.9245}   
\OperatorTok{---}
\NormalTok{Signif. codes}\OperatorTok{:}\StringTok{  }\DecValTok{0} \StringTok{'***'} \FloatTok{0.001} \StringTok{'**'} \FloatTok{0.01} \StringTok{'*'} \FloatTok{0.05} \StringTok{'.'} \FloatTok{0.1} \StringTok{' '} \DecValTok{1}

\NormalTok{Residual standard error}\OperatorTok{:}\StringTok{ }\FloatTok{0.984}\NormalTok{ on }\DecValTok{447}\NormalTok{ degrees of freedom}
\NormalTok{Multiple R}\OperatorTok{-}\NormalTok{squared}\OperatorTok{:}\StringTok{  }\FloatTok{0.129}\NormalTok{, Adjusted R}\OperatorTok{-}\NormalTok{squared}\OperatorTok{:}\StringTok{  }\FloatTok{0.0277} 
\NormalTok{F}\OperatorTok{-}\NormalTok{statistic}\OperatorTok{:}\StringTok{ }\FloatTok{1.27}\NormalTok{ on }\DecValTok{52}\NormalTok{ and }\DecValTok{447}\NormalTok{ DF,  p}\OperatorTok{-}\NormalTok{value}\OperatorTok{:}\StringTok{ }\FloatTok{0.104}
\end{Highlighting}
\end{Shaded}

We can assess how well the model fits the data by comparing the predictions of
the linear model to the true outcomes observed in the data set. This is the well
known (and standard) mean squared error. We can extract that from the \texttt{lm} model
object like so:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(err <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(}\KeywordTok{resid}\NormalTok{(lm_mod)}\OperatorTok{^}\DecValTok{2}\NormalTok{))}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\FloatTok{0.86568}
\end{Highlighting}
\end{Shaded}

The mean squared error is 0.86568. There is an important problem that arises
when we assess the model in this way - that is, we have trained our linear
regression model on the full data set and assessed the error on the full data
set, using up all of our data. We, of course, are generally not interested in
how well the model explains variation in the observed data; rather, we are
interested in how the explanation provided by the model generalizes to a target
population from which the sample is presumably derived. Having used all of our
available data, we cannot honestly evaluate how well the model fits (and thus
explains) variation at the population level.

To resolve this issue, cross-validation allows for a particular procedure (e.g.,
linear regression) to be implemented over subsets of the data, evaluating how
well the procedure fits on a testing (``validation'') set, thereby providing an
honest evaluation of the error.

We can easily add cross-validation to our linear regression procedure using
\texttt{origami}. First, let us define a new function to perform linear regression on a
specific partition of the data (called a ``fold''):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cv_lm <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(fold, data, reg_form) \{}
  \CommentTok{# get name and index of outcome variable from regression formula}
\NormalTok{  out_var <-}\StringTok{ }\KeywordTok{as.character}\NormalTok{(}\KeywordTok{unlist}\NormalTok{(}\KeywordTok{str_split}\NormalTok{(reg_form, }\StringTok{" "}\NormalTok{))[}\DecValTok{1}\NormalTok{])}
\NormalTok{  out_var_ind <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{which}\NormalTok{(}\KeywordTok{colnames}\NormalTok{(data) }\OperatorTok{==}\StringTok{ }\NormalTok{out_var))}

  \CommentTok{# split up data into training and validation sets}
\NormalTok{  train_data <-}\StringTok{ }\KeywordTok{training}\NormalTok{(data)}
\NormalTok{  valid_data <-}\StringTok{ }\KeywordTok{validation}\NormalTok{(data)}

  \CommentTok{# fit linear model on training set and predict on validation set}
\NormalTok{  mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{as.formula}\NormalTok{(reg_form), }\DataTypeTok{data =}\NormalTok{ train_data)}
\NormalTok{  preds <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(mod, }\DataTypeTok{newdata =}\NormalTok{ valid_data)}
\NormalTok{  valid_data <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(valid_data)}

  \CommentTok{# capture results to be returned as output}
\NormalTok{  out <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
    \DataTypeTok{coef =} \KeywordTok{data.frame}\NormalTok{(}\KeywordTok{t}\NormalTok{(}\KeywordTok{coef}\NormalTok{(mod))),}
    \DataTypeTok{SE =}\NormalTok{ (preds }\OperatorTok{-}\StringTok{ }\NormalTok{valid_data[, out_var_ind])}\OperatorTok{^}\DecValTok{2}
\NormalTok{  )}
  \KeywordTok{return}\NormalTok{(out)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Our \texttt{cv\_lm} function is rather simple: we merely split the available data into a
training and validation sets, using the eponymous functions provided in
\texttt{origami}, fit the linear model on the training set, and evaluate the model on
the testing set. This is a simple example of what \texttt{origami} considers to be
\texttt{cv\_fun} -- functions for using cross-validation to perform a particular routine
over an input data set. Having defined such a function, we can simply generate a
set of partitions using \texttt{origami}'s \texttt{make\_folds} function, and apply our \texttt{cv\_lm}
function over the resultant \texttt{folds} object. Below, we replicate the
re-substitution estimate of the error -- we did this ``by hand'' above -- using
the functions \texttt{make\_folds} and \texttt{cv\_lm}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# re-substitution estimate}
\NormalTok{resub <-}\StringTok{ }\KeywordTok{make_folds}\NormalTok{(washb_data, }\DataTypeTok{fold_fun =}\NormalTok{ folds_resubstitution)[[}\DecValTok{1}\NormalTok{]]}
\NormalTok{resub_results <-}\StringTok{ }\KeywordTok{cv_lm}\NormalTok{(}\DataTypeTok{fold =}\NormalTok{ resub, }\DataTypeTok{data =}\NormalTok{ washb_data, }\DataTypeTok{reg_form =} \StringTok{"whz ~ ."}\NormalTok{)}
\KeywordTok{mean}\NormalTok{(resub_results}\OperatorTok{$}\NormalTok{SE, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\FloatTok{0.86568}
\end{Highlighting}
\end{Shaded}

This (nearly) matches the estimate of the error that we obtained above.

We can more honestly evaluate the error by V-fold cross-validation, which
partitions the data into \(v\) subsets, fitting the model on \(v - 1\) of the
subsets and evaluating on the subset that was held out for testing. This is
repeated such that each subset is used for testing. We can easily apply our
\texttt{cv\_lm} function using \texttt{origami}'s \texttt{cross\_validate} (n.b., by default this
performs 10-fold cross-validation):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# cross-validated estimate}
\NormalTok{folds <-}\StringTok{ }\KeywordTok{make_folds}\NormalTok{(washb_data)}
\NormalTok{cvlm_results <-}\StringTok{ }\KeywordTok{cross_validate}\NormalTok{(}
  \DataTypeTok{cv_fun =}\NormalTok{ cv_lm, }\DataTypeTok{folds =}\NormalTok{ folds, }\DataTypeTok{data =}\NormalTok{ washb_data, }\DataTypeTok{reg_form =} \StringTok{"whz ~ ."}\NormalTok{,}
  \DataTypeTok{use_future =} \OtherTok{FALSE}
\NormalTok{)}
\KeywordTok{mean}\NormalTok{(cvlm_results}\OperatorTok{$}\NormalTok{SE, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\FloatTok{1.35}
\end{Highlighting}
\end{Shaded}

Having performed 10-fold cross-validation, we quickly notice that our previous
estimate of the model error (by resubstitution) was a bit optimistic. The honest
estimate of the error is larger.

\hypertarget{cross-validation-with-random-forests}{%
\subsubsection{Cross-validation with random forests}\label{cross-validation-with-random-forests}}

To examine \texttt{origami} further, let us return to our example analysis using the
WASH data set. Here, we will write a new \texttt{cv\_fun} type object. As an example, we
will use Breiman's \texttt{randomForest} \citep{breiman2001random}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# make sure to load the package!}
\KeywordTok{library}\NormalTok{(randomForest)}

\NormalTok{cv_rf <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(fold, data, reg_form) \{}
  \CommentTok{# get name and index of outcome variable from regression formula}
\NormalTok{  out_var <-}\StringTok{ }\KeywordTok{as.character}\NormalTok{(}\KeywordTok{unlist}\NormalTok{(}\KeywordTok{str_split}\NormalTok{(reg_form, }\StringTok{" "}\NormalTok{))[}\DecValTok{1}\NormalTok{])}
\NormalTok{  out_var_ind <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{which}\NormalTok{(}\KeywordTok{colnames}\NormalTok{(data) }\OperatorTok{==}\StringTok{ }\NormalTok{out_var))}

  \CommentTok{# define training and validation sets based on input object of class "folds"}
\NormalTok{  train_data <-}\StringTok{ }\KeywordTok{training}\NormalTok{(data)}
\NormalTok{  valid_data <-}\StringTok{ }\KeywordTok{validation}\NormalTok{(data)}

  \CommentTok{# fit Random Forest regression on training set and predict on holdout set}
\NormalTok{  mod <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(}\DataTypeTok{formula =} \KeywordTok{as.formula}\NormalTok{(reg_form), }\DataTypeTok{data =}\NormalTok{ train_data)}
\NormalTok{  preds <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(mod, }\DataTypeTok{newdata =}\NormalTok{ valid_data)}
\NormalTok{  valid_data <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(valid_data)}

  \CommentTok{# define output object to be returned as list (for flexibility)}
\NormalTok{  out <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
    \DataTypeTok{coef =} \KeywordTok{data.frame}\NormalTok{(mod}\OperatorTok{$}\NormalTok{coefs),}
    \DataTypeTok{SE =}\NormalTok{ ((preds }\OperatorTok{-}\StringTok{ }\NormalTok{valid_data[, out_var_ind])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{  )}
  \KeywordTok{return}\NormalTok{(out)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Above, in writing our \texttt{cv\_rf} function to cross-validate \texttt{randomForest}, we used
our previous function \texttt{cv\_lm} as an example. For now, individual \texttt{cv\_fun} must
be written by hand; however, in future releases, a wrapper may be available to
support auto-generating \texttt{cv\_fun}s to be used with \texttt{origami}.

Below, we use \texttt{cross\_validate} to apply our new \texttt{cv\_rf} function over the \texttt{folds}
object generated by \texttt{make\_folds}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# now, let's cross-validate...}
\NormalTok{folds <-}\StringTok{ }\KeywordTok{make_folds}\NormalTok{(washb_data)}
\NormalTok{cvrf_results <-}\StringTok{ }\KeywordTok{cross_validate}\NormalTok{(}
  \DataTypeTok{cv_fun =}\NormalTok{ cv_rf, }\DataTypeTok{folds =}\NormalTok{ folds, }\DataTypeTok{data =}\NormalTok{ washb_data, }\DataTypeTok{reg_form =} \StringTok{"whz ~ ."}\NormalTok{,}
  \DataTypeTok{use_future =} \OtherTok{FALSE}
\NormalTok{)}
\KeywordTok{mean}\NormalTok{(cvrf_results}\OperatorTok{$}\NormalTok{SE)}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\FloatTok{1.0271}
\end{Highlighting}
\end{Shaded}

Using 10-fold cross-validation (the default), we obtain an honest estimate of
the prediction error of random forests. From this, we gather that the use of
\texttt{origami}'s \texttt{cross\_validate} procedure can be generalized to arbitrary estimation
techniques, given availability of an appropriate \texttt{cv\_fun} function.

\hypertarget{cross-validation-with-arima}{%
\subsubsection{Cross-validation with arima}\label{cross-validation-with-arima}}

Cross-validation can also be used for forecast model selection in a time series
setting. Here, the partitioning scheme mirrors the application of the
forecasting model: we'll train the data on past observations (either all
available or a recent subset), and then use the model fit to predict the next
few observations. We consider the \texttt{AirPassengers} dataset again, a monthly time
series of passenger air traffic in thousands of people.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(AirPassengers)}
\KeywordTok{print}\NormalTok{(AirPassengers)}
\NormalTok{     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec}
\DecValTok{1949} \DecValTok{112} \DecValTok{118} \DecValTok{132} \DecValTok{129} \DecValTok{121} \DecValTok{135} \DecValTok{148} \DecValTok{148} \DecValTok{136} \DecValTok{119} \DecValTok{104} \DecValTok{118}
\DecValTok{1950} \DecValTok{115} \DecValTok{126} \DecValTok{141} \DecValTok{135} \DecValTok{125} \DecValTok{149} \DecValTok{170} \DecValTok{170} \DecValTok{158} \DecValTok{133} \DecValTok{114} \DecValTok{140}
\DecValTok{1951} \DecValTok{145} \DecValTok{150} \DecValTok{178} \DecValTok{163} \DecValTok{172} \DecValTok{178} \DecValTok{199} \DecValTok{199} \DecValTok{184} \DecValTok{162} \DecValTok{146} \DecValTok{166}
\DecValTok{1952} \DecValTok{171} \DecValTok{180} \DecValTok{193} \DecValTok{181} \DecValTok{183} \DecValTok{218} \DecValTok{230} \DecValTok{242} \DecValTok{209} \DecValTok{191} \DecValTok{172} \DecValTok{194}
\DecValTok{1953} \DecValTok{196} \DecValTok{196} \DecValTok{236} \DecValTok{235} \DecValTok{229} \DecValTok{243} \DecValTok{264} \DecValTok{272} \DecValTok{237} \DecValTok{211} \DecValTok{180} \DecValTok{201}
\DecValTok{1954} \DecValTok{204} \DecValTok{188} \DecValTok{235} \DecValTok{227} \DecValTok{234} \DecValTok{264} \DecValTok{302} \DecValTok{293} \DecValTok{259} \DecValTok{229} \DecValTok{203} \DecValTok{229}
\DecValTok{1955} \DecValTok{242} \DecValTok{233} \DecValTok{267} \DecValTok{269} \DecValTok{270} \DecValTok{315} \DecValTok{364} \DecValTok{347} \DecValTok{312} \DecValTok{274} \DecValTok{237} \DecValTok{278}
\DecValTok{1956} \DecValTok{284} \DecValTok{277} \DecValTok{317} \DecValTok{313} \DecValTok{318} \DecValTok{374} \DecValTok{413} \DecValTok{405} \DecValTok{355} \DecValTok{306} \DecValTok{271} \DecValTok{306}
\DecValTok{1957} \DecValTok{315} \DecValTok{301} \DecValTok{356} \DecValTok{348} \DecValTok{355} \DecValTok{422} \DecValTok{465} \DecValTok{467} \DecValTok{404} \DecValTok{347} \DecValTok{305} \DecValTok{336}
\DecValTok{1958} \DecValTok{340} \DecValTok{318} \DecValTok{362} \DecValTok{348} \DecValTok{363} \DecValTok{435} \DecValTok{491} \DecValTok{505} \DecValTok{404} \DecValTok{359} \DecValTok{310} \DecValTok{337}
\DecValTok{1959} \DecValTok{360} \DecValTok{342} \DecValTok{406} \DecValTok{396} \DecValTok{420} \DecValTok{472} \DecValTok{548} \DecValTok{559} \DecValTok{463} \DecValTok{407} \DecValTok{362} \DecValTok{405}
\DecValTok{1960} \DecValTok{417} \DecValTok{391} \DecValTok{419} \DecValTok{461} \DecValTok{472} \DecValTok{535} \DecValTok{622} \DecValTok{606} \DecValTok{508} \DecValTok{461} \DecValTok{390} \DecValTok{432}
\end{Highlighting}
\end{Shaded}

Suppose we want to pick between two forecasting models with different \texttt{arima}
configurations. We can do that by evaluating their forecasting performance.
First, we set up the appropriate cross-validation scheme for time-series.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{folds <-}\StringTok{ }\KeywordTok{make_folds}\NormalTok{(AirPassengers,}
  \DataTypeTok{fold_fun =}\NormalTok{ folds_rolling_origin,}
  \DataTypeTok{first_window =} \DecValTok{36}\NormalTok{, }\DataTypeTok{validation_size =} \DecValTok{24}\NormalTok{, }\DataTypeTok{batch =} \DecValTok{10}
\NormalTok{)}

\CommentTok{# How many folds where generated?}
\KeywordTok{length}\NormalTok{(folds)}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\DecValTok{9}

\CommentTok{# Examine the first 2 folds.}
\NormalTok{folds[[}\DecValTok{1}\NormalTok{]]}
\OperatorTok{$}\NormalTok{v}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\DecValTok{1}

\OperatorTok{$}\NormalTok{training_set}
\NormalTok{ [}\DecValTok{1}\NormalTok{]  }\DecValTok{1}  \DecValTok{2}  \DecValTok{3}  \DecValTok{4}  \DecValTok{5}  \DecValTok{6}  \DecValTok{7}  \DecValTok{8}  \DecValTok{9} \DecValTok{10} \DecValTok{11} \DecValTok{12} \DecValTok{13} \DecValTok{14} \DecValTok{15} \DecValTok{16} \DecValTok{17} \DecValTok{18} \DecValTok{19} \DecValTok{20} \DecValTok{21} \DecValTok{22} \DecValTok{23} \DecValTok{24} \DecValTok{25}
\NormalTok{[}\DecValTok{26}\NormalTok{] }\DecValTok{26} \DecValTok{27} \DecValTok{28} \DecValTok{29} \DecValTok{30} \DecValTok{31} \DecValTok{32} \DecValTok{33} \DecValTok{34} \DecValTok{35} \DecValTok{36}

\OperatorTok{$}\NormalTok{validation_set}
\NormalTok{ [}\DecValTok{1}\NormalTok{] }\DecValTok{37} \DecValTok{38} \DecValTok{39} \DecValTok{40} \DecValTok{41} \DecValTok{42} \DecValTok{43} \DecValTok{44} \DecValTok{45} \DecValTok{46} \DecValTok{47} \DecValTok{48} \DecValTok{49} \DecValTok{50} \DecValTok{51} \DecValTok{52} \DecValTok{53} \DecValTok{54} \DecValTok{55} \DecValTok{56} \DecValTok{57} \DecValTok{58} \DecValTok{59} \DecValTok{60}

\KeywordTok{attr}\NormalTok{(,}\StringTok{"class"}\NormalTok{)}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\StringTok{"fold"}
\NormalTok{folds[[}\DecValTok{2}\NormalTok{]]}
\OperatorTok{$}\NormalTok{v}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\DecValTok{2}

\OperatorTok{$}\NormalTok{training_set}
\NormalTok{ [}\DecValTok{1}\NormalTok{]  }\DecValTok{1}  \DecValTok{2}  \DecValTok{3}  \DecValTok{4}  \DecValTok{5}  \DecValTok{6}  \DecValTok{7}  \DecValTok{8}  \DecValTok{9} \DecValTok{10} \DecValTok{11} \DecValTok{12} \DecValTok{13} \DecValTok{14} \DecValTok{15} \DecValTok{16} \DecValTok{17} \DecValTok{18} \DecValTok{19} \DecValTok{20} \DecValTok{21} \DecValTok{22} \DecValTok{23} \DecValTok{24} \DecValTok{25}
\NormalTok{[}\DecValTok{26}\NormalTok{] }\DecValTok{26} \DecValTok{27} \DecValTok{28} \DecValTok{29} \DecValTok{30} \DecValTok{31} \DecValTok{32} \DecValTok{33} \DecValTok{34} \DecValTok{35} \DecValTok{36} \DecValTok{37} \DecValTok{38} \DecValTok{39} \DecValTok{40} \DecValTok{41} \DecValTok{42} \DecValTok{43} \DecValTok{44} \DecValTok{45} \DecValTok{46}

\OperatorTok{$}\NormalTok{validation_set}
\NormalTok{ [}\DecValTok{1}\NormalTok{] }\DecValTok{47} \DecValTok{48} \DecValTok{49} \DecValTok{50} \DecValTok{51} \DecValTok{52} \DecValTok{53} \DecValTok{54} \DecValTok{55} \DecValTok{56} \DecValTok{57} \DecValTok{58} \DecValTok{59} \DecValTok{60} \DecValTok{61} \DecValTok{62} \DecValTok{63} \DecValTok{64} \DecValTok{65} \DecValTok{66} \DecValTok{67} \DecValTok{68} \DecValTok{69} \DecValTok{70}

\KeywordTok{attr}\NormalTok{(,}\StringTok{"class"}\NormalTok{)}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\StringTok{"fold"}
\end{Highlighting}
\end{Shaded}

By default, \texttt{folds\_rolling\_origin} will increase the size of the training set by
one time point each fold. Had we followed the default option, we would have 85
folds to train! Luckily, we can pass the \texttt{batch} as option to
\texttt{folds\_rolling\_origin} that tells it to increase the size of the training set by
10 points each iteration. Since we want to forecast the immediate next point,
\texttt{gap} argument remains the default (0).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# make sure to load the package!}
\KeywordTok{library}\NormalTok{(forecast)}

\CommentTok{# function to calculate cross-validated squared error}
\NormalTok{cv_forecasts <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(fold, data) \{}
  \CommentTok{# Get training and validation data}
\NormalTok{  train_data <-}\StringTok{ }\KeywordTok{training}\NormalTok{(data)}
\NormalTok{  valid_data <-}\StringTok{ }\KeywordTok{validation}\NormalTok{(data)}
\NormalTok{  valid_size <-}\StringTok{ }\KeywordTok{length}\NormalTok{(valid_data)}

\NormalTok{  train_ts <-}\StringTok{ }\KeywordTok{ts}\NormalTok{(}\KeywordTok{log10}\NormalTok{(train_data), }\DataTypeTok{frequency =} \DecValTok{12}\NormalTok{)}

  \CommentTok{# First arima model}
\NormalTok{  arima_fit <-}\StringTok{ }\KeywordTok{arima}\NormalTok{(train_ts, }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{),}
    \DataTypeTok{seasonal =} \KeywordTok{list}\NormalTok{(}
      \DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{),}
      \DataTypeTok{period =} \DecValTok{12}
\NormalTok{    )}
\NormalTok{  )}
\NormalTok{  raw_arima_pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(arima_fit, }\DataTypeTok{n.ahead =}\NormalTok{ valid_size)}
\NormalTok{  arima_pred <-}\StringTok{ }\DecValTok{10}\OperatorTok{^}\NormalTok{raw_arima_pred}\OperatorTok{$}\NormalTok{pred}
\NormalTok{  arima_MSE <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((arima_pred }\OperatorTok{-}\StringTok{ }\NormalTok{valid_data)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}

  \CommentTok{# Second arima model}
\NormalTok{  arima_fit2 <-}\StringTok{ }\KeywordTok{arima}\NormalTok{(train_ts, }\KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{),}
    \DataTypeTok{seasonal =} \KeywordTok{list}\NormalTok{(}
      \DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{),}
      \DataTypeTok{period =} \DecValTok{12}
\NormalTok{    )}
\NormalTok{  )}
\NormalTok{  raw_arima_pred2 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(arima_fit2, }\DataTypeTok{n.ahead =}\NormalTok{ valid_size)}
\NormalTok{  arima_pred2 <-}\StringTok{ }\DecValTok{10}\OperatorTok{^}\NormalTok{raw_arima_pred2}\OperatorTok{$}\NormalTok{pred}
\NormalTok{  arima_MSE2 <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((arima_pred2 }\OperatorTok{-}\StringTok{ }\NormalTok{valid_data)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}

\NormalTok{  out <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{mse =} \KeywordTok{data.frame}\NormalTok{(}
    \DataTypeTok{fold =} \KeywordTok{fold_index}\NormalTok{(),}
    \DataTypeTok{arima =}\NormalTok{ arima_MSE, }\DataTypeTok{arima2 =}\NormalTok{ arima_MSE2}
\NormalTok{  ))}
  \KeywordTok{return}\NormalTok{(out)}
\NormalTok{\}}

\NormalTok{mses <-}\StringTok{ }\KeywordTok{cross_validate}\NormalTok{(}
  \DataTypeTok{cv_fun =}\NormalTok{ cv_forecasts, }\DataTypeTok{folds =}\NormalTok{ folds, }\DataTypeTok{data =}\NormalTok{ AirPassengers,}
  \DataTypeTok{use_future =} \OtherTok{FALSE}
\NormalTok{)}
\NormalTok{mses}\OperatorTok{$}\NormalTok{mse}
\NormalTok{  fold   arima  arima2}
\DecValTok{1}    \DecValTok{1}   \FloatTok{68.21}  \FloatTok{137.28}
\DecValTok{2}    \DecValTok{2}  \FloatTok{319.68}  \FloatTok{313.15}
\DecValTok{3}    \DecValTok{3}  \FloatTok{578.35}  \FloatTok{713.36}
\DecValTok{4}    \DecValTok{4}  \FloatTok{428.69}  \FloatTok{505.31}
\DecValTok{5}    \DecValTok{5}  \FloatTok{407.33}  \FloatTok{371.27}
\DecValTok{6}    \DecValTok{6}  \FloatTok{281.82}  \FloatTok{250.99}
\DecValTok{7}    \DecValTok{7}  \FloatTok{827.56}  \FloatTok{910.12}
\DecValTok{8}    \DecValTok{8} \FloatTok{2099.59} \FloatTok{2213.15}
\DecValTok{9}    \DecValTok{9}  \FloatTok{398.37}  \FloatTok{293.38}
\KeywordTok{colMeans}\NormalTok{(mses}\OperatorTok{$}\NormalTok{mse[, }\KeywordTok{c}\NormalTok{(}\StringTok{"arima"}\NormalTok{, }\StringTok{"arima2"}\NormalTok{)])}
\NormalTok{ arima arima2 }
\FloatTok{601.07} \FloatTok{634.22} 
\end{Highlighting}
\end{Shaded}

The arima model with no AR component seems to be a better fit for this data.

\hypertarget{exercises}{%
\subsection{Exercises}\label{exercises}}

\hypertarget{review-of-key-concepts}{%
\subsubsection{Review of Key Concepts}\label{review-of-key-concepts}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Compare and contrast V-fold cross-validation with resubstitution
  cross-validation. What are some of the differences between the two methods?
  How are they similar? Describe a scenario when you would use one over the
  other.
\item
  What are the advantages and disadvantages of \(v\)-fold CV relative to:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    holdout CV?
  \item
    leave-one-out CV?
  \end{enumerate}
\item
  Why can't we use V-fold cross-validation for time-series data?
\item
  Would you use rolling window or origin for non-stationary time-series? Why?
\end{enumerate}

\hypertarget{the-ideas-in-action}{%
\subsubsection{The Ideas in Action}\label{the-ideas-in-action}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Let \(Y\) be a binary variable with \(P(Y=1 \mid W) = 0.01\). What kind of
  cross-validation should be use for a rare outcome? How can we do this with
  the \texttt{origami} package?
\item
  Consider the WASH benefits dataset presented in this chapter. How can we
  include cluster information into cross-validation? How can we do this with
  the \texttt{origami} package?
\end{enumerate}

\hypertarget{advanced-topics}{%
\subsubsection{Advanced Topics}\label{advanced-topics}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Think about a dataset with arbitrary spatial dependence, where we know
  the extent of dependence, and groups formed by such dependence are clear
  with no spillover effects. What kind of cross-validation can we use?
\item
  Continuing on the last problem, what kind of procedure, and cross-validation
  method, can we use if the spatial dependence is not clearly defined as in the
  previous problem?
\item
  Consider a classification problem with a large number of predictors. A
  statistician proposes the following analysis:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    First screen the predictors, leaving only covariates with a strong
    correlation with the class labels.
  \item
    Fit some algorithm using only the subset of highly correlated covariates.
  \item
    Use cross-validation to estimate the tuning parameters and the performance
    of the proposed algorithm.
  \end{enumerate}

  Is this a correct application of cross-validation? Why?
\end{enumerate}

\hypertarget{tmle3}{%
\section{The TMLE Framework}\label{tmle3}}

\emph{Jeremy Coyle}

Based on the \href{https://github.com/tlverse/tmle3}{\texttt{tmle3} \texttt{R} package}.

\hypertarget{learn-tmle}{%
\subsection{Learning Objectives}\label{learn-tmle}}

By the end of this chapter, you will be able to

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Understand why we use TMLE for effect estimation.
\item
  Use \texttt{tmle3} to estimate an Average Treatment Effect (ATE).
\item
  Understand how to use \texttt{tmle3} ``Specs'' objects.
\item
  Fit \texttt{tmle3} for a custom set of target parameters.
\item
  Use the delta method to estimate transformations of target parameters.
\end{enumerate}

\hypertarget{tmle-intro}{%
\subsection{Introduction}\label{tmle-intro}}

In the previous chapter on \texttt{sl3} we learned how to estimate a regression
function like \(\mathbb{E}[Y \mid X]\) from data. That's an important first step
in learning from data, but how can we use this predictive model to estimate
statistical and causal effects?

Going back to \protect\hyperlink{intro}{the roadmap for targeted learning}, suppose we'd like to
estimate the effect of a treatment variable \(A\) on an outcome \(Y\). As discussed,
one potential parameter that characterizes that effect is the Average Treatment
Effect (ATE), defined as \(\psi_0 = \mathbb{E}_W[\mathbb{E}[Y \mid A=1,W] - \mathbb{E}[Y \mid A=0,W]]\) and interpreted as the difference in mean outcome
under when treatment \(A=1\) and \(A=0\), averaging over the distribution of
covariates \(W\). We'll illustrate several potential estimators for this
parameter, and motivate the use of the TMLE (targeted maximum likelihood
estimation; targeted minimum loss-based estimation) framework, using the
following example data:

\begin{center}\includegraphics[width=0.8\linewidth]{img/misc/tmle_sim/schematic_1_truedgd} \end{center}

The small ticks on the right indicate the mean outcomes (averaging over \(W\))
under \(A=1\) and \(A=0\) respectively, so their difference is the quantity we'd
like to estimate.

While we hope to motivate the application of TMLE in this chapter, we refer the
interested reader to the two Targeted Learning books and associated works for
full technical details.

\hypertarget{substitution-est}{%
\subsection{Substitution Estimators}\label{substitution-est}}

We can use \texttt{sl3} to fit a Super Learner or other regression model to estimate
the outcome regression function \(\mathbb{E}_0[Y \mid A,W]\), which we often refer
to as \(\overline{Q}_0(A,W)\) and whose estimate we denote \(\overline{Q}_n(A,W)\).
To construct an estimate of the ATE \(\psi_n\), we need only ``plug-in'' the
estimates of \(\overline{Q}_n(A,W)\), evaluated at the two intervention contrasts,
to the corresponding ATE ``plug-in'' formula:
\(\psi_n = \frac{1}{n}\sum(\overline{Q}_n(1,W)-\overline{Q}_n(0,W))\). This kind
of estimator is called a \emph{plug-in} or \emph{substitution} estimator, since accurate
estimates \(\psi_n\) of the parameter \(\psi_0\) may be obtained by substituting
estimates \(\overline{Q}_n(A,W)\) for the relevant regression functions
\(\overline{Q}_0(A,W)\) themselves.

Applying \texttt{sl3} to estimate the outcome regression in our example, we can see
that the ensemble machine learning predictions fit the data quite well:

\begin{center}\includegraphics[width=0.8\linewidth]{img/misc/tmle_sim/schematic_2b_sllik} \end{center}

The solid lines indicate the \texttt{sl3} estimate of the regression function, with the
dotted lines indicating the \texttt{tmle3} updates \protect\hyperlink{tmle-updates}{(described below)}.

While substitution estimators are intuitive, naively using this approach with a
Super Learner estimate of \(\overline{Q}_0(A,W)\) has several limitations. First,
Super Learner is selecting learner weights to minimize risk across the entire
regression function, instead of ``targeting'' the ATE parameter we hope to
estimate, leading to biased estimation. That is, \texttt{sl3} is trying to do well on
the full regression curve on the left, instead of focusing on the small ticks on
the right. What's more, the sampling distribution of this approach is not
asymptotically linear, and therefore inference is not possible.

We can see these limitations illustrated in the estimates generated for the
example data:

\begin{center}\includegraphics[width=0.8\linewidth]{img/misc/tmle_sim/schematic_3_effects} \end{center}

We see that Super Learner, estimates the true parameter value (indicated by the
dashed vertical line) more accurately than GLM. However, it is still less
accurate than TMLE, and valid inference is not possible. In contrast, TMLE
achieves a less biased estimator and valid inference.

\hypertarget{tmle}{%
\subsection{Targeted Maximum Likelihood Estimation}\label{tmle}}

TMLE takes an initial estimate \(\overline{Q}_n(A,W)\) as well as an estimate of
the propensity score \(g_n(A \mid W) = \mathbb{P}(A = 1 \mid W)\) and produces an
updated estimate \(\overline{Q}^{\star}_n(A,W)\) that is ``targeted'' to the
parameter of interest. TMLE keeps the benefits of substitution estimators (it is
one), but augments the original, potentially erratic estimates to \emph{correct for
bias} while also resulting in an \emph{asymptotically linear} (and thus normally
distributed) estimator that accommodates inference via asymptotically consistent
Wald-style confidence intervals.

\hypertarget{tmle-updates}{%
\subsubsection{TMLE Updates}\label{tmle-updates}}

There are different types of TMLEs (and, sometimes, multiple for the same set of
target parameters) -- below, we give an example of the algorithm for TML
estimation of the ATE. \(\overline{Q}^{\star}_n(A,W)\) is the TMLE-augmented
estimate \(f(\overline{Q}^{\star}_n(A,W)) = f(\overline{Q}_n(A,W)) + \epsilon \cdot H_n(A,W)\), where \(f(\cdot)\) is the appropriate link function (e.g.,
\(\text{logit}(x) = \log(x / (1 - x))\)), and an estimate \(\epsilon_n\) of the
coefficient \(\epsilon\) of the ``clever covariate'' \(H_n(A,W)\) is computed. The
form of the covariate \(H_n(A,W)\) differs across target parameters; in this case
of the ATE, it is \(H_n(A,W) = \frac{A}{g_n(A \mid W)} - \frac{1-A}{1-g_n(A, W)}\), with \(g_n(A,W) = \mathbb{P}(A=1 \mid W)\) being the estimated propensity
score, so the estimator depends both on the initial fit (by \texttt{sl3}) of the
outcome regression (\(\overline{Q}_n\)) and of the propensity score (\(g_n\)).

There are several robust augmentations that are used across the \texttt{tlverse},
including the use of an additional layer of cross-validation to avoid
over-fitting bias (i.e., CV-TMLE) as well as approaches for more consistently
estimating several parameters simultaneously (e.g., the points on a survival
curve).

\hypertarget{tmle-infer}{%
\subsubsection{Statistical Inference}\label{tmle-infer}}

Since TMLE yields an \textbf{asymptotically linear} estimator, obtaining statistical
inference is very convenient. Each TML estimator has a corresponding
\textbf{(efficient) influence function} (often, ``EIF'', for short) that describes the
asymptotic distribution of the estimator. By using the estimated EIF, Wald-style
inference (asymptotically correct confidence intervals) can be constructed
simply by plugging into the form of the EIF our initial estimates
\(\overline{Q}_n\) and \(g_n\), then computing the sample standard error.

The following sections describe both a simple and more detailed way of
specifying and estimating a TMLE in the \texttt{tlverse}. In designing \texttt{tmle3}, we
sought to replicate as closely as possible the very general estimation framework
of TMLE, and so each theoretical object relevant to TMLE is encoded in a
corresponding software object/method. First, we will present the simple
application of \texttt{tmle3} to the WASH Benefits example, and then go on to describe
the underlying objects in greater detail.

\hypertarget{easy-bake-example-tmle3-for-ate}{%
\subsection{\texorpdfstring{Easy-Bake Example: \texttt{tmle3} for ATE}{Easy-Bake Example: tmle3 for ATE}}\label{easy-bake-example-tmle3-for-ate}}

We'll illustrate the most basic use of TMLE using the WASH Benefits data
introduced earlier and estimating an average treatment effect.

\hypertarget{load-the-data}{%
\subsubsection{Load the Data}\label{load-the-data}}

We'll use the same WASH Benefits data as the earlier chapters:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(data.table)}
\KeywordTok{library}\NormalTok{(dplyr)}
\KeywordTok{library}\NormalTok{(tmle3)}
\KeywordTok{library}\NormalTok{(sl3)}
\NormalTok{washb_data <-}\StringTok{ }\KeywordTok{fread}\NormalTok{(}
  \KeywordTok{paste0}\NormalTok{(}
    \StringTok{"https://raw.githubusercontent.com/tlverse/tlverse-data/master/"}\NormalTok{,}
    \StringTok{"wash-benefits/washb_data.csv"}
\NormalTok{  ),}
  \DataTypeTok{stringsAsFactors =} \OtherTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{define-the-variable-roles}{%
\subsubsection{Define the variable roles}\label{define-the-variable-roles}}

We'll use the common \(W\) (covariates), \(A\) (treatment/intervention), \(Y\)
(outcome) data structure. \texttt{tmle3} needs to know what variables in the dataset
correspond to each of these roles. We use a list of character vectors to tell
it. We call this a ``Node List'' as it corresponds to the nodes in a Directed
Acyclic Graph (DAG), a way of displaying causal relationships between variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{node_list <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
  \DataTypeTok{W =} \KeywordTok{c}\NormalTok{(}
    \StringTok{"month"}\NormalTok{, }\StringTok{"aged"}\NormalTok{, }\StringTok{"sex"}\NormalTok{, }\StringTok{"momage"}\NormalTok{, }\StringTok{"momedu"}\NormalTok{,}
    \StringTok{"momheight"}\NormalTok{, }\StringTok{"hfiacat"}\NormalTok{, }\StringTok{"Nlt18"}\NormalTok{, }\StringTok{"Ncomp"}\NormalTok{, }\StringTok{"watmin"}\NormalTok{,}
    \StringTok{"elec"}\NormalTok{, }\StringTok{"floor"}\NormalTok{, }\StringTok{"walls"}\NormalTok{, }\StringTok{"roof"}\NormalTok{, }\StringTok{"asset_wardrobe"}\NormalTok{,}
    \StringTok{"asset_table"}\NormalTok{, }\StringTok{"asset_chair"}\NormalTok{, }\StringTok{"asset_khat"}\NormalTok{,}
    \StringTok{"asset_chouki"}\NormalTok{, }\StringTok{"asset_tv"}\NormalTok{, }\StringTok{"asset_refrig"}\NormalTok{,}
    \StringTok{"asset_bike"}\NormalTok{, }\StringTok{"asset_moto"}\NormalTok{, }\StringTok{"asset_sewmach"}\NormalTok{,}
    \StringTok{"asset_mobile"}
\NormalTok{  ),}
  \DataTypeTok{A =} \StringTok{"tr"}\NormalTok{,}
  \DataTypeTok{Y =} \StringTok{"whz"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{handle-missingness}{%
\subsubsection{Handle Missingness}\label{handle-missingness}}

Currently, missingness in \texttt{tmle3} is handled in a fairly simple way:

\begin{itemize}
\tightlist
\item
  Missing covariates are median- (for continuous) or mode- (for discrete)
  imputed, and additional covariates indicating imputation are generated, just
  as described in \protect\hyperlink{sl3}{the \texttt{sl3} chapter}.
\item
  Missing treatment variables are excluded -- such observations are dropped.
\item
  Missing outcomes are efficiently handled by the automatic calculation (and
  incorporation into estimators) of \emph{inverse probability of censoring weights}
  (IPCW); this is also known as IPCW-TMLE and may be thought of as a joint
  intervention to remove missingness and is analogous to the procedure used with
  classical inverse probability weighted estimators.
\end{itemize}

These steps are implemented in the \texttt{process\_missing} function in \texttt{tmle3}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{processed <-}\StringTok{ }\KeywordTok{process_missing}\NormalTok{(washb_data, node_list)}
\NormalTok{washb_data <-}\StringTok{ }\NormalTok{processed}\OperatorTok{$}\NormalTok{data}
\NormalTok{node_list <-}\StringTok{ }\NormalTok{processed}\OperatorTok{$}\NormalTok{node_list}
\end{Highlighting}
\end{Shaded}

\hypertarget{create-a-spec-object}{%
\subsubsection{Create a ``Spec'' Object}\label{create-a-spec-object}}

\texttt{tmle3} is general, and allows most components of the TMLE procedure to be
specified in a modular way. However, most users will not be interested in
manually specifying all of these components. Therefore, \texttt{tmle3} implements a
\texttt{tmle3\_Spec} object that bundles a set of components into a \emph{specification}
(``Spec'') that, with minimal additional detail, can be run to fit a TMLE.

We'll start with using one of the specs, and then work our way down into the
internals of \texttt{tmle3}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ate_spec <-}\StringTok{ }\KeywordTok{tmle_ATE}\NormalTok{(}
  \DataTypeTok{treatment_level =} \StringTok{"Nutrition + WSH"}\NormalTok{,}
  \DataTypeTok{control_level =} \StringTok{"Control"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{define-the-learners}{%
\subsubsection{Define the learners}\label{define-the-learners}}

Currently, the only other thing a user must define are the \texttt{sl3} learners used
to estimate the relevant factors of the likelihood: Q and g.

This takes the form of a list of \texttt{sl3} learners, one for each likelihood factor
to be estimated with \texttt{sl3}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# choose base learners}
\NormalTok{lrnr_mean <-}\StringTok{ }\KeywordTok{make_learner}\NormalTok{(Lrnr_mean)}
\NormalTok{lrnr_rf <-}\StringTok{ }\KeywordTok{make_learner}\NormalTok{(Lrnr_ranger)}

\CommentTok{# define metalearners appropriate to data types}
\NormalTok{ls_metalearner <-}\StringTok{ }\KeywordTok{make_learner}\NormalTok{(Lrnr_nnls)}
\NormalTok{mn_metalearner <-}\StringTok{ }\KeywordTok{make_learner}\NormalTok{(}
\NormalTok{  Lrnr_solnp, metalearner_linear_multinomial,}
\NormalTok{  loss_loglik_multinomial}
\NormalTok{)}
\NormalTok{sl_Y <-}\StringTok{ }\NormalTok{Lrnr_sl}\OperatorTok{$}\KeywordTok{new}\NormalTok{(}
  \DataTypeTok{learners =} \KeywordTok{list}\NormalTok{(lrnr_mean, lrnr_rf),}
  \DataTypeTok{metalearner =}\NormalTok{ ls_metalearner}
\NormalTok{)}
\NormalTok{sl_A <-}\StringTok{ }\NormalTok{Lrnr_sl}\OperatorTok{$}\KeywordTok{new}\NormalTok{(}
  \DataTypeTok{learners =} \KeywordTok{list}\NormalTok{(lrnr_mean, lrnr_rf),}
  \DataTypeTok{metalearner =}\NormalTok{ mn_metalearner}
\NormalTok{)}
\NormalTok{learner_list <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{A =}\NormalTok{ sl_A, }\DataTypeTok{Y =}\NormalTok{ sl_Y)}
\end{Highlighting}
\end{Shaded}

Here, we use a Super Learner as defined in the previous chapter. In the future,
we plan to include reasonable defaults learners.

\hypertarget{fit-the-tmle}{%
\subsubsection{Fit the TMLE}\label{fit-the-tmle}}

We now have everything we need to fit the tmle using \texttt{tmle3}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tmle_fit <-}\StringTok{ }\KeywordTok{tmle3}\NormalTok{(ate_spec, washb_data, node_list, learner_list)}
\KeywordTok{print}\NormalTok{(tmle_fit)}
\NormalTok{A tmle3_Fit that took }\DecValTok{1} \KeywordTok{step}\NormalTok{(s)}
\NormalTok{   type                                    param   init_est tmle_est       se}
\DecValTok{1}\OperatorTok{:}\StringTok{  }\NormalTok{ATE ATE[Y_\{A=Nutrition }\OperatorTok{+}\StringTok{ }\NormalTok{WSH\}}\OperatorTok{-}\NormalTok{Y_\{A=Control\}] }\FloatTok{-0.0031611} \FloatTok{0.010044} \FloatTok{0.050853}
\NormalTok{       lower   upper psi_transformed lower_transformed upper_transformed}
\DecValTok{1}\OperatorTok{:}\StringTok{ }\FloatTok{-0.089626} \FloatTok{0.10971}        \FloatTok{0.010044}         \FloatTok{-0.089626}           \FloatTok{0.10971}
\end{Highlighting}
\end{Shaded}

\hypertarget{evaluate-the-estimates}{%
\subsubsection{Evaluate the Estimates}\label{evaluate-the-estimates}}

We can see the summary results by printing the fit object. Alternatively, we
can extra results from the summary by indexing into it:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{estimates <-}\StringTok{ }\NormalTok{tmle_fit}\OperatorTok{$}\NormalTok{summary}\OperatorTok{$}\NormalTok{psi_transformed}
\KeywordTok{print}\NormalTok{(estimates)}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\FloatTok{0.010044}
\end{Highlighting}
\end{Shaded}

\hypertarget{tmle3-components}{%
\subsection{\texorpdfstring{\texttt{tmle3} Components}{tmle3 Components}}\label{tmle3-components}}

Now that we've successfully used a spec to obtain a TML estimate, let's look
under the hood at the components. The spec has a number of functions that
generate the objects necessary to define and fit a TMLE.

\hypertarget{tmle3_task}{%
\subsubsection{\texorpdfstring{\texttt{tmle3\_task}}{tmle3\_task}}\label{tmle3_task}}

First is, a \texttt{tmle3\_Task}, analogous to an \texttt{sl3\_Task}, containing the data we're
fitting the TMLE to, as well as an NPSEM generated from the \texttt{node\_list}
defined above, describing the variables and their relationships.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tmle_task <-}\StringTok{ }\NormalTok{ate_spec}\OperatorTok{$}\KeywordTok{make_tmle_task}\NormalTok{(washb_data, node_list)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tmle_task}\OperatorTok{$}\NormalTok{npsem}
\OperatorTok{$}\NormalTok{W}
\NormalTok{tmle3_Node}\OperatorTok{:}\StringTok{ }\NormalTok{W}
\NormalTok{    Variables}\OperatorTok{:}\StringTok{ }\NormalTok{month, aged, sex, momedu, hfiacat, Nlt18, Ncomp, watmin, elec, floor, walls, roof, asset_wardrobe, asset_table, asset_chair, asset_khat, asset_chouki, asset_tv, asset_refrig, asset_bike, asset_moto, asset_sewmach, asset_mobile, momage, momheight, delta_momage, delta_momheight}
\NormalTok{    Parents}\OperatorTok{:}\StringTok{ }

\ErrorTok{$}\NormalTok{A}
\NormalTok{tmle3_Node}\OperatorTok{:}\StringTok{ }\NormalTok{A}
\NormalTok{    Variables}\OperatorTok{:}\StringTok{ }\NormalTok{tr}
\NormalTok{    Parents}\OperatorTok{:}\StringTok{ }\NormalTok{W}

\OperatorTok{$}\NormalTok{Y}
\NormalTok{tmle3_Node}\OperatorTok{:}\StringTok{ }\NormalTok{Y}
\NormalTok{    Variables}\OperatorTok{:}\StringTok{ }\NormalTok{whz}
\NormalTok{    Parents}\OperatorTok{:}\StringTok{ }\NormalTok{A, W}
\end{Highlighting}
\end{Shaded}

\hypertarget{initial-likelihood}{%
\subsubsection{Initial Likelihood}\label{initial-likelihood}}

Next, is an object representing the likelihood, factorized according to the
NPSEM described above:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{initial_likelihood <-}\StringTok{ }\NormalTok{ate_spec}\OperatorTok{$}\KeywordTok{make_initial_likelihood}\NormalTok{(}
\NormalTok{  tmle_task,}
\NormalTok{  learner_list}
\NormalTok{)}
\KeywordTok{print}\NormalTok{(initial_likelihood)}
\NormalTok{W}\OperatorTok{:}\StringTok{ }\NormalTok{Lf_emp}
\NormalTok{A}\OperatorTok{:}\StringTok{ }\NormalTok{LF_fit}
\NormalTok{Y}\OperatorTok{:}\StringTok{ }\NormalTok{LF_fit}
\end{Highlighting}
\end{Shaded}

These components of the likelihood indicate how the factors were estimated: the
marginal distribution of \(W\) was estimated using NP-MLE, and the conditional
distributions of \(A\) and \(Y\) were estimated using \texttt{sl3} fits (as defined with
the \texttt{learner\_list}) above.

We can use this in tandem with the \texttt{tmle\_task} object to obtain likelihood
estimates for each observation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{initial_likelihood}\OperatorTok{$}\KeywordTok{get_likelihoods}\NormalTok{(tmle_task)}
\NormalTok{               W       A        Y}
   \DecValTok{1}\OperatorTok{:}\StringTok{ }\FloatTok{0.00021299} \FloatTok{0.34702} \FloatTok{-0.32696}
   \DecValTok{2}\OperatorTok{:}\StringTok{ }\FloatTok{0.00021299} \FloatTok{0.37305} \FloatTok{-0.88218}
   \DecValTok{3}\OperatorTok{:}\StringTok{ }\FloatTok{0.00021299} \FloatTok{0.34685} \FloatTok{-0.79300}
   \DecValTok{4}\OperatorTok{:}\StringTok{ }\FloatTok{0.00021299} \FloatTok{0.33625} \FloatTok{-0.89157}
   \DecValTok{5}\OperatorTok{:}\StringTok{ }\FloatTok{0.00021299} \FloatTok{0.34098} \FloatTok{-0.63477}
  \OperatorTok{---}\StringTok{                            }
\DecValTok{4691}\OperatorTok{:}\StringTok{ }\FloatTok{0.00021299} \FloatTok{0.24334} \FloatTok{-0.61095}
\DecValTok{4692}\OperatorTok{:}\StringTok{ }\FloatTok{0.00021299} \FloatTok{0.24620} \FloatTok{-0.21534}
\DecValTok{4693}\OperatorTok{:}\StringTok{ }\FloatTok{0.00021299} \FloatTok{0.22401} \FloatTok{-0.79223}
\DecValTok{4694}\OperatorTok{:}\StringTok{ }\FloatTok{0.00021299} \FloatTok{0.27641} \FloatTok{-0.94319}
\DecValTok{4695}\OperatorTok{:}\StringTok{ }\FloatTok{0.00021299} \FloatTok{0.20158} \FloatTok{-1.08201}
\end{Highlighting}
\end{Shaded}

\hypertarget{targeted-likelihood-updater}{%
\subsubsection{Targeted Likelihood (updater)}\label{targeted-likelihood-updater}}

We also need to define a ``Targeted Likelihood'' object. This is a special type
of likelihood that is able to be updated using an \texttt{tmle3\_Update} object. This
object defines the update strategy (e.g., submodel, loss function, CV-TMLE or
not).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{targeted_likelihood <-}\StringTok{ }\NormalTok{Targeted_Likelihood}\OperatorTok{$}\KeywordTok{new}\NormalTok{(initial_likelihood)}
\end{Highlighting}
\end{Shaded}

When constructing the targeted likelihood, you can specify different update
options. See the documentation for \texttt{tmle3\_Update} for details of the different
options. For example, you can disable CV-TMLE (the default in \texttt{tmle3}) as
follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{targeted_likelihood_no_cv <-}
\StringTok{  }\NormalTok{Targeted_Likelihood}\OperatorTok{$}\KeywordTok{new}\NormalTok{(initial_likelihood,}
    \DataTypeTok{updater =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{cvtmle =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{parameter-mapping}{%
\subsubsection{Parameter Mapping}\label{parameter-mapping}}

Finally, we need to define the parameters of interest. Here, the spec defines a
single parameter, the ATE. In the next section, we'll see how to add additional
parameters.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tmle_params <-}\StringTok{ }\NormalTok{ate_spec}\OperatorTok{$}\KeywordTok{make_params}\NormalTok{(tmle_task, targeted_likelihood)}
\KeywordTok{print}\NormalTok{(tmle_params)}
\NormalTok{[[}\DecValTok{1}\NormalTok{]]}
\NormalTok{Param_ATE}\OperatorTok{:}\StringTok{ }\NormalTok{ATE[Y_\{A=Nutrition }\OperatorTok{+}\StringTok{ }\NormalTok{WSH\}}\OperatorTok{-}\NormalTok{Y_\{A=Control\}]}
\end{Highlighting}
\end{Shaded}

\hypertarget{putting-it-all-together}{%
\subsubsection{Putting it all together}\label{putting-it-all-together}}

Having used the spec to manually generate all these components, we can now
manually fit a \texttt{tmle3}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tmle_fit_manual <-}\StringTok{ }\KeywordTok{fit_tmle3}\NormalTok{(}
\NormalTok{  tmle_task, targeted_likelihood, tmle_params,}
\NormalTok{  targeted_likelihood}\OperatorTok{$}\NormalTok{updater}
\NormalTok{)}
\KeywordTok{print}\NormalTok{(tmle_fit_manual)}
\NormalTok{A tmle3_Fit that took }\DecValTok{1} \KeywordTok{step}\NormalTok{(s)}
\NormalTok{   type                                    param   init_est tmle_est       se}
\DecValTok{1}\OperatorTok{:}\StringTok{  }\NormalTok{ATE ATE[Y_\{A=Nutrition }\OperatorTok{+}\StringTok{ }\NormalTok{WSH\}}\OperatorTok{-}\NormalTok{Y_\{A=Control\}] }\FloatTok{-0.0062324} \FloatTok{0.017515} \FloatTok{0.050591}
\NormalTok{       lower   upper psi_transformed lower_transformed upper_transformed}
\DecValTok{1}\OperatorTok{:}\StringTok{ }\FloatTok{-0.081641} \FloatTok{0.11667}        \FloatTok{0.017515}         \FloatTok{-0.081641}           \FloatTok{0.11667}
\end{Highlighting}
\end{Shaded}

The result is equivalent to fitting using the \texttt{tmle3} function as above.

\hypertarget{fitting-tmle3-with-multiple-parameters}{%
\subsection{\texorpdfstring{Fitting \texttt{tmle3} with multiple parameters}{Fitting tmle3 with multiple parameters}}\label{fitting-tmle3-with-multiple-parameters}}

Above, we fit a \texttt{tmle3} with just one parameter. \texttt{tmle3} also supports fitting
multiple parameters simultaneously. To illustrate this, we'll use the
\texttt{tmle\_TSM\_all} spec:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tsm_spec <-}\StringTok{ }\KeywordTok{tmle_TSM_all}\NormalTok{()}
\NormalTok{targeted_likelihood <-}\StringTok{ }\NormalTok{Targeted_Likelihood}\OperatorTok{$}\KeywordTok{new}\NormalTok{(initial_likelihood)}
\NormalTok{all_tsm_params <-}\StringTok{ }\NormalTok{tsm_spec}\OperatorTok{$}\KeywordTok{make_params}\NormalTok{(tmle_task, targeted_likelihood)}
\KeywordTok{print}\NormalTok{(all_tsm_params)}
\NormalTok{[[}\DecValTok{1}\NormalTok{]]}
\NormalTok{Param_TSM}\OperatorTok{:}\StringTok{ }\NormalTok{E[Y_\{A=Control\}]}

\NormalTok{[[}\DecValTok{2}\NormalTok{]]}
\NormalTok{Param_TSM}\OperatorTok{:}\StringTok{ }\NormalTok{E[Y_\{A=Handwashing\}]}

\NormalTok{[[}\DecValTok{3}\NormalTok{]]}
\NormalTok{Param_TSM}\OperatorTok{:}\StringTok{ }\NormalTok{E[Y_\{A=Nutrition\}]}

\NormalTok{[[}\DecValTok{4}\NormalTok{]]}
\NormalTok{Param_TSM}\OperatorTok{:}\StringTok{ }\NormalTok{E[Y_\{A=Nutrition }\OperatorTok{+}\StringTok{ }\NormalTok{WSH\}]}

\NormalTok{[[}\DecValTok{5}\NormalTok{]]}
\NormalTok{Param_TSM}\OperatorTok{:}\StringTok{ }\NormalTok{E[Y_\{A=Sanitation\}]}

\NormalTok{[[}\DecValTok{6}\NormalTok{]]}
\NormalTok{Param_TSM}\OperatorTok{:}\StringTok{ }\NormalTok{E[Y_\{A=WSH\}]}

\NormalTok{[[}\DecValTok{7}\NormalTok{]]}
\NormalTok{Param_TSM}\OperatorTok{:}\StringTok{ }\NormalTok{E[Y_\{A=Water\}]}
\end{Highlighting}
\end{Shaded}

This spec generates a Treatment Specific Mean (TSM) for each level of the
exposure variable. Note that we must first generate a new targeted likelihood,
as the old one was targeted to the ATE. However, we can recycle the initial
likelihood we fit above, saving us a super learner step.

\hypertarget{delta-method}{%
\subsubsection{Delta Method}\label{delta-method}}

We can also define parameters based on Delta Method Transformations of other
parameters. For instance, we can estimate a ATE using the delta method and two
of the above TSM parameters:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ate_param <-}\StringTok{ }\KeywordTok{define_param}\NormalTok{(}
\NormalTok{  Param_delta, targeted_likelihood,}
\NormalTok{  delta_param_ATE,}
  \KeywordTok{list}\NormalTok{(all_tsm_params[[}\DecValTok{1}\NormalTok{]], all_tsm_params[[}\DecValTok{4}\NormalTok{]])}
\NormalTok{)}
\KeywordTok{print}\NormalTok{(ate_param)}
\NormalTok{Param_delta}\OperatorTok{:}\StringTok{ }\NormalTok{E[Y_\{A=Nutrition }\OperatorTok{+}\StringTok{ }\NormalTok{WSH\}] }\OperatorTok{-}\StringTok{ }\NormalTok{E[Y_\{A=Control\}]}
\end{Highlighting}
\end{Shaded}

This can similarly be used to estimate other derived parameters like Relative
Risks, and Population Attributable Risks

\hypertarget{fit}{%
\subsubsection{Fit}\label{fit}}

We can now fit a TMLE simultaneously for all TSM parameters, as well as the
above defined ATE parameter

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all_params <-}\StringTok{ }\KeywordTok{c}\NormalTok{(all_tsm_params, ate_param)}

\NormalTok{tmle_fit_multiparam <-}\StringTok{ }\KeywordTok{fit_tmle3}\NormalTok{(}
\NormalTok{  tmle_task, targeted_likelihood, all_params,}
\NormalTok{  targeted_likelihood}\OperatorTok{$}\NormalTok{updater}
\NormalTok{)}

\KeywordTok{print}\NormalTok{(tmle_fit_multiparam)}
\NormalTok{A tmle3_Fit that took }\DecValTok{1} \KeywordTok{step}\NormalTok{(s)}
\NormalTok{   type                                       param   init_est tmle_est}
\DecValTok{1}\OperatorTok{:}\StringTok{  }\NormalTok{TSM                            E[Y_\{A=Control\}] }\FloatTok{-0.5953314} \FloatTok{-0.61981}
\DecValTok{2}\OperatorTok{:}\StringTok{  }\NormalTok{TSM                        E[Y_\{A=Handwashing\}] }\FloatTok{-0.6179897} \FloatTok{-0.66114}
\DecValTok{3}\OperatorTok{:}\StringTok{  }\NormalTok{TSM                          E[Y_\{A=Nutrition\}] }\FloatTok{-0.6119870} \FloatTok{-0.60338}
\DecValTok{4}\OperatorTok{:}\StringTok{  }\NormalTok{TSM                    E[Y_\{A=Nutrition }\OperatorTok{+}\StringTok{ }\NormalTok{WSH\}] }\FloatTok{-0.6015639} \FloatTok{-0.60250}
\DecValTok{5}\OperatorTok{:}\StringTok{  }\NormalTok{TSM                         E[Y_\{A=Sanitation\}] }\FloatTok{-0.5866311} \FloatTok{-0.58147}
\DecValTok{6}\OperatorTok{:}\StringTok{  }\NormalTok{TSM                                E[Y_\{A=WSH\}] }\FloatTok{-0.5213051} \FloatTok{-0.45027}
\DecValTok{7}\OperatorTok{:}\StringTok{  }\NormalTok{TSM                              E[Y_\{A=Water\}] }\FloatTok{-0.5653576} \FloatTok{-0.53554}
\DecValTok{8}\OperatorTok{:}\StringTok{  }\NormalTok{ATE E[Y_\{A=Nutrition }\OperatorTok{+}\StringTok{ }\NormalTok{WSH\}] }\OperatorTok{-}\StringTok{ }\NormalTok{E[Y_\{A=Control\}] }\FloatTok{-0.0062324}  \FloatTok{0.01731}
\NormalTok{         se     lower    upper psi_transformed lower_transformed}
\DecValTok{1}\OperatorTok{:}\StringTok{ }\FloatTok{0.030069} \FloatTok{-0.678746} \FloatTok{-0.56088}        \FloatTok{-0.61981}         \FloatTok{-0.678746}
\DecValTok{2}\OperatorTok{:}\StringTok{ }\FloatTok{0.041821} \FloatTok{-0.743111} \FloatTok{-0.57917}        \FloatTok{-0.66114}         \FloatTok{-0.743111}
\DecValTok{3}\OperatorTok{:}\StringTok{ }\FloatTok{0.041553} \FloatTok{-0.684825} \FloatTok{-0.52194}        \FloatTok{-0.60338}         \FloatTok{-0.684825}
\DecValTok{4}\OperatorTok{:}\StringTok{ }\FloatTok{0.040925} \FloatTok{-0.682712} \FloatTok{-0.52229}        \FloatTok{-0.60250}         \FloatTok{-0.682712}
\DecValTok{5}\OperatorTok{:}\StringTok{ }\FloatTok{0.042313} \FloatTok{-0.664402} \FloatTok{-0.49854}        \FloatTok{-0.58147}         \FloatTok{-0.664402}
\DecValTok{6}\OperatorTok{:}\StringTok{ }\FloatTok{0.045216} \FloatTok{-0.538891} \FloatTok{-0.36165}        \FloatTok{-0.45027}         \FloatTok{-0.538891}
\DecValTok{7}\OperatorTok{:}\StringTok{ }\FloatTok{0.039290} \FloatTok{-0.612551} \FloatTok{-0.45854}        \FloatTok{-0.53554}         \FloatTok{-0.612551}
\DecValTok{8}\OperatorTok{:}\StringTok{ }\FloatTok{0.050596} \FloatTok{-0.081857}  \FloatTok{0.11648}         \FloatTok{0.01731}         \FloatTok{-0.081857}
\NormalTok{   upper_transformed}
\DecValTok{1}\OperatorTok{:}\StringTok{          }\FloatTok{-0.56088}
\DecValTok{2}\OperatorTok{:}\StringTok{          }\FloatTok{-0.57917}
\DecValTok{3}\OperatorTok{:}\StringTok{          }\FloatTok{-0.52194}
\DecValTok{4}\OperatorTok{:}\StringTok{          }\FloatTok{-0.52229}
\DecValTok{5}\OperatorTok{:}\StringTok{          }\FloatTok{-0.49854}
\DecValTok{6}\OperatorTok{:}\StringTok{          }\FloatTok{-0.36165}
\DecValTok{7}\OperatorTok{:}\StringTok{          }\FloatTok{-0.45854}
\DecValTok{8}\OperatorTok{:}\StringTok{           }\FloatTok{0.11648}
\end{Highlighting}
\end{Shaded}

\hypertarget{exercises-1}{%
\subsection{Exercises}\label{exercises-1}}

\hypertarget{tmle3-ex1}{%
\subsubsection{\texorpdfstring{Estimation of the ATE with \texttt{tmle3}}{Estimation of the ATE with tmle3}}\label{tmle3-ex1}}

Follow the steps below to estimate an average treatment effect using data from
the Collaborative Perinatal Project (CPP), available in the \texttt{sl3} package. To
simplify this example, we define a binary intervention variable, \texttt{parity01} --
an indicator of having one or more children before the current child and a
binary outcome, \texttt{haz01} -- an indicator of having an above average height for
age.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the data set}
\KeywordTok{data}\NormalTok{(cpp)}
\NormalTok{cpp <-}\StringTok{ }\NormalTok{cpp }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{as_tibble}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(haz)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{parity01 =} \KeywordTok{as.numeric}\NormalTok{(parity }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{),}
    \DataTypeTok{haz01 =} \KeywordTok{as.numeric}\NormalTok{(haz }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Define the variable roles \((W,A,Y)\) by creating a list of these nodes.
  Include the following baseline covariates in \(W\): \texttt{apgar1}, \texttt{apgar5},
  \texttt{gagebrth}, \texttt{mage}, \texttt{meducyrs}, \texttt{sexn}. Both \(A\) and \(Y\) are specified
  above. The missingness in the data (specifically, the missingness in the
  columns that are specified in the node list) will need to be taking care of.
  The \texttt{process\_missing} function can be used to accomplish this, like the
  \texttt{washb\_data} example above.
\item
  Define a \texttt{tmle3\_Spec} object for the ATE, \texttt{tmle\_ATE()}.
\item
  Using the same base learning libraries defined above, specify \texttt{sl3} base
  learners for estimation of \(\overline{Q}_0 = \mathbb{E}_0(Y \mid A,Y)\) and
  \(g_0 = \mathbb{P}(A = 1 \mid W)\).
\item
  Define the metalearner like below.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{metalearner <-}\StringTok{ }\KeywordTok{make_learner}\NormalTok{(}
\NormalTok{  Lrnr_solnp,}
  \DataTypeTok{loss_function =}\NormalTok{ loss_loglik_binomial,}
  \DataTypeTok{learner_function =}\NormalTok{ metalearner_logistic_binomial}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Define one super learner for estimating \(\overline{Q}_0\) and another for
  estimating \(g_0\). Use the metalearner above for both super learners.
\item
  Create a list of the two super learners defined in the step above and call
  this object \texttt{learner\_list}. The list names should be \texttt{A} (defining the super
  learner for estimation of \(g_0\)) and \texttt{Y} (defining the super learner for
  estimation of \(\overline{Q}_0\)).
\item
  Fit the TMLE with the \texttt{tmle3} function by specifying (1) the \texttt{tmle3\_Spec},
  which we defined in Step 2; (2) the data; (3) the list of nodes, which we
  specified in Step 1; and (4) the list of super learners for estimation of
  \(g_0\) and \(\overline{Q}_0\), which we defined in Step 6. \emph{Note}: Like before,
  you will need to explicitly make a copy of the data (to work around
  \texttt{data.table} optimizations), e.g., (\texttt{cpp2\ \textless{}-\ data.table::copy(cpp)}), then
  use the \texttt{cpp2} data going forward.
\end{enumerate}

\hypertarget{tmle3-ex2}{%
\subsubsection{\texorpdfstring{Estimation of Strata-Specific ATEs with \texttt{tmle3}}{Estimation of Strata-Specific ATEs with tmle3}}\label{tmle3-ex2}}

For this exercise, we will work with a random sample of 5,000 patients who
participated in the International Stroke Trial (IST). This data is described in
the \protect\hyperlink{ist}{Chapter 3.2 of the \texttt{tlverse} handbook}. We included the data below
and a summarized description that is relevant for this exercise.

The outcome, \(Y\), indicates recurrent ischemic stroke within 14 days after
randomization (\texttt{DRSISC}); the treatment of interest, \(A\), is the randomized
aspirin vs.~no aspirin treatment allocation (\texttt{RXASP} in \texttt{ist}); and the
adjustment set, \(W\), consists simply of other variables measured at baseline. In
this data, the outcome is occasionally missing, but there is no need to create a
variable indicating this missingness (such as \(\Delta\)) for analyses in the
\texttt{tlverse}, since the missingness is automatically detected when \texttt{NA} are present
in the outcome. Covariates with missing values (\texttt{RATRIAL}, \texttt{RASP3} and \texttt{RHEP24})
have already been imputed. Additional covariates were created
(\texttt{MISSING\_RATRIAL\_RASP3} and \texttt{MISSING\_RHEP24}), which indicate whether or not
the covariate was imputed. The missingness was identical for \texttt{RATRIAL} and
\texttt{RASP3}, which is why only one covariate indicating imputation for these two
covariates was created.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Estimate the average effect of randomized asprin treatment (\texttt{RXASP} = 1) on
  recurrent ischemic stroke. Even though the missingness mechanism on \(Y\),
  \(\Delta\), does not need to be specified in the node list, it does still need
  to be accounted for in the TMLE. In other words, for this estimation problem,
  \(\Delta\) is a relevant factor of the likelihood. Thus, when defining the
  list of \texttt{sl3} learners for each likelihood factor, be sure to include a list
  of learners for estimation of \(\Delta\), say \texttt{sl\_Delta}, and specify this in
  the learner list, like so
  \texttt{learner\_list\ \textless{}-\ list(A\ =\ sl\_A,\ delta\_Y\ =\ sl\_Delta,\ Y\ =\ sl\_Y)}.
\item
  Recall that this RCT was conducted internationally. Suposse there is concern
  that the dose of asprin may have varied across geographical regions, and an
  average across all geographical regions may not be warranted. Calculate the
  strata specific ATEs according to geographical region (\texttt{REGION}).
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ist_data <-}\StringTok{ }\KeywordTok{fread}\NormalTok{(}
  \KeywordTok{paste0}\NormalTok{(}
    \StringTok{"https://raw.githubusercontent.com/tlverse/deming2019-workshop/"}\NormalTok{,}
    \StringTok{"master/data/ist_sample.csv"}
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{summary}{%
\subsection{Summary}\label{summary}}

\texttt{tmle3} is a general purpose framework for generating TML estimates. The easiest
way to use it is to use a predefined spec, allowing you to just fill in the
blanks for the data, variable roles, and \texttt{sl3} learners. However, digging under
the hood allows users to specify a wide range of TMLEs. In the next sections,
we'll see how this framework can be used to estimate advanced parameters such as
optimal treatments and stochastic shift interventions.

\hypertarget{causal-mediation-analysis}{%
\section{Causal Mediation Analysis}\label{causal-mediation-analysis}}

\emph{Nima Hejazi}

Based on the \href{https://github.com/tlverse/tmle3mediate}{\texttt{tmle3mediate} \texttt{R}
package} by \emph{Nima Hejazi, James
Duncan, and David McCoy}.

Updated: 2021-04-19

\hypertarget{introduction-to-causal-mediation-analysis}{%
\subsection{Introduction to Causal Mediation Analysis}\label{introduction-to-causal-mediation-analysis}}

A treatment often affects an outcome indirectly, through a particular pathway,
by its effect on \emph{intermediate variables} (mediators). Causal mediation analysis
concerns the construction and evaluation of these \emph{indirect effects} and their
complementary \emph{direct effects}. Generally, the indirect effect (IE) of a
treatment on an outcome is the portion of the total effect that is found to work
\emph{through} mediator variables, while the direct effect often encompasses all
other components of the total effect, including both the effect of the treatment
on the outcome \emph{and} the effect through all paths not explicitly involving the
mediators). Identifying and quantifying the mechanisms underlying causal effects
is an increasingly desirable endeavor in public health, medicine, and the social
sciences, as such mechanistic knowledge improves understanding of both \emph{why} and
\emph{how} treatments may be effective.

While the study of mediation analysis may be traced back quite far, the field
only came into its modern form with the identification and careful study of the
natural direct and indirect effects {[}\citet{robins1992identifiability};
pearl2001direct{]}. The natural direct effect (NDE) and the natural indirect
effect (NIE) are based on a decomposition of the average treatment effect (ATE)
in the presence of mediators \citep{vanderweele2015explanation}; requisite
theory for the construction of efficient estimators of these quantities only
receiving attention relatively recently \citep{tchetgen2012semiparametric}.

\hypertarget{data-structure-and-notation}{%
\subsection{Data Structure and Notation}\label{data-structure-and-notation}}

Consider \(n\) observed units \(O_1, \ldots, O_n\), where each observed data random
variable takes the form \(O = (W, A, Z, Y)\), for a vector of observed covariates
\(W\), a binary or continuous treatment \(A\), possibly multivariate mediators \(Z\),
and a binary or continuous outcome \(Y\). To avoid undue assumptions, we assume
only that \(O \sim \mathcal{P} \in \M\) where \(\M\) is the nonparametric
statistical model defined as all continuous densities on \(O\) with respect to an
arbitrary dominating measure.

We formalize the definition of our counterfactual variables using the following
non-parametric structural equation model (NPSEM):
\begin{align}
  W &= f_W(U_W) \\
  A &= f_A(W, U_A) \\
  Z &= f_Z(W, A, U_M) \\
  Y &= f_Y(W, A, Z, U_Y).
  \label{eq:npsem-mediate}
\end{align}
This set of equations
represents a mechanistic model generating the observed data \(O\); furthermore,
the NPSEM encodes several fundamental assumptions. Firstly, there is an implicit
temporal ordering: \(W\) occurs first, depending only on exogenous factors \(U_W\);
\(A\) happens next, based on both \(W\) and exogenous factors \(U_A\); then come the
mediators \(Z\), which depend on \(A\), \(W\), and another set of exogenous factors
\(U_Z\); and finally appears the outcome \(Y\). We assume neither access to the set
of exogenous factors \(\{U_W, U_A, U_Z, U_Y\}\) nor knowledge of the forms of the
deterministic generating functions \(\{f_W, f_A, f_Z, f_Y\}\). The NPSEM
corresponds to the following DAG:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dagitty)}
\KeywordTok{library}\NormalTok{(ggdag)}

\CommentTok{# make DAG by specifying dependence structure}
\NormalTok{dag <-}\StringTok{ }\KeywordTok{dagitty}\NormalTok{(}
  \StringTok{"dag \{}
\StringTok{    W -> A}
\StringTok{    W -> Z}
\StringTok{    W -> Y}
\StringTok{    A -> Z}
\StringTok{    A -> Y}
\StringTok{    Z -> Y}
\StringTok{    W -> A -> Y}
\StringTok{    W -> A -> Z -> Y}
\StringTok{  \}"}
\NormalTok{)}
\KeywordTok{exposures}\NormalTok{(dag) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{)}
\KeywordTok{outcomes}\NormalTok{(dag) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Y"}\NormalTok{)}
\NormalTok{tidy_dag <-}\StringTok{ }\KeywordTok{tidy_dagitty}\NormalTok{(dag)}

\CommentTok{# visualize DAG}
\KeywordTok{ggdag}\NormalTok{(tidy_dag) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{10-tmle3mediate_files/figure-latex/mediation-DAG-1} \end{center}

The likelihood of the data \(O\) admits a factorization, wherein, for \(p_0^O\),
the density of \(O\) with respect to the product measure, the density evaluated
on a particular observation \(o\) may be a written
\begin{equation}
  p_0^O(x) = q^O_{0,Y}(y \mid Z = z A = a, W = w)
    q^O_{0,Z}(z \mid A = a, W = w)
    q^O_{0,A}(a \mid W = w)
    q^O_{0,W}(w),
  \label{eq:likelihood-factorization-mediate}
\end{equation}
where \(q_{0, Y}\) is the conditional density of \(Y\) given \((Z, A, W)\), \(q_{0, Z}\)
is the conditional density of \(Z\) given \((A, W)\), \(q_{0, A}\) is the conditional
density of \(A\) given \(W\), and \(q_{0, W}\) is the density of \(W\). Further, for
ease of notation, we let \(\bar{Q}_Y(Z, A, W) = \E[Y \mid Z, A, W]\),
\(\bar{Q}_Z(A, W) = \P[Z \mid A, W]\) (later re-parametrized to \(e(A \mid Z, W) = \P(A \mid Z, W)\)), \(g(A \mid W) = \P(A \mid W)\), and \(q_W\) the marginal
distribution of \(W\).

Finally, note that we have explicitly excluded potential confounders of the
mediator-outcome relationship affected by exposure (i.e., variables affected by
\(A\) and affecting both \(Z\) and \(Y\)). Mediation analysis in the presence of such
variables is exceptionally challenging \citep{avin2005identifiability}; thus, most
efforts to develop definitions of causal direct and indirect effects explicitly
disallowed such a form of confounding. While we will not discuss the matter
here, the interested reader may consult recent advances in the vast literature
on causal mediation analysis, among them \citet{diaz2020nonparametric} and
\citet{hejazi2021nonparametric}.

\hypertarget{decomposing-the-average-treatment-effect}{%
\subsection{Decomposing the Average Treatment Effect}\label{decomposing-the-average-treatment-effect}}

The natural direct and indirect effects arise from a decomposition of the ATE:
\begin{equation*}
  \E[Y(1) - Y(0)] =
    \underbrace{\E[Y(1, Z(0)) - Y(0, Z(0))]}_{NDE} +
    \underbrace{\E[Y(1, Z(1)) - Y(1, Z(0))]}_{NIE}.
\end{equation*}
In particular, the natural indirect effect (NIE) measures the effect of the
treatment \(A \in \{0, 1\}\) on the outcome \(Y\) through the mediators \(Z\), while
the natural direct effect (NDE) measures the effect of the treatment on the
outcome \emph{through all other paths}. Identification of the natural direct and
indirect effects requires the following non-testable causal assumptions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Exchangeability}: \(Y(a, z) \indep (A, Z) \mid W\), which further implies
  \(\E\{Y(a, z) \mid A=a, W=w, Z=z\} = \E\{Y(a, z) \mid W=w\}\). This is a
  special case of the randomization assumption, extended to observational
  studies with mediators.
\item
  \emph{Treatment positivity}: For any \(a \in \mathcal{A}\) and \(w \in \mathcal{W}\), \(\xi < g(a \mid w) < 1 - \xi\), for \(\xi > 0\). This mirrors the
  assumption required for static intervention, discussed previously.
\item
  \emph{Mediator positivity}: For any \(z \in \mathcal{Z}\), \(a \in \mathcal{A}\), and
  \(w \in \mathcal{W}\), \(\epsilon < g(a \mid w)\), for \(\epsilon > 0\). This only
  requires that the conditional density of the mediators be bounded away from
  zero for all \((z, a, w)\) in their joint support \(\mathcal{Z} \times \mathcal{A} \times \mathcal{W}\).
\item
  \emph{Cross-world counterfactual independence}: For all \(a \neq a'\), both
  contained in \(\mathcal{A}\) and \(z \in \mathcal{Z}\), \(Y(a', z)\) is independent
  of \(Z(a)\), given \(W\). That is, the counterfactual outcome under the treatment
  contrast \(a' \in \mathcal{A}\) and the counterfactual mediator \(Z(a) in \mathcal{Z}\) (under a different contrast \(a \in \mathcal{A}\)) are
  independent. Note that the counterfactual outcome and mediator are defined
  under differing contrasts, hence the ``cross-world'' designation.
\end{enumerate}

We note that many attempts have been made to weaken the last assumption, that of
cross-world counterfactual independence, including work by
\citet{petersen2006estimation} and \citet{imai2010identification}; however, importantly,
\citet{robins2010alternative} established that this assumption cannot be satisfied in
randomized experiments. Thus, the natural direct and indirect effects are not
identifiable in randomized experiments, calling into question their utility.
Despite this significant limitation, we will turn to considering estimation of
the statistical functionals corresponding to these effects in observational
studies.

\hypertarget{the-natural-direct-effect}{%
\subsection{The Natural Direct Effect}\label{the-natural-direct-effect}}

The NDE is defined as
\begin{equation*}
  \Psi_{NDE} = \E[Y(1, Z(0)) - Y(0, Z(0))]
  \overset{\text{rand.}}{=} \sum_w \sum_z
  [\underbrace{\E(Y \mid A = 1, z, w)}_{\bar{Q}_Y(A = 1, z, w)} -
  \underbrace{\E(Y \mid A = 0, z, w)}_{\bar{Q}_Y(A = 0, z, w)}] \times
  \underbrace{p(z \mid A = 0, w)}_{Q_Z(0, w))} \underbrace{p(w)}_{q_W},
\end{equation*}
where the likelihood factors \(p(z \mid A = 0, w)\) and \(p(w)\) (among other
conditional densities) arise from a factorization of the joint likelihood:
\begin{equation*}
  p(w, a, z, y) = \underbrace{p(y \mid w, a, z)}_{Q_Y(A, W, Z)}
  \underbrace{p(z \mid w, a)}_{Q_Z(Z \mid A, W)}
  \underbrace{p(a \mid w)}_{g(A \mid W)}
  \underbrace{p(w)}_{Q_W}.
\end{equation*}

The process of estimating the NDE begins by constructing \(\bar{Q}_{Y, n}\), an
estimate of the outcome mechanism \(\bar{Q}_Y(Z, A, W) = \E \{Y \mid Z, A, W\}\) (i.e., the conditional mean of \(Y\), given \(Z\), \(A\), and \(W\)). With an
estimate of this conditional expectation in hand, predictions of the
counterfactual quantities \(\bar{Q}_Y(Z, 1, W)\) (setting \(A = 1\)) and, likewise,
\(\bar{Q}_Y(Z, 0, W)\) (setting \(A = 0\)) can readily be obtained. We denote the
difference of these counterfactual quantities \(\bar{Q}_{\text{diff}}\), i.e.,
\(\bar{Q}_{\text{diff}} = \bar{Q}_Y(Z, 1, W) - \bar{Q}_Y(Z, 0, W)\).
\(\bar{Q}_{\text{diff}}\) represents the difference in the conditional mean of
\(Y\) attributable to changes in \(A\) while keeping \(Z\) and \(W\) at their \emph{natural}
(that is, observed) values.

The estimation procedure treats \(\bar{Q}_{\text{diff}}\) itself as a nuisance
parameter, regressing its estimate \(\bar{Q}_{\text{diff}, n}\) on \(W\), among
control observations only (i.e., those for whom \(A = 0\) is observed); the goal
of this step is to remove part of the marginal impact of \(Z\) on
\(\bar{Q}_{\text{diff}}\), since \(W\) is a parent of \(Z\). Regressing this
difference on \(W\) among the controls recovers the expected
\(\bar{Q}_{\text{diff}}\), had all individuals been set to the control condition
\(A = 0\). Any residual additive effect of \(Z\) on \(\bar{Q}_{\text{diff}}\) is
removed during the TML estimation step using the auxiliary (or ``clever'')
covariate, which accounts for the mediators \(Z\). This auxiliary covariate takes
the form
\begin{equation*}
  C_Y(Q_Z, g)(O) = \Bigg\{\frac{\mathbb{I}(A = 1)}{g(1 \mid W)}
  \frac{Q_Z(Z \mid 0, W)}{Q_Z(Z \mid 1, W)} -
  \frac{\mathbb{I}(A = 0)}{g(0 \mid W)} \Bigg\}.
\end{equation*}
Breaking this down, \(\frac{\mathbb{I}(A = 1)}{g(1 \mid W)}\) is the inverse
propensity score weight for \(A = 1\) and, likewise, \(\frac{\mathbb{I}(A = 0)} {g(0 \mid W)}\) is the inverse propensity score weight for \(A = 0\). The middle
term is the ratio of the conditional densities of the mediator under the control
(\(A = 0\)) and treatment (\(A = 1\)) conditions.

This subtle appearance of a ratio of conditional densities is concerning --
unfortunately, tools to estimate such quantities are sparse in the statistics
literature, and the problem is still more complicated (and computationally
taxing) when \(Z\) is high-dimensional. As only the ratio of these conditional
densities is required, a convenient re-parametrization may be achieved, that is,
\begin{equation*}
  \frac{p(A = 0 \mid Z, W) g(0 \mid W)}{p(A = 1 \mid Z, W) g(1 \mid W)}.
\end{equation*}
Going forward, we will denote this re-parameterized conditional probability
\(e(A \mid Z, W) := p(A \mid Z, W)\). Similar re-parameterizations have been used
in \citet{zheng2012targeted} and \citet{tchetgen2013inverse}. This is particularly useful
since this reformulation reduces the problem to one concerning only the
estimation of conditional means, opening the door to the use of a wide range of
machine learning algorithms (e.g., most of those in
\href{https://github.com/tlverse/sl3}{\texttt{sl3}}).

Underneath the hood, the counterfactual outcome difference
\(\bar{Q}_{\text{diff}}\) and \(e(A \mid Z, W)\), the conditional probability of \(A\)
given \(Z\) and \(W\), are used in constructing the auxiliary covariate for TML
estimation. These nuisance parameters play an important role in the
bias-correcting update step of the TMLE procedure.

\hypertarget{the-natural-indirect-effect}{%
\subsection{The Natural Indirect Effect}\label{the-natural-indirect-effect}}

Derivation and estimation of the NIE is analogous to that of the NDE. The NIE
is the effect of \(A\) on \(Y\) \emph{only through the mediator(s) \(Z\)}. This quantity
-- known as the natural indirect effect \(\E(Y(Z(1), 1) - \E(Y(Z(0), 1)\) --
corresponds to the difference of the conditional expectation of \(Y\) given \(A = 1\) and \(Z(1)\) (the values the mediator would take under \(A = 1\)) and the
conditional expectation of \(Y\) given \(A = 1\) and \(Z(0)\) (the values the mediator
would take under \(A = 0\)).

As with the NDE, the re-parameterization trick can be used to estimate \(\E(A \mid Z, W)\), avoiding estimation of a possibly multivariate conditional density.
However, in this case, the mediated mean outcome difference, denoted
\(\Psi_Z(Q)\), is instead estimated as follows
\begin{equation*}
  \Psi_{NIE}(Q) = \E_{QZ}(\Psi_{NIE, Z}(Q)(1, W) - \Psi_{NIE, Z}(Q)(0, W))
\end{equation*}

Here, \(\bar{Q}_Y(Z, 1, W)\) (the predicted values for \(Y\) given \(Z\) and \(W\) when
\(A = 1\)) is regressed on \(W\), among the treated units (for whom \(A = 1\) is
observed) to obtain the conditional mean \(\Psi_{NIE, Z}(Q)(1, W)\). Performing
the same procedure, but now regressing \(\bar{Q}_Y(Z, 1, W)\) on \(W\) among the
control units (for whom \(A = 0\) is observed) yields \(\Psi_{NIE,Z}(Q)(0, W)\). The
difference of these two estimates is the NIE and can be thought of as the
additive marginal effect of treatment on the conditional expectation of \(Y\)
given \(W\), \(A = 1\), \(Z\) through its effects on \(Z\). So, in the case of the NIE,
our estimate \(\psi_n\) is slightly different, but the same quantity \(e(A \mid Z, W)\) comes into play as the auxiliary covariate.

\hypertarget{the-population-intervention-indirect-effects}{%
\subsection{The Population Intervention (In)Direct Effects}\label{the-population-intervention-indirect-effects}}

At times, the natural direct and indirect effects may prove too limiting, as
these effect definitions are based on \emph{static interventions} (i.e., setting
\(A = 0\) or \(A = 1\)), which may be unrealistic for real-world interventions. In
such cases, one may turn instead to the population intervention direct effect
(PIDE) and the population intervention indirect effect (PIIE), which are based
on decomposing the effect of the population intervention effect (PIE) of
flexible stochastic interventions \citep{diaz2020causal}.

A particular type of stochastic intervention well-suited to working with binary
treatments is the \emph{incremental propensity score intervention} (IPSI), first
proposed by \citet{kennedy2017nonparametric}. Such interventions do not
deterministically set the treatment level of an observed unit to a fixed
quantity (i.e., setting \(A = 1\)), but instead \emph{alter the odds of receiving the
treatment} by a fixed amount (\(0 \leq \delta \leq \infty\)) for each individual.
In particular, this intervention takes the form
\begin{equation*}
  g_{\delta}(1 \mid w) = \frac{\delta g(1 \mid w)}{\delta g(1 \mid w) + 1
  - g(1\mid w)},
\end{equation*}
where the scalar \(0 < \delta < \infty\) specifies a \emph{change in the odds of
receiving treatment}. As described by \citet{diaz2020causal}, this stochastic
intervention is a special case of exponential tilting, a framework that unifies
post-intervention treatment values that are draws from an altered distribution.

Unlike the natural direct and indirect effects, the conditions required for
identifiability of the population intervention direct and indirect effects are
more lax. Most importantly, these differences involve a (1) treatment positivity
assumption that only requires that the counterfactual treatment be in the
observed support of the treatment \(\mathcal{A}\), and (2) no requirement of the
independence any cross-world counterfactuals.

\hypertarget{decomposing-the-population-intervention-effect}{%
\subsection{Decomposing the Population Intervention Effect}\label{decomposing-the-population-intervention-effect}}

We may decompose the population intervention effect (PIE) in terms of the
\emph{population intervention direct effect} (PIDE) and the \emph{population
intervention indirect effect} (PIIE):
\begin{equation*}
  \mathbb{E}\{Y(A_\delta)\} - \mathbb{E}Y =
    \overbrace{\mathbb{E}\{Y(A_\delta, Z(A_\delta))
      - Y(A_\delta, Z)\}}^{\text{PIIE}} +
    \overbrace{\mathbb{E}\{Y(A_\delta, Z) - Y(A, Z)\}}^{\text{PIDE}}.
\end{equation*}

This decomposition of the PIE as the sum of the population intervention direct
and indirect effects has an interpretation analogous to the corresponding
standard decomposition of the average treatment effect. In the sequel, we will
compute each of the components of the direct and indirect effects above using
appropriate estimators as follows

\begin{itemize}
\tightlist
\item
  For \(\mathbb{E}\{Y(A, Z)\}\), the sample mean \(\frac{1}{n}\sum_{i=1}^n Y_i\) is
  consistent;
\item
  for \(\mathbb{E}\{Y(A_{\delta}, Z)\}\), a TML estimator for the effect of a
  joint intervention altering the treatment mechanism but not the mediation
  mechanism, based on the proposal in \citet{diaz2020causal}; and,
\item
  for \(\mathbb{E}\{Y(A_{\delta}, Z_{A_{\delta}})\}\), an efficient estimator for
  the effect of a joint intervention altering both the treatment and mediation
  mechanisms, as proposed in \citet{kennedy2017nonparametric} and implemented in the
  \href{https://github.com/ehkennedy/npcausal}{\texttt{npcausal} R package}.
\end{itemize}

\hypertarget{estimating-the-effect-decomposition-term}{%
\subsection{Estimating the Effect Decomposition Term}\label{estimating-the-effect-decomposition-term}}

As described by \citet{diaz2020causal}, the statistical functional identifying the
decomposition term that appears in both the PIDE and PIIE
\(\mathbb{E}\{Y(A_{\delta}, Z)\}\), which corresponds to altering the treatment
mechanism while keeping the mediation mechanism fixed, is
\begin{equation*}
  \theta_0(\delta) = \int m_0(a, z, w) g_{0,\delta}(a \mid w) p_0(z, w)
    d\nu(a, z, w),
\end{equation*}
for which a TML estimator is available. The corresponding \emph{efficient influence
function} (EIF) with respect to the nonparametric model \(\mathcal{M}\) is
\(D_{\eta,\delta}(o) = D^Y_{\eta,\delta}(o) + D^A_{\eta,\delta}(o) + D^{Z,W}_{\eta,\delta}(o) - \theta(\delta)\).

The TML estimator may be computed basd on the EIF estimating equation and may
incorporate cross-validation \citep{zheng2011cross, chernozhukov2018double} to
circumvent possibly restrictive entropy conditions (e.g., Donsker class). The
resultant estimator is
\begin{equation*}
  \hat{\theta}(\delta) = \frac{1}{n} \sum_{i = 1}^n D_{\hat{\eta}_{j(i)},
  \delta}(O_i) = \frac{1}{n} \sum_{i = 1}^n \left\{ D^Y_{\hat{\eta}_{j(i)},
  \delta}(O_i) + D^A_{\hat{\eta}_{j(i)}, \delta}(O_i) +
  D^{Z,W}_{\hat{\eta}_{j(i)}, \delta}(O_i) \right\},
\end{equation*}
which is implemented in \texttt{tmle3mediate} (a one-step estimator is also avaialble,
in the \href{https://github.com/nhejazi/medshift}{\texttt{medshift} R package}). We
demonstrate the use of \texttt{tmle3mediate} to obtain \(\mathbb{E}\{Y(A_{\delta}, Z)\}\)
via its TML estimator.

\hypertarget{evaluating-the-direct-and-indirect-effects}{%
\subsection{Evaluating the Direct and Indirect Effects}\label{evaluating-the-direct-and-indirect-effects}}

We now turn to estimating the natural direct and indirect effects, as well as
the population intervention direct effect, using the WASH Benefits data,
introduced in earlier chapters. Let's first load the data:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(data.table)}
\KeywordTok{library}\NormalTok{(sl3)}
\KeywordTok{library}\NormalTok{(tmle3)}
\KeywordTok{library}\NormalTok{(tmle3mediate)}

\CommentTok{# download data}
\NormalTok{washb_data <-}\StringTok{ }\KeywordTok{fread}\NormalTok{(}
  \KeywordTok{paste0}\NormalTok{(}
    \StringTok{"https://raw.githubusercontent.com/tlverse/tlverse-data/master/"}\NormalTok{,}
    \StringTok{"wash-benefits/washb_data.csv"}
\NormalTok{  ),}
  \DataTypeTok{stringsAsFactors =} \OtherTok{TRUE}
\NormalTok{)}

\CommentTok{# make intervention node binary}
\NormalTok{washb_data[, tr }\OperatorTok{:}\ErrorTok{=}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(tr }\OperatorTok{!=}\StringTok{ "Control"}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

We'll next define the baseline covariates \(W\), treatment \(A\), mediators \(Z\),
and outcome \(Y\) nodes of the NPSEM via a ``Node List'' object:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{node_list <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
  \DataTypeTok{W =} \KeywordTok{c}\NormalTok{(}
    \StringTok{"momage"}\NormalTok{, }\StringTok{"momedu"}\NormalTok{, }\StringTok{"momheight"}\NormalTok{, }\StringTok{"hfiacat"}\NormalTok{, }\StringTok{"Nlt18"}\NormalTok{, }\StringTok{"Ncomp"}\NormalTok{, }\StringTok{"watmin"}\NormalTok{,}
    \StringTok{"elec"}\NormalTok{, }\StringTok{"floor"}\NormalTok{, }\StringTok{"walls"}\NormalTok{, }\StringTok{"roof"}
\NormalTok{  ),}
  \DataTypeTok{A =} \StringTok{"tr"}\NormalTok{,}
  \DataTypeTok{Z =} \KeywordTok{c}\NormalTok{(}\StringTok{"sex"}\NormalTok{, }\StringTok{"month"}\NormalTok{, }\StringTok{"aged"}\NormalTok{),}
  \DataTypeTok{Y =} \StringTok{"whz"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here the \texttt{node\_list} encodes the parents of each node -- for example, \(Z\) (the
mediators) have parents \(A\) (the treatment) and \(W\) (the baseline confounders),
and \(Y\) (the outcome) has parents \(Z\), \(A\), and \(W\). We'll also handle any
missingness in the data by invoking \texttt{process\_missing}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{processed <-}\StringTok{ }\KeywordTok{process_missing}\NormalTok{(washb_data, node_list)}
\NormalTok{washb_data <-}\StringTok{ }\NormalTok{processed}\OperatorTok{$}\NormalTok{data}
\NormalTok{node_list <-}\StringTok{ }\NormalTok{processed}\OperatorTok{$}\NormalTok{node_list}
\end{Highlighting}
\end{Shaded}

We'll now construct an ensemble learner using a handful of popular machine
learning algorithms:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# SL learners used for continuous data (the nuisance parameter M)}
\NormalTok{enet_contin_learner <-}\StringTok{ }\NormalTok{Lrnr_glmnet}\OperatorTok{$}\KeywordTok{new}\NormalTok{(}
  \DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{family =} \StringTok{"gaussian"}\NormalTok{, }\DataTypeTok{nfolds =} \DecValTok{5}
\NormalTok{)}
\NormalTok{lasso_contin_learner <-}\StringTok{ }\NormalTok{Lrnr_glmnet}\OperatorTok{$}\KeywordTok{new}\NormalTok{(}
  \DataTypeTok{alpha =} \DecValTok{1}\NormalTok{, }\DataTypeTok{family =} \StringTok{"gaussian"}\NormalTok{, }\DataTypeTok{nfolds =} \DecValTok{5}
\NormalTok{)}
\NormalTok{fglm_contin_learner <-}\StringTok{ }\NormalTok{Lrnr_glm_fast}\OperatorTok{$}\KeywordTok{new}\NormalTok{(}\DataTypeTok{family =} \KeywordTok{gaussian}\NormalTok{())}
\NormalTok{mean_learner <-}\StringTok{ }\NormalTok{Lrnr_mean}\OperatorTok{$}\KeywordTok{new}\NormalTok{()}
\NormalTok{contin_learner_lib <-}\StringTok{ }\NormalTok{Stack}\OperatorTok{$}\KeywordTok{new}\NormalTok{(}
\NormalTok{  enet_contin_learner, lasso_contin_learner, fglm_contin_learner, mean_learner}
\NormalTok{)}
\NormalTok{sl_contin_learner <-}\StringTok{ }\NormalTok{Lrnr_sl}\OperatorTok{$}\KeywordTok{new}\NormalTok{(}\DataTypeTok{learners =}\NormalTok{ contin_learner_lib)}

\CommentTok{# SL learners used for binary data (nuisance parameters G and E in this case)}
\NormalTok{enet_binary_learner <-}\StringTok{ }\NormalTok{Lrnr_glmnet}\OperatorTok{$}\KeywordTok{new}\NormalTok{(}
  \DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{, }\DataTypeTok{nfolds =} \DecValTok{5}
\NormalTok{)}
\NormalTok{lasso_binary_learner <-}\StringTok{ }\NormalTok{Lrnr_glmnet}\OperatorTok{$}\KeywordTok{new}\NormalTok{(}
  \DataTypeTok{alpha =} \DecValTok{1}\NormalTok{, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{, }\DataTypeTok{nfolds =} \DecValTok{5}
\NormalTok{)}
\NormalTok{fglm_binary_learner <-}\StringTok{ }\NormalTok{Lrnr_glm_fast}\OperatorTok{$}\KeywordTok{new}\NormalTok{(}\DataTypeTok{family =} \KeywordTok{binomial}\NormalTok{())}
\NormalTok{binary_learner_lib <-}\StringTok{ }\NormalTok{Stack}\OperatorTok{$}\KeywordTok{new}\NormalTok{(}
\NormalTok{  enet_binary_learner, lasso_binary_learner, fglm_binary_learner, mean_learner}
\NormalTok{)}
\NormalTok{sl_binary_learner <-}\StringTok{ }\NormalTok{Lrnr_sl}\OperatorTok{$}\KeywordTok{new}\NormalTok{(}\DataTypeTok{learners =}\NormalTok{ binary_learner_lib)}

\CommentTok{# create list for treatment and outcome mechanism regressions}
\NormalTok{learner_list <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
  \DataTypeTok{Y =}\NormalTok{ sl_contin_learner,}
  \DataTypeTok{A =}\NormalTok{ sl_binary_learner}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{estimating-the-natural-indirect-effect}{%
\subsection{Estimating the Natural Indirect Effect}\label{estimating-the-natural-indirect-effect}}

We demonstrate calculation of the NIE below, starting by instantiating a ``Spec''
object that encodes exactly which learners to use for the nuisance parameters
\(e(A \mid Z, W)\) and \(\Psi_Z\). We then pass our Spec object to the \texttt{tmle3}
function, alongside the data, the node list (created above), and a learner list
indicating which machine learning algorithms to use for estimating the nuisance
parameters based on \(A\) and \(Y\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tmle_spec_NIE <-}\StringTok{ }\KeywordTok{tmle_NIE}\NormalTok{(}
  \DataTypeTok{e_learners =}\NormalTok{ Lrnr_cv}\OperatorTok{$}\KeywordTok{new}\NormalTok{(lasso_binary_learner, }\DataTypeTok{full_fit =} \OtherTok{TRUE}\NormalTok{),}
  \DataTypeTok{psi_Z_learners =}\NormalTok{ Lrnr_cv}\OperatorTok{$}\KeywordTok{new}\NormalTok{(lasso_contin_learner, }\DataTypeTok{full_fit =} \OtherTok{TRUE}\NormalTok{),}
  \DataTypeTok{max_iter =} \DecValTok{1}
\NormalTok{)}
\NormalTok{washb_NIE <-}\StringTok{ }\KeywordTok{tmle3}\NormalTok{(}
\NormalTok{  tmle_spec_NIE, washb_data, node_list, learner_list}
\NormalTok{)}
\NormalTok{washb_NIE}
\NormalTok{A tmle3_Fit that took }\DecValTok{1} \KeywordTok{step}\NormalTok{(s)}
\NormalTok{   type                  param  init_est  tmle_est       se     lower    upper}
\DecValTok{1}\OperatorTok{:}\StringTok{  }\NormalTok{NIE NIE[Y_\{A=}\DecValTok{1}\NormalTok{\} }\OperatorTok{-}\StringTok{ }\NormalTok{Y_\{A=}\DecValTok{0}\NormalTok{\}] }\FloatTok{0.0030972} \FloatTok{0.0029694} \FloatTok{0.015535} \FloatTok{-0.027479} \FloatTok{0.033418}
\NormalTok{   psi_transformed lower_transformed upper_transformed}
\DecValTok{1}\OperatorTok{:}\StringTok{       }\FloatTok{0.0029694}         \FloatTok{-0.027479}          \FloatTok{0.033418}
\end{Highlighting}
\end{Shaded}

Based on the output, we conclude that the indirect effect of the treatment
through the mediators (sex, month, aged) is
0.00297.

\hypertarget{estimating-the-natural-direct-effect}{%
\subsection{Estimating the Natural Direct Effect}\label{estimating-the-natural-direct-effect}}

An analogous procedure applies for estimation of the NDE, only replacing the
Spec object for the NIE with \texttt{tmle\_spec\_NDE} to define learners for the NDE
nuisance parameters:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tmle_spec_NDE <-}\StringTok{ }\KeywordTok{tmle_NDE}\NormalTok{(}
  \DataTypeTok{e_learners =}\NormalTok{ Lrnr_cv}\OperatorTok{$}\KeywordTok{new}\NormalTok{(lasso_binary_learner, }\DataTypeTok{full_fit =} \OtherTok{TRUE}\NormalTok{),}
  \DataTypeTok{psi_Z_learners =}\NormalTok{ Lrnr_cv}\OperatorTok{$}\KeywordTok{new}\NormalTok{(lasso_contin_learner, }\DataTypeTok{full_fit =} \OtherTok{TRUE}\NormalTok{),}
  \DataTypeTok{max_iter =} \DecValTok{1}
\NormalTok{)}
\NormalTok{washb_NDE <-}\StringTok{ }\KeywordTok{tmle3}\NormalTok{(}
\NormalTok{  tmle_spec_NDE, washb_data, node_list, learner_list}
\NormalTok{)}
\NormalTok{washb_NDE}
\NormalTok{A tmle3_Fit that took }\DecValTok{1} \KeywordTok{step}\NormalTok{(s)}
\NormalTok{   type                  param init_est tmle_est      se    lower  upper}
\DecValTok{1}\OperatorTok{:}\StringTok{  }\NormalTok{NDE NDE[Y_\{A=}\DecValTok{1}\NormalTok{\} }\OperatorTok{-}\StringTok{ }\NormalTok{Y_\{A=}\DecValTok{0}\NormalTok{\}] }\FloatTok{0.028931} \FloatTok{0.028931} \FloatTok{0.31693} \FloatTok{-0.59223} \FloatTok{0.6501}
\NormalTok{   psi_transformed lower_transformed upper_transformed}
\DecValTok{1}\OperatorTok{:}\StringTok{        }\FloatTok{0.028931}          \FloatTok{-0.59223}            \FloatTok{0.6501}
\end{Highlighting}
\end{Shaded}

From this, we can draw the conclusion that the direct effect of the treatment
(through all paths not involving the mediators (sex, month, aged)) is
0.02893. Note that, together, the estimates of
the natural direct and indirect effects approximately recover the \emph{average
treatment effect}, that is, based on these estimates of the NDE and NIE, the
ATE is roughly
0.0319.

\hypertarget{estimating-the-population-intervention-direct-effect}{%
\subsection{Estimating the Population Intervention Direct Effect}\label{estimating-the-population-intervention-direct-effect}}

As previously noted, the assumptions underlying the natural direct and indirect
effects may be challenging to justify; moreover, the effect definitions
themselves depend on the application of a static intervention to the treatment,
sharply limiting their flexibility. When considering binary treatments,
incremental propensity score shifts provide an alternative class of flexible,
stochastic interventions. We'll now consider estimating the PIDE with an IPSI
that modulates the odds of receiving treatment by \(\delta = 3\). Such an
intervention may be interpreted (hypothetically) as the effect of a design that
encourages study participants to opt in to receiving the treatment, thus
increasing their relative odds of receiving said treatment. To exemplify our
approach, we postulate a motivational intervention that \emph{triples the odds}
(i.e., \(\delta = 3\)) of receiving the treatment for each individual:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set the IPSI multiplicative shift}
\NormalTok{delta_ipsi <-}\StringTok{ }\DecValTok{3}

\CommentTok{# instantiate tmle3 spec for stochastic mediation}
\NormalTok{tmle_spec_pie_decomp <-}\StringTok{ }\KeywordTok{tmle_medshift}\NormalTok{(}
  \DataTypeTok{delta =}\NormalTok{ delta_ipsi,}
  \DataTypeTok{e_learners =}\NormalTok{ Lrnr_cv}\OperatorTok{$}\KeywordTok{new}\NormalTok{(lasso_binary_learner, }\DataTypeTok{full_fit =} \OtherTok{TRUE}\NormalTok{),}
  \DataTypeTok{phi_learners =}\NormalTok{ Lrnr_cv}\OperatorTok{$}\KeywordTok{new}\NormalTok{(lasso_contin_learner, }\DataTypeTok{full_fit =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{)}

\CommentTok{# compute the TML estimate}
\NormalTok{washb_pie_decomp <-}\StringTok{ }\KeywordTok{tmle3}\NormalTok{(}
\NormalTok{  tmle_spec_pie_decomp, washb_data, node_list, learner_list}
\NormalTok{)}
\NormalTok{washb_pie_decomp}
\NormalTok{A tmle3_Fit that took }\DecValTok{510} \KeywordTok{step}\NormalTok{(s)}
\NormalTok{   type         param init_est tmle_est       se   lower    upper}
\DecValTok{1}\OperatorTok{:}\StringTok{ }\NormalTok{PIDE E[Y_\{A=}\OtherTok{NULL}\NormalTok{\}] }\FloatTok{-0.58163} \FloatTok{-0.58385} \FloatTok{0.016302} \FloatTok{-0.6158} \FloatTok{-0.55189}
\NormalTok{   psi_transformed lower_transformed upper_transformed}
\DecValTok{1}\OperatorTok{:}\StringTok{        }\FloatTok{-0.58385}           \FloatTok{-0.6158}          \FloatTok{-0.55189}
\end{Highlighting}
\end{Shaded}

Recall that, based on the decomposition outlined previously, the PIDE may be
denoted \(\beta_{\text{PIDE}}(\delta) = \theta_0(\delta) - \mathbb{E}Y\). Thus, an
estimator of the PIDE, \(\hat{\beta}_{\text{PIDE}}(\delta)\) may be expressed as a
composition of estimators of its constituent parameters:
\begin{equation*}
  \hat{\beta}_{\text{PIDE}}({\delta}) = \hat{\theta}(\delta) -
  \frac{1}{n} \sum_{i = 1}^n Y_i.
\end{equation*}

Based on the above, we may construct an estimator of the PIDE using the already
estimated decomposition term and the empirical (marginal) mean of the outcome.
Thus, our estimate of the PIDE is approximately
0.00223.
Note that this is a straightforward application of the delta method and could
equivalently be performed using the functionality exposed in the \href{https://github.com/tlverse/tmle3}{\texttt{tmle3}
package}.

\hypertarget{r6}{%
\section{\texorpdfstring{A Primer on the \texttt{R6} Class System}{A Primer on the R6 Class System}}\label{r6}}

A central goal of the Targeted Learning statistical paradigm is to estimate
scientifically relevant parameters in realistic (usually nonparametric) models.

The \texttt{tlverse} is designed using basic OOP principles and the \texttt{R6} OOP framework.
While we've tried to make it easy to use the \texttt{tlverse} packages without worrying
much about OOP, it is helpful to have some intuition about how the \texttt{tlverse} is
structured. Here, we briefly outline some key concepts from OOP. Readers
familiar with OOP basics are invited to skip this section.

\hypertarget{classes-fields-and-methods}{%
\subsection{Classes, Fields, and Methods}\label{classes-fields-and-methods}}

The key concept of OOP is that of an object, a collection of data and functions
that corresponds to some conceptual unit. Objects have two main types of
elements:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{fields}, which can be thought of as nouns, are information about an object,
  and
\item
  \emph{methods}, which can be thought of as verbs, are actions an object can
  perform.
\end{enumerate}

Objects are members of classes, which define what those specific fields and
methods are. Classes can inherit elements from other classes (sometimes called
base classes) -- accordingly, classes that are similar, but not exactly the
same, can share some parts of their definitions.

Many different implementations of OOP exist, with variations in how these
concepts are implemented and used. R has several different implementations,
including \texttt{S3}, \texttt{S4}, reference classes, and \texttt{R6}. The \texttt{tlverse} uses the \texttt{R6}
implementation. In \texttt{R6}, methods and fields of a class object are accessed using
the \texttt{\$} operator. For a more thorough introduction to \texttt{R}'s various OOP systems,
see \url{http://adv-r.had.co.nz/OO-essentials.html}, from Hadley Wickham's \emph{Advanced
R} \citep{wickham2014advanced}.

\hypertarget{object-oriented-programming-python-and-r}{%
\subsection{\texorpdfstring{Object Oriented Programming: \texttt{Python} and \texttt{R}}{Object Oriented Programming: Python and R}}\label{object-oriented-programming-python-and-r}}

OO concepts (classes with inherentence) were baked into Python from the first
published version (version 0.9 in 1991). In contrast, \texttt{R} gets its OO ``approach''
from its predecessor, \texttt{S}, first released in 1976. For the first 15 years, \texttt{S}
had no support for classes, then, suddenly, \texttt{S} got two OO frameworks bolted on
in rapid succession: informal classes with \texttt{S3} in 1991, and formal classes with
\texttt{S4} in 1998. This process continues, with new OO frameworks being periodically
released, to try to improve the lackluster OO support in \texttt{R}, with reference
classes (\texttt{R5}, 2010) and \texttt{R6} (2014). Of these, \texttt{R6} behaves most like Python
classes (and also most like OOP focused languages like C++ and Java), including
having method definitions be part of class definitions, and allowing objects to
be modified by reference.

\bibliography{book.bib,packages.bib}

\backmatter
\printindex

\end{document}
